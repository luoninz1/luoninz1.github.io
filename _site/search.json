[
  {
    "objectID": "urop-symposium.html",
    "href": "urop-symposium.html",
    "title": "2025 UC Irvine Undergraduate Research Symposium",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "urop-symposium.html#symposium-poster-interim-presentation",
    "href": "urop-symposium.html#symposium-poster-interim-presentation",
    "title": "2025 UC Irvine Undergraduate Research Symposium",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "nasdaq-prediction.html",
    "href": "nasdaq-prediction.html",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "",
    "text": "Math 178 Final Project"
  },
  {
    "objectID": "nasdaq-prediction.html#abstract",
    "href": "nasdaq-prediction.html#abstract",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "Abstract",
    "text": "Abstract\n\nDataset: Nasdaq Index\nThe dataset is from UCI Machine Learning Repository.\nLink: https://archive.ics.uci.edu/dataset/554/cnnpred+cnn+based+stock+market+prediction+using+a+diverse+set+of+variables\n“This dataset contains several daily features of S&P 500, NASDAQ Composite, Dow Jones Industrial Average, RUSSELL 2000, and NYSE Composite from 2010 to 2017……It covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates.” [1]\nFor this project, we only use the “NASDAQ Composite” part of this dataset.\n\n\nObjectives\nThe goal of this project is predicting the future Nasdaq Composite Index (COMP) using historical data by machine learning and deep learning models.\nParticularly, we have two goals:\n\nPredicting the value of Nasdaq Index of the next day given historical data.\nConducting an investment simulation by predicting increasing or decreasing trend of Nasdaq Index of the next day.\n\n\n\nMethods\nFirst, we cleaned the data, did basic feature engineering, implemented exploratory data visulization, and made baseline models in section 1. Then, we used a variety of regression models to try to predict the values of future Nasdaq Index, respectively. Also, we implemented some feature selection methods and optimized the hyperparameters to improve the performance of the models.\n\nRegression Models (Section 2-4): Linear Regression, Tree Based Models (Decision Tree, Random Forest, XGBoost), MLP, LSTM, 1DCNN, CNN LSTM\nFeature Selection Method: Recursive Feature Elimination (RFE)\n\nFinally, in section 5, we evaluated and compared the accuracy of the different models on the testing data and implement an investment simulation to evaluate the profitabilities of these models.\n\n\nResults\nBecause the noise of data is high and the test data is deviated from the training data, it is very hard to make precise predictions. The performance of all the models we trained are not very good, which are even worse than the baseline model. This result is expected. It reflects the complexity of the financial market. More advanced model and more significant features might be helpful to improve the prediction."
  },
  {
    "objectID": "nasdaq-prediction.html#data-preparation",
    "href": "nasdaq-prediction.html#data-preparation",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "1. Data Preparation",
    "text": "1. Data Preparation\n\n1.1 Exploratory data analysis and data preprocessing\n\n# Import the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\nimport joblib\nfrom umap.umap_ import UMAP\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, Flatten, MaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nFirstly, we take a look at the columns and the dataset.\n\n# Load the dataset.\nFILE_PATH = \"Processed_NASDAQ.csv\"\ndf = pd.read_csv(FILE_PATH)\n\n# Display the data.\nprint(df)\n\n            Date        Close    Volume       mom      mom1      mom2  \\\n0     2009-12-31  2269.149902       NaN       NaN       NaN       NaN   \n1     2010-01-04  2308.419922  0.560308  0.017306       NaN       NaN   \n2     2010-01-05  2308.709961  0.225994  0.000126  0.017306       NaN   \n3     2010-01-06  2301.090088 -0.048364 -0.003300  0.000126  0.017306   \n4     2010-01-07  2300.050049  0.007416 -0.000452 -0.003300  0.000126   \n...          ...          ...       ...       ...       ...       ...   \n1979  2017-11-09  6750.049805  0.058830 -0.005755  0.003153 -0.002750   \n1980  2017-11-10  6750.939941 -0.116863  0.000132 -0.005755  0.003153   \n1981  2017-11-13  6757.600098 -0.000091  0.000987  0.000132 -0.005755   \n1982  2017-11-14  6737.870117  0.005087 -0.002920  0.000987  0.000132   \n1983  2017-11-15  6706.209961 -0.037891 -0.004699 -0.002920  0.000987   \n\n          mom3     ROC_5    ROC_10    ROC_15  ...   NZD  silver-F  RUSSELL-F  \\\n0          NaN       NaN       NaN       NaN  ...  0.03      0.26      -1.08   \n1          NaN       NaN       NaN       NaN  ...  1.52      3.26       1.61   \n2          NaN       NaN       NaN       NaN  ... -0.07      1.96      -0.20   \n3          NaN       NaN       NaN       NaN  ...  0.56      2.15      -0.02   \n4     0.017306       NaN       NaN       NaN  ... -0.72      0.94       0.50   \n...        ...       ...       ...       ...  ...   ...       ...        ...   \n1979  0.003252  0.522862  2.947790  2.194980  ... -0.24     -0.62      -0.34   \n1980 -0.002750 -0.199573  0.741356  1.838727  ... -0.27     -0.58      -0.20   \n1981  0.003153 -0.424963  0.875362  2.592598  ... -0.38      0.72      -0.04   \n1982 -0.005755 -0.441942  0.151616  2.113229  ... -0.39      0.17      -0.21   \n1983  0.000132 -1.221221 -0.153648  2.168224  ...  0.03     -0.60      -0.46   \n\n      S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n0     -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  \n1      1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  \n2      0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  \n3      0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  \n4      0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  \n...     ...   ...             ...           ...      ...   ...   ...  \n1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  \n1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  \n1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  \n1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  \n1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  \n\n[1984 rows x 84 columns]\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1984 entries, 0 to 1983\nData columns (total 84 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Date            1984 non-null   object \n 1   Close           1984 non-null   float64\n 2   Volume          1983 non-null   float64\n 3   mom             1983 non-null   float64\n 4   mom1            1982 non-null   float64\n 5   mom2            1981 non-null   float64\n 6   mom3            1980 non-null   float64\n 7   ROC_5           1979 non-null   float64\n 8   ROC_10          1974 non-null   float64\n 9   ROC_15          1969 non-null   float64\n 10  ROC_20          1964 non-null   float64\n 11  EMA_10          1975 non-null   float64\n 12  EMA_20          1965 non-null   float64\n 13  EMA_50          1935 non-null   float64\n 14  EMA_200         1785 non-null   float64\n 15  DTB4WK          1984 non-null   float64\n 16  DTB3            1984 non-null   float64\n 17  DTB6            1984 non-null   float64\n 18  DGS5            1984 non-null   float64\n 19  DGS10           1984 non-null   float64\n 20  Oil             1983 non-null   float64\n 21  Gold            1983 non-null   float64\n 22  DAAA            1984 non-null   float64\n 23  DBAA            1984 non-null   float64\n 24  GBP             1506 non-null   float64\n 25  JPY             1505 non-null   float64\n 26  CAD             1501 non-null   float64\n 27  CNY             1505 non-null   float64\n 28  AAPL            1983 non-null   float64\n 29  AMZN            1983 non-null   float64\n 30  GE              1983 non-null   float64\n 31  JNJ             1983 non-null   float64\n 32  JPM             1983 non-null   float64\n 33  MSFT            1983 non-null   float64\n 34  WFC             1983 non-null   float64\n 35  XOM             1983 non-null   float64\n 36  FCHI            1944 non-null   float64\n 37  FTSE            1913 non-null   float64\n 38  GDAXI           1917 non-null   float64\n 39  GSPC            1983 non-null   float64\n 40  HSI             1814 non-null   float64\n 41  DJI             1983 non-null   float64\n 42  SSEC            1746 non-null   float64\n 43  RUT             1983 non-null   float64\n 44  NYSE            1983 non-null   float64\n 45  TE1             1984 non-null   float64\n 46  TE2             1984 non-null   float64\n 47  TE3             1984 non-null   float64\n 48  TE5             1984 non-null   float64\n 49  TE6             1984 non-null   float64\n 50  DE1             1984 non-null   float64\n 51  DE2             1984 non-null   float64\n 52  DE4             1984 non-null   float64\n 53  DE5             1984 non-null   float64\n 54  DE6             1984 non-null   float64\n 55  CTB3M           1983 non-null   float64\n 56  CTB6M           1983 non-null   float64\n 57  CTB1Y           1983 non-null   float64\n 58  Name            1984 non-null   object \n 59  AUD             1984 non-null   float64\n 60  Brent           1984 non-null   float64\n 61  CAC-F           1980 non-null   float64\n 62  copper-F        1984 non-null   float64\n 63  WIT-oil         1984 non-null   float64\n 64  DAX-F           1980 non-null   float64\n 65  DJI-F           1984 non-null   float64\n 66  EUR             1984 non-null   float64\n 67  FTSE-F          1978 non-null   float64\n 68  gold-F          1984 non-null   float64\n 69  HSI-F           1909 non-null   float64\n 70  KOSPI-F         1905 non-null   float64\n 71  NASDAQ-F        1984 non-null   float64\n 72  GAS-F           1984 non-null   float64\n 73  Nikkei-F        1984 non-null   float64\n 74  NZD             1984 non-null   float64\n 75  silver-F        1984 non-null   float64\n 76  RUSSELL-F       1984 non-null   float64\n 77  S&P-F           1984 non-null   float64\n 78  CHF             1984 non-null   float64\n 79  Dollar index-F  1984 non-null   float64\n 80  Dollar index    1984 non-null   float64\n 81  wheat-F         1982 non-null   float64\n 82  XAG             1984 non-null   float64\n 83  XAU             1984 non-null   float64\ndtypes: float64(82), object(2)\nmemory usage: 1.3+ MB\n\n\nAs we can see, there are a lot of null values. We try to fill these null values to keep as many samples as possible. Considering the nature of time-series prediction, it is important to avoid leaking the future data when we fill these values. So, we don’t choose the methods filling backward or filling the mean values which involves using future values to fill the past. Instead, we utilize approaches including filling forward and dropping null values.\n\n# EMA_200 contains too many null values.\n# Dropping these rows would lose a lot of data, and filling them could distort the data pattern.\n# Therefore, we choose to drop this feature.\ndf = df.drop(columns=['EMA_200'], errors='ignore')\n\n# Fill forward missing values and then drop any remaining null values.\ndf = df.ffill()\ndf = df.dropna()\n\nprint(df)\n\n            Date        Close    Volume       mom      mom1      mom2  \\\n49    2010-03-15  2362.209961 -0.067949 -0.002302 -0.000338  0.004031   \n50    2010-03-16  2378.010010  0.125608  0.006689 -0.002302 -0.000338   \n51    2010-03-17  2389.090088  0.036119  0.004659  0.006689 -0.002302   \n52    2010-03-18  2391.280029 -0.047493  0.000917  0.004659  0.006689   \n53    2010-03-19  2374.409912  0.406201 -0.007055  0.000917  0.004659   \n...          ...          ...       ...       ...       ...       ...   \n1979  2017-11-09  6750.049805  0.058830 -0.005755  0.003153 -0.002750   \n1980  2017-11-10  6750.939941 -0.116863  0.000132 -0.005755  0.003153   \n1981  2017-11-13  6757.600098 -0.000091  0.000987  0.000132 -0.005755   \n1982  2017-11-14  6737.870117  0.005087 -0.002920  0.000987  0.000132   \n1983  2017-11-15  6706.209961 -0.037891 -0.004699 -0.002920  0.000987   \n\n          mom3     ROC_5    ROC_10    ROC_15  ...   NZD  silver-F  RUSSELL-F  \\\n49    0.007805  1.286334  3.898710  5.360318  ...  0.14     -0.47      -0.27   \n50    0.004031  1.594839  4.262557  7.435037  ...  1.35      1.46       0.77   \n51   -0.000338  1.277693  4.753414  6.851388  ...  0.21      0.98       0.49   \n52   -0.002302  0.963498  4.317477  7.029749  ...  0.18     -0.54      -0.28   \n53    0.006689  0.285092  2.065889  6.082846  ... -0.95     -2.24       0.54   \n...        ...       ...       ...       ...  ...   ...       ...        ...   \n1979  0.003252  0.522862  2.947790  2.194980  ... -0.24     -0.62      -0.34   \n1980 -0.002750 -0.199573  0.741356  1.838727  ... -0.27     -0.58      -0.20   \n1981  0.003153 -0.424963  0.875362  2.592598  ... -0.38      0.72      -0.04   \n1982 -0.005755 -0.441942  0.151616  2.113229  ... -0.39      0.17      -0.21   \n1983  0.000132 -1.221221 -0.153648  2.168224  ...  0.03     -0.60      -0.46   \n\n      S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n49    -0.09  0.39            0.41          0.53    -1.34 -0.06  0.60  \n50     0.80 -0.62           -0.21         -0.62     1.83  2.11  1.50  \n51     0.54 -0.09           -0.19         -0.14     1.74  0.40 -0.06  \n52     0.02  0.30            0.74          0.73    -1.31 -0.69  0.08  \n53     0.60  0.38            0.66          0.62    -0.95 -2.36 -1.63  \n...     ...   ...             ...           ...      ...   ...   ...  \n1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  \n1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  \n1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  \n1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  \n1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  \n\n[1935 rows x 83 columns]\n\n\nFeature Processing and Feature Expansion\n\nCreate three date-related features.\nThe ‘Close’ (Nasdaq Composite Index) and ‘EMA’ (Exponential Moving Average) features are numerical values, but most of the other features are rate of change (ROC). It doesn’t make sense to use ROC to predict a numerical value, especially for linear models. (For example, from 5000 to 6000, the ROC is 0.2. From 1000 to 2000, the ROC is 1.) Therefore, we converge the ‘Close’ and ‘EMA’ features to rate of change.\nWe insert the t-1 to t-k days data to every t day’s row by creating new columns.\nSqaure all the features to create new features to enhance non-linearity capture and highlight volatility effects.\n\n\nimport pandas as pd\n\n# Convert the 'Date' column to datetime format.\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create new features:\n# 1. Day of the week (Monday=0, Sunday=6)\ndf['day_of_week'] = df['Date'].dt.dayofweek\n\n# 2. Day of the month\ndf['day_of_month'] = df['Date'].dt.day\n\n# 3. Month as a number\ndf['month'] = df['Date'].dt.month\n\n# Compute the rate of change (as a decimal, not a percentage)\ndf['Close_ROC'] = df['Close'].pct_change()\ndf['EMA10_ROC'] = df['EMA_10'].pct_change()\ndf['EMA20_ROC'] = df['EMA_20'].pct_change()\ndf['EMA50_ROC'] = df['EMA_50'].pct_change()\n\n# Save a copy of the original DataFrame.\ndf_raw = df.copy()\n\n# Drop non-numeric columns.\ndf = df.drop(columns=['Date', 'Name'], errors='ignore')\n\n# Drop the first row because the rate of change doesn't apply to it.\ndf = df.dropna()\n\n# Save the list of original columns (before adding lag features).\noriginal_cols = df.columns.tolist()\n\n# Number of previous days to include.\nk = 5\n\n# For each lag (from 1 to k), create new columns by shifting the original columns and\n# create squared features for each lagged column.\nfor i in range(1, k + 1):\n    for col in original_cols:\n        lagged_col = f'{col}_day-{i}'\n        squared_col = f'{lagged_col}^2'\n        df[lagged_col] = df[col].shift(i)\n        df[squared_col] = df[lagged_col] ** 2\n\n# Drop rows with NaN values resulting from shifting.\ndf = df.dropna()\n\nprint(df)\n\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\n\n\n            Close    Volume       mom      mom1      mom2      mom3     ROC_5  \\\n55    2415.239990  0.006985  0.008283  0.008840 -0.007055  0.000917  1.565594   \n56    2398.760010 -0.011816 -0.006823  0.008283  0.008840 -0.007055  0.404753   \n57    2397.409912  0.110328 -0.000563 -0.006823  0.008283  0.008840  0.256343   \n58    2395.129883 -0.124568 -0.000951 -0.000563 -0.006823  0.008283  0.872637   \n59    2404.360107 -0.168958  0.003854 -0.000951 -0.000563 -0.006823  0.374059   \n...           ...       ...       ...       ...       ...       ...       ...   \n1979  6750.049805  0.058830 -0.005755  0.003153 -0.002750  0.003252  0.522862   \n1980  6750.939941 -0.116863  0.000132 -0.005755  0.003153 -0.002750 -0.199573   \n1981  6757.600098 -0.000091  0.000987  0.000132 -0.005755  0.003153 -0.424963   \n1982  6737.870117  0.005087 -0.002920  0.000987  0.000132 -0.005755 -0.441942   \n1983  6706.209961 -0.037891 -0.004699 -0.002920  0.000987  0.000132 -1.221221   \n\n        ROC_10    ROC_15    ROC_20  ...  month_day-5  month_day-5^2  \\\n55    3.185402  5.894885  9.117033  ...          3.0            9.0   \n56    1.687618  5.177407  7.283873  ...          3.0            9.0   \n57    1.222311  4.584888  7.304113  ...          3.0            9.0   \n58    1.160216  2.956553  7.008563  ...          3.0            9.0   \n59    1.784352  3.093639  5.752628  ...          3.0            9.0   \n...        ...       ...       ...  ...          ...            ...   \n1979  2.947790  2.194980  2.405216  ...         11.0          121.0   \n1980  0.741356  1.838727  2.197162  ...         11.0          121.0   \n1981  0.875362  2.592598  2.016910  ...         11.0          121.0   \n1982  0.151616  2.113229  1.724273  ...         11.0          121.0   \n1983 -0.153648  2.168224  1.237727  ...         11.0          121.0   \n\n      Close_ROC_day-5  Close_ROC_day-5^2  EMA10_ROC_day-5  EMA10_ROC_day-5^2  \\\n55           0.006689       4.473834e-05         0.003607           0.000013   \n56           0.004659       2.170992e-05         0.003802           0.000014   \n57           0.000917       8.402331e-07         0.003268           0.000011   \n58          -0.007055       4.977088e-05         0.001364           0.000002   \n59           0.008840       7.814714e-05         0.002731           0.000007   \n...               ...                ...              ...                ...   \n1979        -0.000237       5.602981e-08         0.001613           0.000003   \n1980         0.007372       5.434081e-05         0.002668           0.000007   \n1981         0.003252       1.057747e-05         0.002775           0.000008   \n1982        -0.002750       7.560430e-06         0.001758           0.000003   \n1983         0.003153       9.942828e-06         0.002014           0.000004   \n\n      EMA20_ROC_day-5  EMA20_ROC_day-5^2  EMA50_ROC_day-5  EMA50_ROC_day-5^2  \n55           0.003339           0.000011         0.002181           0.000005  \n56           0.003468           0.000012         0.002284           0.000005  \n57           0.003217           0.000010         0.002227           0.000005  \n58           0.002209           0.000005         0.001843           0.000003  \n59           0.002854           0.000008         0.002130           0.000005  \n...               ...                ...              ...                ...  \n1979         0.001461           0.000002         0.001191           0.000001  \n1980         0.002031           0.000004         0.001441           0.000002  \n1981         0.002150           0.000005         0.001514           0.000002  \n1982         0.001674           0.000003         0.001341           0.000002  \n1983         0.001817           0.000003         0.001414           0.000002  \n\n[1929 rows x 968 columns]\n\n\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[lagged_col] = df[col].shift(i)\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[squared_col] = df[lagged_col] ** 2\n\n\n\ndf\n\n\n\n\n\n\n\n\nClose\nVolume\nmom\nmom1\nmom2\nmom3\nROC_5\nROC_10\nROC_15\nROC_20\n...\nmonth_day-5\nmonth_day-5^2\nClose_ROC_day-5\nClose_ROC_day-5^2\nEMA10_ROC_day-5\nEMA10_ROC_day-5^2\nEMA20_ROC_day-5\nEMA20_ROC_day-5^2\nEMA50_ROC_day-5\nEMA50_ROC_day-5^2\n\n\n\n\n55\n2415.239990\n0.006985\n0.008283\n0.008840\n-0.007055\n0.000917\n1.565594\n3.185402\n5.894885\n9.117033\n...\n3.0\n9.0\n0.006689\n4.473834e-05\n0.003607\n0.000013\n0.003339\n0.000011\n0.002181\n0.000005\n\n\n56\n2398.760010\n-0.011816\n-0.006823\n0.008283\n0.008840\n-0.007055\n0.404753\n1.687618\n5.177407\n7.283873\n...\n3.0\n9.0\n0.004659\n2.170992e-05\n0.003802\n0.000014\n0.003468\n0.000012\n0.002284\n0.000005\n\n\n57\n2397.409912\n0.110328\n-0.000563\n-0.006823\n0.008283\n0.008840\n0.256343\n1.222311\n4.584888\n7.304113\n...\n3.0\n9.0\n0.000917\n8.402331e-07\n0.003268\n0.000011\n0.003217\n0.000010\n0.002227\n0.000005\n\n\n58\n2395.129883\n-0.124568\n-0.000951\n-0.000563\n-0.006823\n0.008283\n0.872637\n1.160216\n2.956553\n7.008563\n...\n3.0\n9.0\n-0.007055\n4.977088e-05\n0.001364\n0.000002\n0.002209\n0.000005\n0.001843\n0.000003\n\n\n59\n2404.360107\n-0.168958\n0.003854\n-0.000951\n-0.000563\n-0.006823\n0.374059\n1.784352\n3.093639\n5.752628\n...\n3.0\n9.0\n0.008840\n7.814714e-05\n0.002731\n0.000007\n0.002854\n0.000008\n0.002130\n0.000005\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1979\n6750.049805\n0.058830\n-0.005755\n0.003153\n-0.002750\n0.003252\n0.522862\n2.947790\n2.194980\n2.405216\n...\n11.0\n121.0\n-0.000237\n5.602981e-08\n0.001613\n0.000003\n0.001461\n0.000002\n0.001191\n0.000001\n\n\n1980\n6750.939941\n-0.116863\n0.000132\n-0.005755\n0.003153\n-0.002750\n-0.199573\n0.741356\n1.838727\n2.197162\n...\n11.0\n121.0\n0.007372\n5.434081e-05\n0.002668\n0.000007\n0.002031\n0.000004\n0.001441\n0.000002\n\n\n1981\n6757.600098\n-0.000091\n0.000987\n0.000132\n-0.005755\n0.003153\n-0.424963\n0.875362\n2.592598\n2.016910\n...\n11.0\n121.0\n0.003252\n1.057747e-05\n0.002775\n0.000008\n0.002150\n0.000005\n0.001514\n0.000002\n\n\n1982\n6737.870117\n0.005087\n-0.002920\n0.000987\n0.000132\n-0.005755\n-0.441942\n0.151616\n2.113229\n1.724273\n...\n11.0\n121.0\n-0.002750\n7.560430e-06\n0.001758\n0.000003\n0.001674\n0.000003\n0.001341\n0.000002\n\n\n1983\n6706.209961\n-0.037891\n-0.004699\n-0.002920\n0.000987\n0.000132\n-1.221221\n-0.153648\n2.168224\n1.237727\n...\n11.0\n121.0\n0.003153\n9.942828e-06\n0.002014\n0.000004\n0.001817\n0.000003\n0.001414\n0.000002\n\n\n\n\n1929 rows × 968 columns\n\n\n\n\n\n1.2 Data standardization and train test split\nWe use the historical data to predict the future data. Therefore, we use lagged_cols as predictors.\n\n# Filter out the lagged columns.\nlagged_cols = [col for col in df.columns if '_day-' in col]\nprint(\"Input Features:\", lagged_cols)\n\n# Define target variable.\ntarget_var_regression = 'Close_ROC'\n\n# Define X and y.\nX = df[lagged_cols]\ny = df[target_var_regression]\n\n# Standardize features using StandardScaler.\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(\n    scaler.fit_transform(X),\n    columns=X.columns,\n    index=X.index\n)\n\n# Split the data into training and testing sets.\nsplit_ratio = 0.8  # 80% train data, 20% test data\nsplit_index = int(len(df) * split_ratio)\nX_train = X_scaled.iloc[:split_index]\nX_test = X_scaled.iloc[split_index:]\n\ny_train = y.iloc[:split_index]\ny_test = y.iloc[split_index:]\n\nprint(X_train)\n\nInput Features: ['Close_day-1', 'Close_day-1^2', 'Volume_day-1', 'Volume_day-1^2', 'mom_day-1', 'mom_day-1^2', 'mom1_day-1', 'mom1_day-1^2', 'mom2_day-1', 'mom2_day-1^2', 'mom3_day-1', 'mom3_day-1^2', 'ROC_5_day-1', 'ROC_5_day-1^2', 'ROC_10_day-1', 'ROC_10_day-1^2', 'ROC_15_day-1', 'ROC_15_day-1^2', 'ROC_20_day-1', 'ROC_20_day-1^2', 'EMA_10_day-1', 'EMA_10_day-1^2', 'EMA_20_day-1', 'EMA_20_day-1^2', 'EMA_50_day-1', 'EMA_50_day-1^2', 'DTB4WK_day-1', 'DTB4WK_day-1^2', 'DTB3_day-1', 'DTB3_day-1^2', 'DTB6_day-1', 'DTB6_day-1^2', 'DGS5_day-1', 'DGS5_day-1^2', 'DGS10_day-1', 'DGS10_day-1^2', 'Oil_day-1', 'Oil_day-1^2', 'Gold_day-1', 'Gold_day-1^2', 'DAAA_day-1', 'DAAA_day-1^2', 'DBAA_day-1', 'DBAA_day-1^2', 'GBP_day-1', 'GBP_day-1^2', 'JPY_day-1', 'JPY_day-1^2', 'CAD_day-1', 'CAD_day-1^2', 'CNY_day-1', 'CNY_day-1^2', 'AAPL_day-1', 'AAPL_day-1^2', 'AMZN_day-1', 'AMZN_day-1^2', 'GE_day-1', 'GE_day-1^2', 'JNJ_day-1', 'JNJ_day-1^2', 'JPM_day-1', 'JPM_day-1^2', 'MSFT_day-1', 'MSFT_day-1^2', 'WFC_day-1', 'WFC_day-1^2', 'XOM_day-1', 'XOM_day-1^2', 'FCHI_day-1', 'FCHI_day-1^2', 'FTSE_day-1', 'FTSE_day-1^2', 'GDAXI_day-1', 'GDAXI_day-1^2', 'GSPC_day-1', 'GSPC_day-1^2', 'HSI_day-1', 'HSI_day-1^2', 'DJI_day-1', 'DJI_day-1^2', 'SSEC_day-1', 'SSEC_day-1^2', 'RUT_day-1', 'RUT_day-1^2', 'NYSE_day-1', 'NYSE_day-1^2', 'TE1_day-1', 'TE1_day-1^2', 'TE2_day-1', 'TE2_day-1^2', 'TE3_day-1', 'TE3_day-1^2', 'TE5_day-1', 'TE5_day-1^2', 'TE6_day-1', 'TE6_day-1^2', 'DE1_day-1', 'DE1_day-1^2', 'DE2_day-1', 'DE2_day-1^2', 'DE4_day-1', 'DE4_day-1^2', 'DE5_day-1', 'DE5_day-1^2', 'DE6_day-1', 'DE6_day-1^2', 'CTB3M_day-1', 'CTB3M_day-1^2', 'CTB6M_day-1', 'CTB6M_day-1^2', 'CTB1Y_day-1', 'CTB1Y_day-1^2', 'AUD_day-1', 'AUD_day-1^2', 'Brent_day-1', 'Brent_day-1^2', 'CAC-F_day-1', 'CAC-F_day-1^2', 'copper-F_day-1', 'copper-F_day-1^2', 'WIT-oil_day-1', 'WIT-oil_day-1^2', 'DAX-F_day-1', 'DAX-F_day-1^2', 'DJI-F_day-1', 'DJI-F_day-1^2', 'EUR_day-1', 'EUR_day-1^2', 'FTSE-F_day-1', 'FTSE-F_day-1^2', 'gold-F_day-1', 'gold-F_day-1^2', 'HSI-F_day-1', 'HSI-F_day-1^2', 'KOSPI-F_day-1', 'KOSPI-F_day-1^2', 'NASDAQ-F_day-1', 'NASDAQ-F_day-1^2', 'GAS-F_day-1', 'GAS-F_day-1^2', 'Nikkei-F_day-1', 'Nikkei-F_day-1^2', 'NZD_day-1', 'NZD_day-1^2', 'silver-F_day-1', 'silver-F_day-1^2', 'RUSSELL-F_day-1', 'RUSSELL-F_day-1^2', 'S&P-F_day-1', 'S&P-F_day-1^2', 'CHF_day-1', 'CHF_day-1^2', 'Dollar index-F_day-1', 'Dollar index-F_day-1^2', 'Dollar index_day-1', 'Dollar index_day-1^2', 'wheat-F_day-1', 'wheat-F_day-1^2', 'XAG_day-1', 'XAG_day-1^2', 'XAU_day-1', 'XAU_day-1^2', 'day_of_week_day-1', 'day_of_week_day-1^2', 'day_of_month_day-1', 'day_of_month_day-1^2', 'month_day-1', 'month_day-1^2', 'Close_ROC_day-1', 'Close_ROC_day-1^2', 'EMA10_ROC_day-1', 'EMA10_ROC_day-1^2', 'EMA20_ROC_day-1', 'EMA20_ROC_day-1^2', 'EMA50_ROC_day-1', 'EMA50_ROC_day-1^2', 'Close_day-2', 'Close_day-2^2', 'Volume_day-2', 'Volume_day-2^2', 'mom_day-2', 'mom_day-2^2', 'mom1_day-2', 'mom1_day-2^2', 'mom2_day-2', 'mom2_day-2^2', 'mom3_day-2', 'mom3_day-2^2', 'ROC_5_day-2', 'ROC_5_day-2^2', 'ROC_10_day-2', 'ROC_10_day-2^2', 'ROC_15_day-2', 'ROC_15_day-2^2', 'ROC_20_day-2', 'ROC_20_day-2^2', 'EMA_10_day-2', 'EMA_10_day-2^2', 'EMA_20_day-2', 'EMA_20_day-2^2', 'EMA_50_day-2', 'EMA_50_day-2^2', 'DTB4WK_day-2', 'DTB4WK_day-2^2', 'DTB3_day-2', 'DTB3_day-2^2', 'DTB6_day-2', 'DTB6_day-2^2', 'DGS5_day-2', 'DGS5_day-2^2', 'DGS10_day-2', 'DGS10_day-2^2', 'Oil_day-2', 'Oil_day-2^2', 'Gold_day-2', 'Gold_day-2^2', 'DAAA_day-2', 'DAAA_day-2^2', 'DBAA_day-2', 'DBAA_day-2^2', 'GBP_day-2', 'GBP_day-2^2', 'JPY_day-2', 'JPY_day-2^2', 'CAD_day-2', 'CAD_day-2^2', 'CNY_day-2', 'CNY_day-2^2', 'AAPL_day-2', 'AAPL_day-2^2', 'AMZN_day-2', 'AMZN_day-2^2', 'GE_day-2', 'GE_day-2^2', 'JNJ_day-2', 'JNJ_day-2^2', 'JPM_day-2', 'JPM_day-2^2', 'MSFT_day-2', 'MSFT_day-2^2', 'WFC_day-2', 'WFC_day-2^2', 'XOM_day-2', 'XOM_day-2^2', 'FCHI_day-2', 'FCHI_day-2^2', 'FTSE_day-2', 'FTSE_day-2^2', 'GDAXI_day-2', 'GDAXI_day-2^2', 'GSPC_day-2', 'GSPC_day-2^2', 'HSI_day-2', 'HSI_day-2^2', 'DJI_day-2', 'DJI_day-2^2', 'SSEC_day-2', 'SSEC_day-2^2', 'RUT_day-2', 'RUT_day-2^2', 'NYSE_day-2', 'NYSE_day-2^2', 'TE1_day-2', 'TE1_day-2^2', 'TE2_day-2', 'TE2_day-2^2', 'TE3_day-2', 'TE3_day-2^2', 'TE5_day-2', 'TE5_day-2^2', 'TE6_day-2', 'TE6_day-2^2', 'DE1_day-2', 'DE1_day-2^2', 'DE2_day-2', 'DE2_day-2^2', 'DE4_day-2', 'DE4_day-2^2', 'DE5_day-2', 'DE5_day-2^2', 'DE6_day-2', 'DE6_day-2^2', 'CTB3M_day-2', 'CTB3M_day-2^2', 'CTB6M_day-2', 'CTB6M_day-2^2', 'CTB1Y_day-2', 'CTB1Y_day-2^2', 'AUD_day-2', 'AUD_day-2^2', 'Brent_day-2', 'Brent_day-2^2', 'CAC-F_day-2', 'CAC-F_day-2^2', 'copper-F_day-2', 'copper-F_day-2^2', 'WIT-oil_day-2', 'WIT-oil_day-2^2', 'DAX-F_day-2', 'DAX-F_day-2^2', 'DJI-F_day-2', 'DJI-F_day-2^2', 'EUR_day-2', 'EUR_day-2^2', 'FTSE-F_day-2', 'FTSE-F_day-2^2', 'gold-F_day-2', 'gold-F_day-2^2', 'HSI-F_day-2', 'HSI-F_day-2^2', 'KOSPI-F_day-2', 'KOSPI-F_day-2^2', 'NASDAQ-F_day-2', 'NASDAQ-F_day-2^2', 'GAS-F_day-2', 'GAS-F_day-2^2', 'Nikkei-F_day-2', 'Nikkei-F_day-2^2', 'NZD_day-2', 'NZD_day-2^2', 'silver-F_day-2', 'silver-F_day-2^2', 'RUSSELL-F_day-2', 'RUSSELL-F_day-2^2', 'S&P-F_day-2', 'S&P-F_day-2^2', 'CHF_day-2', 'CHF_day-2^2', 'Dollar index-F_day-2', 'Dollar index-F_day-2^2', 'Dollar index_day-2', 'Dollar index_day-2^2', 'wheat-F_day-2', 'wheat-F_day-2^2', 'XAG_day-2', 'XAG_day-2^2', 'XAU_day-2', 'XAU_day-2^2', 'day_of_week_day-2', 'day_of_week_day-2^2', 'day_of_month_day-2', 'day_of_month_day-2^2', 'month_day-2', 'month_day-2^2', 'Close_ROC_day-2', 'Close_ROC_day-2^2', 'EMA10_ROC_day-2', 'EMA10_ROC_day-2^2', 'EMA20_ROC_day-2', 'EMA20_ROC_day-2^2', 'EMA50_ROC_day-2', 'EMA50_ROC_day-2^2', 'Close_day-3', 'Close_day-3^2', 'Volume_day-3', 'Volume_day-3^2', 'mom_day-3', 'mom_day-3^2', 'mom1_day-3', 'mom1_day-3^2', 'mom2_day-3', 'mom2_day-3^2', 'mom3_day-3', 'mom3_day-3^2', 'ROC_5_day-3', 'ROC_5_day-3^2', 'ROC_10_day-3', 'ROC_10_day-3^2', 'ROC_15_day-3', 'ROC_15_day-3^2', 'ROC_20_day-3', 'ROC_20_day-3^2', 'EMA_10_day-3', 'EMA_10_day-3^2', 'EMA_20_day-3', 'EMA_20_day-3^2', 'EMA_50_day-3', 'EMA_50_day-3^2', 'DTB4WK_day-3', 'DTB4WK_day-3^2', 'DTB3_day-3', 'DTB3_day-3^2', 'DTB6_day-3', 'DTB6_day-3^2', 'DGS5_day-3', 'DGS5_day-3^2', 'DGS10_day-3', 'DGS10_day-3^2', 'Oil_day-3', 'Oil_day-3^2', 'Gold_day-3', 'Gold_day-3^2', 'DAAA_day-3', 'DAAA_day-3^2', 'DBAA_day-3', 'DBAA_day-3^2', 'GBP_day-3', 'GBP_day-3^2', 'JPY_day-3', 'JPY_day-3^2', 'CAD_day-3', 'CAD_day-3^2', 'CNY_day-3', 'CNY_day-3^2', 'AAPL_day-3', 'AAPL_day-3^2', 'AMZN_day-3', 'AMZN_day-3^2', 'GE_day-3', 'GE_day-3^2', 'JNJ_day-3', 'JNJ_day-3^2', 'JPM_day-3', 'JPM_day-3^2', 'MSFT_day-3', 'MSFT_day-3^2', 'WFC_day-3', 'WFC_day-3^2', 'XOM_day-3', 'XOM_day-3^2', 'FCHI_day-3', 'FCHI_day-3^2', 'FTSE_day-3', 'FTSE_day-3^2', 'GDAXI_day-3', 'GDAXI_day-3^2', 'GSPC_day-3', 'GSPC_day-3^2', 'HSI_day-3', 'HSI_day-3^2', 'DJI_day-3', 'DJI_day-3^2', 'SSEC_day-3', 'SSEC_day-3^2', 'RUT_day-3', 'RUT_day-3^2', 'NYSE_day-3', 'NYSE_day-3^2', 'TE1_day-3', 'TE1_day-3^2', 'TE2_day-3', 'TE2_day-3^2', 'TE3_day-3', 'TE3_day-3^2', 'TE5_day-3', 'TE5_day-3^2', 'TE6_day-3', 'TE6_day-3^2', 'DE1_day-3', 'DE1_day-3^2', 'DE2_day-3', 'DE2_day-3^2', 'DE4_day-3', 'DE4_day-3^2', 'DE5_day-3', 'DE5_day-3^2', 'DE6_day-3', 'DE6_day-3^2', 'CTB3M_day-3', 'CTB3M_day-3^2', 'CTB6M_day-3', 'CTB6M_day-3^2', 'CTB1Y_day-3', 'CTB1Y_day-3^2', 'AUD_day-3', 'AUD_day-3^2', 'Brent_day-3', 'Brent_day-3^2', 'CAC-F_day-3', 'CAC-F_day-3^2', 'copper-F_day-3', 'copper-F_day-3^2', 'WIT-oil_day-3', 'WIT-oil_day-3^2', 'DAX-F_day-3', 'DAX-F_day-3^2', 'DJI-F_day-3', 'DJI-F_day-3^2', 'EUR_day-3', 'EUR_day-3^2', 'FTSE-F_day-3', 'FTSE-F_day-3^2', 'gold-F_day-3', 'gold-F_day-3^2', 'HSI-F_day-3', 'HSI-F_day-3^2', 'KOSPI-F_day-3', 'KOSPI-F_day-3^2', 'NASDAQ-F_day-3', 'NASDAQ-F_day-3^2', 'GAS-F_day-3', 'GAS-F_day-3^2', 'Nikkei-F_day-3', 'Nikkei-F_day-3^2', 'NZD_day-3', 'NZD_day-3^2', 'silver-F_day-3', 'silver-F_day-3^2', 'RUSSELL-F_day-3', 'RUSSELL-F_day-3^2', 'S&P-F_day-3', 'S&P-F_day-3^2', 'CHF_day-3', 'CHF_day-3^2', 'Dollar index-F_day-3', 'Dollar index-F_day-3^2', 'Dollar index_day-3', 'Dollar index_day-3^2', 'wheat-F_day-3', 'wheat-F_day-3^2', 'XAG_day-3', 'XAG_day-3^2', 'XAU_day-3', 'XAU_day-3^2', 'day_of_week_day-3', 'day_of_week_day-3^2', 'day_of_month_day-3', 'day_of_month_day-3^2', 'month_day-3', 'month_day-3^2', 'Close_ROC_day-3', 'Close_ROC_day-3^2', 'EMA10_ROC_day-3', 'EMA10_ROC_day-3^2', 'EMA20_ROC_day-3', 'EMA20_ROC_day-3^2', 'EMA50_ROC_day-3', 'EMA50_ROC_day-3^2', 'Close_day-4', 'Close_day-4^2', 'Volume_day-4', 'Volume_day-4^2', 'mom_day-4', 'mom_day-4^2', 'mom1_day-4', 'mom1_day-4^2', 'mom2_day-4', 'mom2_day-4^2', 'mom3_day-4', 'mom3_day-4^2', 'ROC_5_day-4', 'ROC_5_day-4^2', 'ROC_10_day-4', 'ROC_10_day-4^2', 'ROC_15_day-4', 'ROC_15_day-4^2', 'ROC_20_day-4', 'ROC_20_day-4^2', 'EMA_10_day-4', 'EMA_10_day-4^2', 'EMA_20_day-4', 'EMA_20_day-4^2', 'EMA_50_day-4', 'EMA_50_day-4^2', 'DTB4WK_day-4', 'DTB4WK_day-4^2', 'DTB3_day-4', 'DTB3_day-4^2', 'DTB6_day-4', 'DTB6_day-4^2', 'DGS5_day-4', 'DGS5_day-4^2', 'DGS10_day-4', 'DGS10_day-4^2', 'Oil_day-4', 'Oil_day-4^2', 'Gold_day-4', 'Gold_day-4^2', 'DAAA_day-4', 'DAAA_day-4^2', 'DBAA_day-4', 'DBAA_day-4^2', 'GBP_day-4', 'GBP_day-4^2', 'JPY_day-4', 'JPY_day-4^2', 'CAD_day-4', 'CAD_day-4^2', 'CNY_day-4', 'CNY_day-4^2', 'AAPL_day-4', 'AAPL_day-4^2', 'AMZN_day-4', 'AMZN_day-4^2', 'GE_day-4', 'GE_day-4^2', 'JNJ_day-4', 'JNJ_day-4^2', 'JPM_day-4', 'JPM_day-4^2', 'MSFT_day-4', 'MSFT_day-4^2', 'WFC_day-4', 'WFC_day-4^2', 'XOM_day-4', 'XOM_day-4^2', 'FCHI_day-4', 'FCHI_day-4^2', 'FTSE_day-4', 'FTSE_day-4^2', 'GDAXI_day-4', 'GDAXI_day-4^2', 'GSPC_day-4', 'GSPC_day-4^2', 'HSI_day-4', 'HSI_day-4^2', 'DJI_day-4', 'DJI_day-4^2', 'SSEC_day-4', 'SSEC_day-4^2', 'RUT_day-4', 'RUT_day-4^2', 'NYSE_day-4', 'NYSE_day-4^2', 'TE1_day-4', 'TE1_day-4^2', 'TE2_day-4', 'TE2_day-4^2', 'TE3_day-4', 'TE3_day-4^2', 'TE5_day-4', 'TE5_day-4^2', 'TE6_day-4', 'TE6_day-4^2', 'DE1_day-4', 'DE1_day-4^2', 'DE2_day-4', 'DE2_day-4^2', 'DE4_day-4', 'DE4_day-4^2', 'DE5_day-4', 'DE5_day-4^2', 'DE6_day-4', 'DE6_day-4^2', 'CTB3M_day-4', 'CTB3M_day-4^2', 'CTB6M_day-4', 'CTB6M_day-4^2', 'CTB1Y_day-4', 'CTB1Y_day-4^2', 'AUD_day-4', 'AUD_day-4^2', 'Brent_day-4', 'Brent_day-4^2', 'CAC-F_day-4', 'CAC-F_day-4^2', 'copper-F_day-4', 'copper-F_day-4^2', 'WIT-oil_day-4', 'WIT-oil_day-4^2', 'DAX-F_day-4', 'DAX-F_day-4^2', 'DJI-F_day-4', 'DJI-F_day-4^2', 'EUR_day-4', 'EUR_day-4^2', 'FTSE-F_day-4', 'FTSE-F_day-4^2', 'gold-F_day-4', 'gold-F_day-4^2', 'HSI-F_day-4', 'HSI-F_day-4^2', 'KOSPI-F_day-4', 'KOSPI-F_day-4^2', 'NASDAQ-F_day-4', 'NASDAQ-F_day-4^2', 'GAS-F_day-4', 'GAS-F_day-4^2', 'Nikkei-F_day-4', 'Nikkei-F_day-4^2', 'NZD_day-4', 'NZD_day-4^2', 'silver-F_day-4', 'silver-F_day-4^2', 'RUSSELL-F_day-4', 'RUSSELL-F_day-4^2', 'S&P-F_day-4', 'S&P-F_day-4^2', 'CHF_day-4', 'CHF_day-4^2', 'Dollar index-F_day-4', 'Dollar index-F_day-4^2', 'Dollar index_day-4', 'Dollar index_day-4^2', 'wheat-F_day-4', 'wheat-F_day-4^2', 'XAG_day-4', 'XAG_day-4^2', 'XAU_day-4', 'XAU_day-4^2', 'day_of_week_day-4', 'day_of_week_day-4^2', 'day_of_month_day-4', 'day_of_month_day-4^2', 'month_day-4', 'month_day-4^2', 'Close_ROC_day-4', 'Close_ROC_day-4^2', 'EMA10_ROC_day-4', 'EMA10_ROC_day-4^2', 'EMA20_ROC_day-4', 'EMA20_ROC_day-4^2', 'EMA50_ROC_day-4', 'EMA50_ROC_day-4^2', 'Close_day-5', 'Close_day-5^2', 'Volume_day-5', 'Volume_day-5^2', 'mom_day-5', 'mom_day-5^2', 'mom1_day-5', 'mom1_day-5^2', 'mom2_day-5', 'mom2_day-5^2', 'mom3_day-5', 'mom3_day-5^2', 'ROC_5_day-5', 'ROC_5_day-5^2', 'ROC_10_day-5', 'ROC_10_day-5^2', 'ROC_15_day-5', 'ROC_15_day-5^2', 'ROC_20_day-5', 'ROC_20_day-5^2', 'EMA_10_day-5', 'EMA_10_day-5^2', 'EMA_20_day-5', 'EMA_20_day-5^2', 'EMA_50_day-5', 'EMA_50_day-5^2', 'DTB4WK_day-5', 'DTB4WK_day-5^2', 'DTB3_day-5', 'DTB3_day-5^2', 'DTB6_day-5', 'DTB6_day-5^2', 'DGS5_day-5', 'DGS5_day-5^2', 'DGS10_day-5', 'DGS10_day-5^2', 'Oil_day-5', 'Oil_day-5^2', 'Gold_day-5', 'Gold_day-5^2', 'DAAA_day-5', 'DAAA_day-5^2', 'DBAA_day-5', 'DBAA_day-5^2', 'GBP_day-5', 'GBP_day-5^2', 'JPY_day-5', 'JPY_day-5^2', 'CAD_day-5', 'CAD_day-5^2', 'CNY_day-5', 'CNY_day-5^2', 'AAPL_day-5', 'AAPL_day-5^2', 'AMZN_day-5', 'AMZN_day-5^2', 'GE_day-5', 'GE_day-5^2', 'JNJ_day-5', 'JNJ_day-5^2', 'JPM_day-5', 'JPM_day-5^2', 'MSFT_day-5', 'MSFT_day-5^2', 'WFC_day-5', 'WFC_day-5^2', 'XOM_day-5', 'XOM_day-5^2', 'FCHI_day-5', 'FCHI_day-5^2', 'FTSE_day-5', 'FTSE_day-5^2', 'GDAXI_day-5', 'GDAXI_day-5^2', 'GSPC_day-5', 'GSPC_day-5^2', 'HSI_day-5', 'HSI_day-5^2', 'DJI_day-5', 'DJI_day-5^2', 'SSEC_day-5', 'SSEC_day-5^2', 'RUT_day-5', 'RUT_day-5^2', 'NYSE_day-5', 'NYSE_day-5^2', 'TE1_day-5', 'TE1_day-5^2', 'TE2_day-5', 'TE2_day-5^2', 'TE3_day-5', 'TE3_day-5^2', 'TE5_day-5', 'TE5_day-5^2', 'TE6_day-5', 'TE6_day-5^2', 'DE1_day-5', 'DE1_day-5^2', 'DE2_day-5', 'DE2_day-5^2', 'DE4_day-5', 'DE4_day-5^2', 'DE5_day-5', 'DE5_day-5^2', 'DE6_day-5', 'DE6_day-5^2', 'CTB3M_day-5', 'CTB3M_day-5^2', 'CTB6M_day-5', 'CTB6M_day-5^2', 'CTB1Y_day-5', 'CTB1Y_day-5^2', 'AUD_day-5', 'AUD_day-5^2', 'Brent_day-5', 'Brent_day-5^2', 'CAC-F_day-5', 'CAC-F_day-5^2', 'copper-F_day-5', 'copper-F_day-5^2', 'WIT-oil_day-5', 'WIT-oil_day-5^2', 'DAX-F_day-5', 'DAX-F_day-5^2', 'DJI-F_day-5', 'DJI-F_day-5^2', 'EUR_day-5', 'EUR_day-5^2', 'FTSE-F_day-5', 'FTSE-F_day-5^2', 'gold-F_day-5', 'gold-F_day-5^2', 'HSI-F_day-5', 'HSI-F_day-5^2', 'KOSPI-F_day-5', 'KOSPI-F_day-5^2', 'NASDAQ-F_day-5', 'NASDAQ-F_day-5^2', 'GAS-F_day-5', 'GAS-F_day-5^2', 'Nikkei-F_day-5', 'Nikkei-F_day-5^2', 'NZD_day-5', 'NZD_day-5^2', 'silver-F_day-5', 'silver-F_day-5^2', 'RUSSELL-F_day-5', 'RUSSELL-F_day-5^2', 'S&P-F_day-5', 'S&P-F_day-5^2', 'CHF_day-5', 'CHF_day-5^2', 'Dollar index-F_day-5', 'Dollar index-F_day-5^2', 'Dollar index_day-5', 'Dollar index_day-5^2', 'wheat-F_day-5', 'wheat-F_day-5^2', 'XAG_day-5', 'XAG_day-5^2', 'XAU_day-5', 'XAU_day-5^2', 'day_of_week_day-5', 'day_of_week_day-5^2', 'day_of_month_day-5', 'day_of_month_day-5^2', 'month_day-5', 'month_day-5^2', 'Close_ROC_day-5', 'Close_ROC_day-5^2', 'EMA10_ROC_day-5', 'EMA10_ROC_day-5^2', 'EMA20_ROC_day-5', 'EMA20_ROC_day-5^2', 'EMA50_ROC_day-5', 'EMA50_ROC_day-5^2']\n      Close_day-1  Close_day-1^2  Volume_day-1  Volume_day-1^2  mom_day-1  \\\n55      -1.316708      -1.145535     -0.878617       -0.022459   0.779534   \n56      -1.300546      -1.136318     -0.048001       -0.062368   0.726812   \n57      -1.313971      -1.143979     -0.123872       -0.062276  -0.701704   \n58      -1.315071      -1.144605      0.369029       -0.050116  -0.109669   \n59      -1.316928      -1.145660     -0.578869       -0.046736  -0.146381   \n...           ...            ...           ...             ...        ...   \n1593     0.622093       0.502464      0.325335       -0.052412  -0.645461   \n1594     0.656495       0.541583     -0.918432       -0.018396   0.779839   \n1595     0.612203       0.491282      0.146735       -0.059333  -1.123691   \n1596     0.581589       0.456847     -0.144396       -0.062128  -0.802541   \n1597     0.574624       0.449051     -0.205371       -0.061382  -0.227548   \n\n      mom_day-1^2  mom1_day-1  mom1_day-1^2  mom2_day-1  mom2_day-1^2  ...  \\\n55      -0.130427   -0.723318     -0.239273    0.030443     -0.426815  ...   \n56      -0.167015    0.779653     -0.130513   -0.723315     -0.239273  ...   \n57      -0.251500    0.726937     -0.167101    0.779657     -0.130512  ...   \n58      -0.428728   -0.701426     -0.251588    0.726941     -0.167101  ...   \n59      -0.426476   -0.109455     -0.428822   -0.701423     -0.251588  ...   \n...           ...         ...           ...         ...           ...  ...   \n1593    -0.281251   -1.181046      0.112329   -0.542522     -0.328662  ...   \n1594    -0.130209   -0.645190     -0.281341   -1.181042      0.112330  ...   \n1595     0.058214    0.779958     -0.130294   -0.645186     -0.281340  ...   \n1596    -0.191370   -1.123368      0.058133    0.779961     -0.130294  ...   \n1597    -0.417395   -0.802253     -0.191457   -1.123365      0.058134  ...   \n\n      month_day-5  month_day-5^2  Close_ROC_day-5  Close_ROC_day-5^2  \\\n55      -1.069114      -1.012034         0.575377          -0.258634   \n56      -1.069114      -1.012034         0.383501          -0.346900   \n57      -1.069114      -1.012034         0.029610          -0.426891   \n58      -1.069114      -1.012034        -0.724125          -0.239345   \n59      -1.069114      -1.012034         0.778801          -0.130582   \n...           ...            ...              ...                ...   \n1593    -0.770184      -0.856133        -0.258263          -0.412757   \n1594    -0.770184      -0.856133        -0.202110          -0.421092   \n1595    -0.770184      -0.856133        -0.543337          -0.328736   \n1596    -0.770184      -0.856133        -1.181839           0.112266   \n1597    -0.770184      -0.856133        -0.645999          -0.281413   \n\n      EMA10_ROC_day-5  EMA10_ROC_day-5^2  EMA20_ROC_day-5  EMA20_ROC_day-5^2  \\\n55           0.993332           0.155614         1.348414           0.789463   \n56           1.056562           0.224377         1.411268           0.895477   \n57           0.882989           0.044257         1.289674           0.694041   \n58           0.263398          -0.376990         0.801326           0.037388   \n59           0.708346          -0.109536         1.113639           0.429215   \n...               ...                ...              ...                ...   \n1593        -0.478602          -0.425671        -0.168581          -0.543339   \n1594        -0.514958          -0.415289        -0.249065          -0.548287   \n1595        -0.757198          -0.315673        -0.488311          -0.523867   \n1596        -1.351179           0.152671        -1.013943          -0.264569   \n1597        -1.504202           0.324898        -1.227174          -0.078794   \n\n      EMA50_ROC_day-5  EMA50_ROC_day-5^2  \n55           1.368601           1.338713  \n56           1.454741           1.540091  \n57           1.407105           1.427586  \n58           1.084101           0.739215  \n59           1.325418           1.241235  \n...               ...                ...  \n1593         0.174005          -0.502153  \n1594         0.096821          -0.560023  \n1595        -0.098169          -0.673204  \n1596        -0.509592          -0.756795  \n1597        -0.713234          -0.720245  \n\n[1543 rows x 880 columns]\n\n\n\n\n1.3 Exploratory data visualization\n\n1.3.1 Dimension reduction method and probability density curves\nHere, we used UMAP and Probability Density Curves to visualize the train and test data.\nUMAP is a dimension reduction method by which we can visualize the high dimensional data in a 2-D graph.\nWe can see that the distribution of train and test data are different. Therefore, we assume that training a good model tends to be hard for this dataset.\n\n# Initialize the UMAP model (using a fixed random state for reproducibility).\numap_model = UMAP(n_components=2, random_state=42)\n\n# Fit UMAP on the training data and transform both training and test data.\nX_train_umap = umap_model.fit_transform(X_train)\nX_test_umap = umap_model.transform(X_test)\n\n# Plot the UMAP projection.\nplt.figure(figsize=(10, 8))\nplt.scatter(\n    X_train_umap[:, 0],\n    X_train_umap[:, 1],\n    c='blue',\n    label='Train',\n    alpha=0.6\n)\nplt.scatter(\n    X_test_umap[:, 0],\n    X_test_umap[:, 1],\n    c='red',\n    label='Test',\n    alpha=0.6\n)\nplt.xlabel('UMAP Dimension 1')\nplt.ylabel('UMAP Dimension 2')\nplt.title('UMAP Projection of Train and Test Data')\nplt.legend()\nplt.show()\n\nC:\\Users\\zln92\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\n\nsns.kdeplot(y_train, label=\"Train\", color=\"blue\", shade=True)\nsns.kdeplot(y_test, label=\"Test\", color=\"red\", shade=True)\n\nplt.xlabel(\"Close_ROC\")\nplt.ylabel(\"Density\")\nplt.title(\"Probability Density Curve for Close Rate of Change (Train vs Test)\")\nplt.legend()\n\nplt.show()\n\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3388851737.py:3: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(y_train, label=\"Train\", color=\"blue\", shade=True)\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nC:\\Users\\zln92\\AppData\\Local\\Temp\\ipykernel_9188\\3388851737.py:4: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(y_test, label=\"Test\", color=\"red\", shade=True)\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 Visualization of “Close_ROC”\nWe can also see it in another perspective. The daily rate of change of Nasdaq Composite Index looks like noise, which could be hard to predict.\n\nplt.figure(figsize=(12, 6))\nplt.plot(\n    df_raw[\"Date\"],\n    df_raw[\"Close_ROC\"],\n    label=\"True Close_ROC\",\n    linewidth=1\n)\n\nplt.title(\"Rate of Change of Nasdaq Index\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Close_ROC\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.4 Baseline model\nThe baseline model is just predict every Nasdaq index value on t day to be the value on t-1 day. It reflects the Martingale in probability theory. [3]\n\\[{\\displaystyle \\mathbf {E} (\\vert X_{n}\\vert )&lt;\\infty }\\] \\[\\mathbf {E} (X_{n+1}\\mid X_{1},\\ldots ,X_{n})=X_{n}.\\]\nSince we are predicting the value doesn’t change from t-1 day to t day, the predicted rate of change (‘Close_ROC’) of our baseline model is just 0.\nIn this project, we will use Root Mean Sqare Error (RMSE) to evaluate how the predicted value deviate from the true value in a more intuitive way.\n\n# Create baseline predictions (all zeros) for training and test sets.\nbaseline_train_predict = pd.Series(0, index=range(len(X_train)))\nbaseline_test_predict = pd.Series(0, index=range(len(X_test)))\n\n# Calculate Mean Squared Error for training and test sets.\nbaseline_mse_train_final = mean_squared_error(baseline_train_predict, y_train)\nbaseline_mse_test_final = mean_squared_error(baseline_test_predict, y_test)\n\n# Calculate Root Mean Squared Error.\nbaseline_rmse_train_final = np.sqrt(baseline_mse_train_final)\nbaseline_rmse_test_final = np.sqrt(baseline_mse_test_final)\n\n# Print the results.\nprint(\"Baseline Regression Model Train MSE:\", baseline_mse_train_final)\nprint(\"Baseline Regression Model Test MSE:\", baseline_mse_test_final)\nprint(\"Baseline Regression Model Train RMSE:\", baseline_rmse_train_final)\nprint(\"Baseline Regression Model Test RMSE:\", baseline_rmse_test_final)\n\nBaseline Regression Model Train MSE: 0.0001273147808231767\nBaseline Regression Model Test MSE: 5.152090781204474e-05\nBaseline Regression Model Train RMSE: 0.01128338516683609\nBaseline Regression Model Test RMSE: 0.00717780661567618"
  },
  {
    "objectID": "nasdaq-prediction.html#linear-regression",
    "href": "nasdaq-prediction.html#linear-regression",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "2. Linear Regression",
    "text": "2. Linear Regression\n\n2.1 Linear regression with all the features\nSince we have squared features, this is actually polynomial regression. We just treat them as independent features, so we call “linear regression” here.\n\n# Initialize and fit the linear regression model.\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Print training and testing MSE.\ntrain_mse = mean_squared_error(lin_reg.predict(X_train), y_train)\ntest_mse = mean_squared_error(lin_reg.predict(X_test), y_test)\n\nprint(\"Training mean_squared_error:\", train_mse)\nprint(\"Testing mean_squared_error:\", test_mse)\n\nTraining mean_squared_error: 4.631345421141122e-05\nTesting mean_squared_error: 0.02637718530839419\n\n\n\n# Output model parameters and score.\nprint(\"Model coefficients:\", lin_reg.coef_)\nprint(\"Model intercept:\", lin_reg.intercept_)\n\nModel coefficients: [ 2.62057358e+07 -2.96578327e+00 -4.13480215e-04 -2.72071920e-05\n -1.44017558e+05  1.08345848e+05 -3.83533363e+08 -1.91476076e+08\n  3.32686386e+08 -3.68066553e+07  1.42758759e+08 -1.02127726e+09\n  1.47720671e+00 -3.62809338e-02  2.81973891e-02  1.54078007e-04\n  1.03226490e-02  8.03850591e-04  3.86587158e-03 -7.49139115e-04\n -7.99382745e+07  3.06860472e+01 -9.05965750e+07 -4.54048501e+01\n -7.21136865e+07  3.83088106e+01 -4.45410220e+08  6.79812764e-02\n  8.55298351e+08 -9.78320895e-02 -5.46330650e+07  7.55610084e-02\n -2.95291096e-03  1.14880130e-02  3.76559227e+08  2.28153099e-01\n  2.10749358e-03  1.81356270e-03 -6.28710911e-03 -7.41733983e-03\n -7.72151243e+08 -6.47891592e-02 -3.65905154e+08 -7.72383958e-01\n -2.26842239e-04 -3.40396538e-04  7.73995183e-04 -2.21330673e-04\n -4.31597233e-04 -4.28985804e-04 -6.03104010e-04  4.24114987e-04\n -2.10508704e-04 -4.15660441e-04 -8.28200020e-04 -2.55010091e-04\n -1.24806305e-04 -2.48365104e-05 -8.67899507e-06 -9.38728452e-04\n -3.14038247e-04  6.48051500e-05 -1.09843910e-04  2.30763108e-04\n -6.44342974e-04  6.96144998e-05 -1.93879940e-04 -3.78049910e-04\n -9.99668613e-04  1.44044682e-03  2.07625888e-03 -7.32921064e-04\n  1.91627815e-03  1.51914731e-03  1.26789380e-02 -4.05019894e-03\n -4.49717045e-04  2.38418579e-06 -9.28389281e-03  2.46874616e-03\n  6.05840236e-04  3.62843275e-04  2.59920582e-03  8.25606287e-03\n -2.50494666e-03  6.75324351e-04 -4.96984245e+08  2.57943077e-02\n  1.41094127e+09  1.48746443e-01 -7.44916334e+08 -4.16972808e-01\n -3.66827576e+07 -4.96551488e-03 -1.09662973e+08  5.89082390e-03\n -3.31723571e+08 -2.32027322e-02  4.38763215e+08  1.42487660e-02\n -2.97501488e+08  1.08641377e+00  1.09941140e+08  1.35141216e-01\n  8.55251198e+08 -1.56077679e-01 -1.46519393e-03  3.91995534e-04\n -1.66719779e-03  6.44147396e-04  4.99680638e-04 -2.11019143e-02\n -5.02852723e-04 -4.82596457e-04  1.74440444e-04 -1.90682709e-04\n  1.51686370e-03  1.65253878e-04  7.86118209e-04  2.66898423e-05\n -2.18183175e-03 -3.47549096e-04 -2.00533681e-03 -2.17219070e-03\n  5.87402191e-03 -2.01880932e-03 -2.13204045e-03 -1.44053251e-04\n -1.85622275e-03 -3.54498625e-04  5.21294773e-04 -3.44254076e-04\n  9.89805907e-04  2.32450664e-04 -4.98671085e-04 -6.44958578e-04\n  9.33244824e-04  2.64229253e-03 -6.23449683e-04 -2.65441835e-04\n  1.49097294e-04 -1.91995874e-04 -3.00809741e-04  1.73951499e-04\n -6.03713095e-04 -9.93054360e-04 -3.84919345e-04 -4.43592109e-03\n -3.98005359e-03  4.17549163e-03  4.94679436e-04  3.55049036e-04\n -4.55371011e-03  1.43658184e-03  1.13740435e-03 -5.35298139e-04\n  9.96394083e-05  2.48168595e-04 -3.32668424e-05  1.34536251e-03\n  8.28970224e-04  3.19182873e-04 -9.38921375e-03  4.71939472e-03\n -2.79052742e-03  3.52782756e-03  4.36417758e-04  4.33474779e-05\n  1.44019503e+05 -1.08345880e+05 -7.07136685e+00  1.44147981e-01\n  4.69361988e+00 -1.11443514e-01 -1.50132220e+00  3.95782832e-02\n -7.58076859e+07  2.99460695e+00  3.75622651e-04 -7.50156119e-04\n  3.83694284e+08  1.91701349e+08 -9.18530914e+08  2.73091193e+08\n  3.62502912e+08  1.52348178e+08  4.77554878e+08 -5.35554408e+08\n -5.57312391e-01  1.40600875e-02  2.43721716e-02 -2.65014917e-03\n  8.74751061e-03 -1.63334794e-03  7.03550130e-03 -1.67050026e-03\n  3.48745010e+08 -4.99560316e+01  2.54087698e+08  4.90134665e+01\n  2.55770365e+08  2.61316518e+00 -1.38367010e+09 -5.43577196e-02\n  1.59453852e+09  1.00604813e-01 -2.50301275e+08 -1.56793077e-01\n  9.19553638e-03 -8.35467130e-03  5.02880167e+08 -1.80158225e-01\n -1.99749693e-03  1.80619014e-02 -1.44760683e-02 -1.41290780e-02\n -8.34795431e+08 -1.26528181e-02  7.60811340e+08  4.72815812e-01\n -4.32543457e-04 -5.92859462e-04  4.31454740e-04 -2.16336921e-04\n -3.31010669e-04 -2.15161592e-04 -1.63820572e-04 -8.88444483e-05\n  6.64774328e-04  2.55461782e-04  5.99021092e-04  1.55138317e-04\n  3.57009470e-04 -4.54422552e-04 -5.69013879e-04 -6.82063401e-04\n -5.65672293e-04  6.94105867e-04  2.25262716e-04 -3.78400087e-04\n -6.79403543e-04  1.48117542e-04  4.70077619e-04  1.46068633e-05\n -1.78450719e-03 -3.00404429e-03  2.74198316e-03 -4.17916104e-04\n -1.30403787e-04 -1.17681921e-04  1.23955309e-03  2.33393908e-03\n  7.98057765e-04 -5.55988401e-04 -1.97822601e-03  2.24778429e-03\n  9.08467919e-05 -7.57798553e-05  1.81981549e-03 -2.00450979e-03\n -3.15406546e-03 -5.66351973e-03  2.90933191e+08 -5.28128953e-02\n -3.21072634e+08 -6.03636376e-01 -1.83294853e+08  8.44726223e-01\n -3.47278482e+08  5.04561327e-03  5.61183222e+07 -3.77563387e-03\n -3.58220901e+08  5.76590672e-02  2.65414736e+08 -2.56042853e-02\n  9.34688393e+07 -1.64581435e+00 -8.73863724e+08  9.95124876e-01\n  5.14246105e+08  2.69232988e-02 -3.54088843e-05 -6.97188079e-06\n -1.39476359e-03  1.21091641e-02  3.05553898e-03  1.01378933e-03\n -1.20602548e-03 -1.00100413e-04  1.29752606e-03 -6.65001571e-05\n  2.63286754e-03  4.09756228e-03  1.64574012e-04  9.65138897e-05\n  2.85319984e-05 -2.78608873e-04 -7.69028440e-04  2.88311392e-04\n  3.05164233e-03  1.62302330e-03 -4.99407062e-04 -5.28521836e-04\n -1.76540762e-03  2.82354653e-04  1.61041319e-03 -5.39831817e-04\n -6.63969666e-04  1.62825733e-03 -3.86139378e-04 -4.97372821e-05\n -2.13696435e-03 -1.11355074e-03  2.92520970e-04 -2.71508470e-04\n  8.33291560e-05  5.51383011e-04  5.49517572e-05 -8.99387524e-04\n -1.92902237e-03  2.63471156e-04 -2.01787427e-03  5.47098927e-04\n  4.75287437e-03  2.32979655e-05 -1.61765143e-04 -4.68520448e-04\n  5.63289970e-04  2.76416540e-04 -1.97694823e-03  1.69471279e-03\n -9.46354121e-05 -1.76105648e-04  1.78915262e-03  3.23541462e-05\n  2.33054161e-05  4.70131636e-05  1.34656974e-02 -1.64756649e-02\n  6.21721335e-03 -6.68330677e-03  4.00772691e-03 -5.05124032e-03\n -1.60918669e+05 -2.25273099e+05  2.75560892e+00 -7.24322610e-02\n -8.04407997e+00  1.80345116e-01  4.39857134e+00 -1.05086368e-01\n -7.72841657e+07  2.67670179e+00 -7.79964030e-05  1.41989440e-04\n  5.85908410e+08 -2.36372760e+08 -3.88973875e+08  2.46040601e+08\n -7.93894042e+07 -7.93844439e+07  6.76234786e+07  5.30399462e+08\n -4.60655686e-01  9.54800285e-03  1.89510286e-02 -1.07686967e-04\n  8.87437165e-03  5.30703925e-04  1.09641999e-03 -9.89402644e-04\n  1.31660047e+08  2.26757506e+01 -1.74827360e+08 -9.69528705e+01\n  1.38989136e+08  9.37246974e+01  2.28047130e+08 -1.24002793e-02\n -4.51970514e+08  1.77573613e-02  3.50800373e+08 -1.09964549e-02\n  3.88414450e-02 -4.14879173e-02 -2.83458982e+08  2.31746739e-01\n  4.37937677e-04  8.14113766e-04 -1.63009055e-02 -1.71709713e-02\n  4.31237909e+07  7.53031261e-02 -1.14368990e+07 -2.09814034e-01\n -5.79290092e-04  3.48025933e-04  5.13102859e-05  4.69595194e-04\n  5.82609326e-04 -3.34728509e-04 -1.08151697e-04 -4.05339524e-04\n  1.70242041e-04  1.01359561e-04 -8.63801688e-05 -7.65771139e-04\n -2.49238685e-04  1.25181861e-04 -1.96946785e-04 -7.63842836e-05\n -5.63133508e-05  4.28296626e-05  2.79400498e-04 -5.25413547e-04\n -2.33098865e-04  4.47896309e-04 -7.36817718e-04  4.76781279e-04\n -2.96624377e-04 -3.17106256e-03  9.71117988e-04  1.11708045e-03\n -9.83774662e-04 -3.30153853e-04  9.23985988e-03 -9.73826088e-03\n  3.13572586e-04  2.31659040e-04 -9.07392614e-03  1.40720280e-03\n  7.20554963e-05 -3.00392509e-04 -6.78368099e-03 -4.56801988e-03\n  2.91998312e-03  2.39097327e-03 -1.79739846e+08  5.55952955e-02\n  3.44793685e+08  4.96307658e-02 -3.67253684e+08 -2.77184501e-01\n  8.50978318e+07  7.90213235e-04 -1.15281738e+08 -8.10250640e-05\n  1.84825199e+07 -1.49305463e-02 -3.72744568e+08 -4.25354913e-02\n  2.88947553e+08  5.66416565e-01 -3.05309134e+08 -2.32664082e-01\n  6.17188763e+08 -1.55461222e-01 -2.58110464e-04 -3.28820199e-04\n  7.00112432e-04 -4.75697964e-03  5.36713749e-04  8.85460898e-03\n  9.03290696e-04  5.27605414e-04 -5.18757850e-04 -1.93601474e-04\n -7.99672678e-04  3.87602113e-03  4.72556800e-04 -1.03782024e-03\n  5.65238297e-04  5.32228500e-04  2.93800607e-03 -1.04389060e-03\n  7.75189605e-03  2.39936728e-03 -2.26914417e-03 -2.87275761e-04\n -2.09900178e-03  2.94567086e-04  1.32662058e-03 -1.70119107e-04\n -1.07938051e-03 -6.77078962e-04  4.40768898e-04 -9.54102725e-05\n -5.07216901e-04 -1.51727814e-03 -1.06364489e-04  6.56111166e-04\n  1.06427819e-04  6.80577010e-04 -3.14056873e-04  1.09440088e-03\n -2.06752494e-03 -8.77022743e-04  5.26526477e-03  2.11834721e-03\n -6.47418015e-03 -2.78832531e-03 -7.00398348e-04 -7.10269436e-04\n -1.61170959e-04  3.08081508e-06 -1.86529569e-03 -4.44613397e-05\n  1.09211542e-05 -3.22155654e-04  2.04877555e-03  3.93580645e-04\n -5.79819083e-04  6.46566972e-04 -4.12782910e-03  5.99174108e-03\n -7.61861354e-03  8.57660547e-03  3.62806022e-03 -2.84300745e-03\n -6.38821926e+04  8.82228909e+04  3.58049703e+00 -7.42405821e-02\n  2.38376074e-01 -2.03442611e-02 -3.09107400e-01  1.90193057e-02\n -2.82500000e+07 -2.94284438e+00  3.62894498e-04  2.31517479e-04\n -1.16249588e+08  6.22557977e+08 -1.94127721e+08  2.88146497e+08\n -4.39620157e+08  1.91783593e+08  2.82384558e+08  9.13197426e+07\n  5.64981140e-01 -1.64119219e-02  1.45581588e-02 -5.27389348e-05\n  6.34174049e-03 -3.42745613e-03 -3.35102901e-03  2.65965424e-03\n -2.69673272e+08  1.03001527e+01  1.98795357e+08  1.47285370e+02\n -1.60457035e+08 -2.63196674e+02 -3.92276175e+08  1.61441274e-02\n  1.01167300e+08 -3.73200870e-02  1.95694508e+07  3.18813231e-02\n -1.92042366e-02  2.18161196e-02  2.84032488e+08 -4.77510955e-01\n -3.11491638e-03  5.95536875e-03 -4.04088572e-03 -3.38539761e-03\n  3.05973037e+08 -5.74705862e-02  1.11678769e+08  9.90952712e-01\n -1.71770342e-04 -1.89806800e-04 -1.15001574e-04  4.02650796e-04\n -2.91088596e-04 -3.32625583e-04 -8.56917351e-04  8.16769898e-04\n  4.90834936e-04 -4.63884324e-04  3.31852585e-04 -4.96966764e-04\n  4.19577584e-04 -5.63133508e-04  4.18039039e-04  1.61416829e-05\n  1.28729828e-03 -4.40614298e-04  3.68189067e-05 -4.76390123e-05\n -1.54921599e-03  1.14366226e-03  1.04306638e-03 -9.47734341e-04\n -3.92310321e-05 -4.48897481e-05  3.08420137e-03  1.43991783e-05\n -1.68686360e-03  4.35502268e-04 -6.24807179e-03 -1.90264732e-03\n -3.23191285e-04  2.48501077e-04  4.34424914e-03  6.84682280e-04\n  7.95170199e-04 -1.95369124e-04  6.84965402e-04  5.83965331e-04\n -2.46786140e-03  4.89757676e-03 -5.18328132e+08  1.09250715e-01\n  1.79601207e+08  3.62519262e-01 -2.47009983e+08  5.18836062e-02\n -4.17491373e+07 -1.04687922e-03 -1.85779460e+07  1.77665055e-03\n  1.32070162e+08 -9.09690559e-03 -1.88515923e+08 -1.85406953e-03\n  1.76285228e+08 -2.36321699e-01 -6.42166704e+08 -3.49990245e-01\n  2.64162880e+08 -5.42632584e-01 -4.04356048e-04  6.41353428e-04\n  1.96583569e-03 -1.00218542e-02 -2.08716840e-04  1.75200775e-02\n -5.42805530e-04  2.86944211e-04  1.42880157e-03  7.07386993e-04\n  4.97370958e-04 -4.43143770e-04  1.90753490e-04  4.05076891e-04\n  9.93933529e-04 -7.85485841e-04  1.24306977e-03  1.09615736e-04\n -3.57621815e-03 -2.88124196e-04  2.58515589e-04 -6.89113513e-04\n -3.31896730e-03  7.39726238e-05 -5.70341945e-04 -2.81434506e-04\n -2.98067927e-04  2.85591930e-04  5.95809892e-04 -1.63037702e-03\n -2.09925510e-03  7.47689977e-04  7.10783526e-04 -3.43123917e-04\n  4.92839143e-04  3.08316201e-04  4.26768325e-04  9.22667794e-04\n -1.99169666e-03  8.67165625e-04  8.94172117e-04 -1.17521919e-03\n  5.26542775e-03 -9.59337689e-04  3.52272764e-04  6.52877148e-04\n -1.61186606e-03  6.81344420e-04  1.19907595e-03 -4.04238701e-04\n -1.18480530e-04 -3.55364755e-04  1.24654919e-03 -3.22546810e-04\n  1.86721236e-03 -1.33803114e-04  8.55335407e-03 -7.87111837e-03\n  9.74478573e-03 -9.66214202e-03  9.27622616e-03 -1.15540028e-02\n -3.82076496e+04  3.30500619e+05 -3.28309132e+00  6.62801974e-02\n  6.12602887e+00 -1.25332962e-01 -3.85327780e+00  8.61278027e-02\n  4.61519744e-01 -3.01127065e-01  5.62573317e-04 -7.04266131e-05\n -2.03795091e+08  3.26931383e+08  3.71996679e+08 -7.22183056e+08\n -2.82384558e+08 -9.13197426e+07 -2.45863415e-01  1.87298656e-03\n  2.38748454e-02  1.12282299e-03  1.04319006e-02 -9.96150076e-04\n  3.41490656e-03  3.06953676e-03  8.38867947e-03 -1.46391243e-03\n -2.23602878e+07 -1.02882111e+01 -1.64056761e+08 -5.94880942e+01\n -1.39140906e+08  1.31203108e+02  2.38022836e+07  3.93006045e-02\n  7.42691152e+06  1.08493103e-02  4.51356170e+07  4.94066365e-02\n -1.08511001e-02  4.10205126e-03  1.91970002e+08  2.61775924e-01\n -9.43323597e-04  4.52343503e-03 -6.39970042e-03 -5.78400958e-03\n  1.56734623e+08  6.66545406e-02 -5.91931049e+08 -4.00250580e-01\n  4.24778089e-04  5.40474430e-05  5.28376549e-05 -2.54702754e-04\n  3.04075889e-04 -2.34063715e-04  1.62553042e-04 -3.27820890e-04\n -4.48601320e-04 -7.36340880e-05  3.05745751e-04  1.33980066e-05\n  5.41724265e-04 -5.16455621e-04  3.23073938e-04  1.71296299e-04\n  4.40615695e-04 -4.31347638e-04 -7.10375607e-05 -3.19455750e-04\n -7.05904327e-04  9.96874645e-04 -6.13827258e-04  6.39501959e-04\n -5.39699569e-04 -7.07104802e-04  1.06311589e-03 -2.10737064e-03\n -1.19427405e-03  2.00462714e-03 -2.93325726e-03 -4.14118171e-04\n  4.14527953e-04  3.97972763e-04  8.52432102e-04 -4.15523723e-03\n -2.51466408e-04 -3.91051173e-04 -4.37567011e-04 -7.49830157e-04\n  3.72385047e-03  3.94434109e-03  3.64448997e+08 -7.99464416e-02\n  3.65861028e+08 -1.88737954e-01 -4.74268066e+08 -7.55061917e-02\n  6.78573043e+07  5.66391274e-04 -1.01895521e+08 -3.61134112e-03\n  6.75782746e+07 -4.83452156e-03  3.44988389e+08  3.84124294e-02\n -2.87154506e+08 -3.26753877e-01  7.33240256e+08  2.38411695e-01\n -5.63321193e+08  4.87546895e-01  1.74969807e-03 -3.10759991e-03\n -1.04537606e-03 -9.20918584e-03  1.68327987e-03  1.21587068e-02\n -1.62206125e-03 -3.72342765e-04  6.29968941e-04  9.71682370e-04\n -2.03307718e-04  7.73645937e-04 -9.34612006e-04  6.97683543e-04\n -4.10247594e-04 -1.08621828e-03  1.33115798e-04 -1.05115399e-03\n -3.01816501e-04  2.09816732e-03  7.41318567e-04  6.00114465e-04\n -6.33637421e-04  8.00475478e-04 -1.63827091e-04  2.63186172e-04\n -3.70287336e-04 -5.57564199e-04  4.41348180e-04  8.45052302e-04\n -1.06884167e-03  1.00009702e-03 -2.66503543e-04  7.26211816e-04\n  3.33329663e-04  3.85753810e-05  1.17437262e-03 -1.31708384e-03\n  8.22907314e-04 -8.98305327e-04 -1.73187815e-03  1.55647844e-03\n  2.12565064e-04 -6.05812296e-03  9.02129337e-04  2.30273232e-04\n  3.51615250e-04  3.25694680e-04 -3.63683328e-04 -9.16386954e-04\n -3.43021937e-04  5.69811091e-05  3.79960984e-04  4.59177420e-04\n -1.17439777e-04  2.15541571e-04 -1.45976511e-02  1.60959316e-02\n -4.56042588e-03  4.34851833e-03 -1.94329768e-02  2.18475163e-02\n -2.42662771e+05 -1.39027815e+05  1.69911600e+00 -4.18187603e-02\n -2.89512119e+00  6.85486030e-02  1.36955070e+00 -3.46447825e-02]\nModel intercept: 0.004142097607368128\n\n\n\n\n2.2 Feature selection for linear regression: Recursive Feature Elimination (RFE)\nFrom above, we can notice that the overfitting occurs. To solve the overfitting, we will try to use fewer features.\nSince there are there are too many features, it is too computational expensive to try all the combinations of features. Therefore, we need to select the most important features. We implement Recursive Feature Elimination (RFE) to select the most important features.\n\n2.2.1 Visualization of feature importance\n\n# Assuming `X.columns` contains the feature names and `lin_reg.coef_` contains the coefficients.\ndf_coef = pd.DataFrame({\n    'feature': X.columns,\n    'coef': np.abs(lin_reg.coef_)  # Taking absolute values of coefficients\n})\n\ndf_coef['day'] = df_coef['feature'].str.extract(r'day-(\\d+)').astype(float)\ndf_coef['base_feature'] = df_coef['feature'].str.replace(\n    r'_day-\\d+(?=\\^2)|_day-\\d+',\n    '',\n    regex=True\n)\n\n# Group by 'base_feature' and 'day' to ensure uniqueness.\ndf_coef_grouped = df_coef.groupby(['base_feature', 'day'])['coef'].mean().unstack()\n\n# Plot heatmap.\nplt.figure(figsize=(10, 30))\nplt.imshow(\n    df_coef_grouped,\n    aspect='auto',\n    cmap='viridis',\n    interpolation='nearest'\n)\nplt.colorbar(label=\"Absolute Coefficient Value\")\nplt.xticks(\n    ticks=np.arange(len(df_coef_grouped.columns)),\n    labels=df_coef_grouped.columns,\n    rotation=45\n)\nplt.yticks(\n    ticks=np.arange(len(df_coef_grouped.index)),\n    labels=df_coef_grouped.index\n)\nplt.xlabel(\"Day Lag\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance Heatmap (Absolute Coefficients)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Since the 'LinearRegression' object doesn't have 'feature_names_in_', manually retrieve column names.\ndf_coef = pd.DataFrame({\n    'feature': X.columns,\n    'coef': lin_reg.coef_\n})\n\nchart = alt.Chart(df_coef).mark_bar().encode(\n    x='coef',\n    y=alt.Y('feature', sort='-x')  # Sort by descending coefficient values.\n).properties(\n    title='Coefficients'\n)\n\nchart\n\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\altair\\utils\\core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\n\n\n2.2.2 RFE Cross-Validation\nWe used RFECV to do RFE and Cross-Validation (CV) at the same time.\n\n# Time Series Cross-Validation.\ntscv = TimeSeriesSplit(n_splits=3)\n\nstep_lin_reg = 1\n\n# Initialize RFECV.\nselector_lin_reg_cv = RFECV(\n    estimator=LinearRegression(n_jobs=-1),\n    step=step_lin_reg,  # Remove 1 feature at each step.\n    cv=tscv,          # Time series split.\n    verbose=1,\n    scoring='neg_mean_squared_error'  # Negative MSE for scoring.\n)\n\n# Train and automatically select the optimal number of features.\nselector_lin_reg_cv.fit(X_train, y_train)\n\n# Save the best model for later use.\nbest_model_reg_final = selector_lin_reg_cv.estimator\n\nFitting estimator with 880 features.\nFitting estimator with 879 features.\nFitting estimator with 878 features.\nFitting estimator with 877 features.\nFitting estimator with 876 features.\nFitting estimator with 875 features.\nFitting estimator with 874 features.\nFitting estimator with 873 features.\nFitting estimator with 872 features.\nFitting estimator with 871 features.\nFitting estimator with 870 features.\nFitting estimator with 869 features.\nFitting estimator with 868 features.\nFitting estimator with 867 features.\nFitting estimator with 866 features.\nFitting estimator with 865 features.\nFitting estimator with 864 features.\nFitting estimator with 863 features.\nFitting estimator with 862 features.\nFitting estimator with 861 features.\nFitting estimator with 860 features.\nFitting estimator with 859 features.\nFitting estimator with 858 features.\nFitting estimator with 857 features.\nFitting estimator with 856 features.\nFitting estimator with 855 features.\nFitting estimator with 854 features.\nFitting estimator with 853 features.\nFitting estimator with 852 features.\nFitting estimator with 851 features.\nFitting estimator with 850 features.\nFitting estimator with 849 features.\nFitting estimator with 848 features.\nFitting estimator with 847 features.\nFitting estimator with 846 features.\nFitting estimator with 845 features.\nFitting estimator with 844 features.\nFitting estimator with 843 features.\nFitting estimator with 842 features.\nFitting estimator with 841 features.\nFitting estimator with 840 features.\nFitting estimator with 839 features.\nFitting estimator with 838 features.\nFitting estimator with 837 features.\nFitting estimator with 836 features.\nFitting estimator with 835 features.\nFitting estimator with 834 features.\nFitting estimator with 833 features.\nFitting estimator with 832 features.\nFitting estimator with 831 features.\nFitting estimator with 830 features.\nFitting estimator with 829 features.\nFitting estimator with 828 features.\nFitting estimator with 827 features.\nFitting estimator with 826 features.\nFitting estimator with 825 features.\nFitting estimator with 824 features.\nFitting estimator with 823 features.\nFitting estimator with 822 features.\nFitting estimator with 821 features.\nFitting estimator with 820 features.\nFitting estimator with 819 features.\nFitting estimator with 818 features.\nFitting estimator with 817 features.\nFitting estimator with 816 features.\nFitting estimator with 815 features.\nFitting estimator with 814 features.\nFitting estimator with 813 features.\nFitting estimator with 812 features.\nFitting estimator with 811 features.\nFitting estimator with 810 features.\nFitting estimator with 809 features.\nFitting estimator with 808 features.\nFitting estimator with 807 features.\nFitting estimator with 806 features.\nFitting estimator with 805 features.\nFitting estimator with 804 features.\nFitting estimator with 803 features.\nFitting estimator with 802 features.\nFitting estimator with 801 features.\nFitting estimator with 800 features.\nFitting estimator with 799 features.\nFitting estimator with 798 features.\nFitting estimator with 797 features.\nFitting estimator with 796 features.\nFitting estimator with 795 features.\nFitting estimator with 794 features.\nFitting estimator with 793 features.\nFitting estimator with 792 features.\nFitting estimator with 791 features.\nFitting estimator with 790 features.\nFitting estimator with 789 features.\nFitting estimator with 788 features.\nFitting estimator with 787 features.\nFitting estimator with 786 features.\nFitting estimator with 785 features.\nFitting estimator with 784 features.\nFitting estimator with 783 features.\nFitting estimator with 782 features.\nFitting estimator with 781 features.\nFitting estimator with 780 features.\nFitting estimator with 779 features.\nFitting estimator with 778 features.\nFitting estimator with 777 features.\nFitting estimator with 776 features.\nFitting estimator with 775 features.\nFitting estimator with 774 features.\nFitting estimator with 773 features.\nFitting estimator with 772 features.\nFitting estimator with 771 features.\nFitting estimator with 770 features.\nFitting estimator with 769 features.\nFitting estimator with 768 features.\nFitting estimator with 767 features.\nFitting estimator with 766 features.\nFitting estimator with 765 features.\nFitting estimator with 764 features.\nFitting estimator with 763 features.\nFitting estimator with 762 features.\nFitting estimator with 761 features.\nFitting estimator with 760 features.\nFitting estimator with 759 features.\nFitting estimator with 758 features.\nFitting estimator with 757 features.\nFitting estimator with 756 features.\nFitting estimator with 755 features.\nFitting estimator with 754 features.\nFitting estimator with 753 features.\nFitting estimator with 752 features.\nFitting estimator with 751 features.\nFitting estimator with 750 features.\nFitting estimator with 749 features.\nFitting estimator with 748 features.\nFitting estimator with 747 features.\nFitting estimator with 746 features.\nFitting estimator with 745 features.\nFitting estimator with 744 features.\nFitting estimator with 743 features.\nFitting estimator with 742 features.\nFitting estimator with 741 features.\nFitting estimator with 740 features.\nFitting estimator with 739 features.\nFitting estimator with 738 features.\nFitting estimator with 737 features.\nFitting estimator with 736 features.\nFitting estimator with 735 features.\nFitting estimator with 734 features.\nFitting estimator with 733 features.\nFitting estimator with 732 features.\nFitting estimator with 731 features.\nFitting estimator with 730 features.\nFitting estimator with 729 features.\nFitting estimator with 728 features.\nFitting estimator with 727 features.\nFitting estimator with 726 features.\nFitting estimator with 725 features.\nFitting estimator with 724 features.\nFitting estimator with 723 features.\nFitting estimator with 722 features.\nFitting estimator with 721 features.\nFitting estimator with 720 features.\nFitting estimator with 719 features.\nFitting estimator with 718 features.\nFitting estimator with 717 features.\nFitting estimator with 716 features.\nFitting estimator with 715 features.\nFitting estimator with 714 features.\nFitting estimator with 713 features.\nFitting estimator with 712 features.\nFitting estimator with 711 features.\nFitting estimator with 710 features.\nFitting estimator with 709 features.\nFitting estimator with 708 features.\nFitting estimator with 707 features.\nFitting estimator with 706 features.\nFitting estimator with 705 features.\nFitting estimator with 704 features.\nFitting estimator with 703 features.\nFitting estimator with 702 features.\nFitting estimator with 701 features.\nFitting estimator with 700 features.\nFitting estimator with 699 features.\nFitting estimator with 698 features.\nFitting estimator with 697 features.\nFitting estimator with 696 features.\nFitting estimator with 695 features.\nFitting estimator with 694 features.\nFitting estimator with 693 features.\nFitting estimator with 692 features.\nFitting estimator with 691 features.\nFitting estimator with 690 features.\nFitting estimator with 689 features.\nFitting estimator with 688 features.\nFitting estimator with 687 features.\nFitting estimator with 686 features.\nFitting estimator with 685 features.\nFitting estimator with 684 features.\nFitting estimator with 683 features.\nFitting estimator with 682 features.\nFitting estimator with 681 features.\nFitting estimator with 680 features.\nFitting estimator with 679 features.\nFitting estimator with 678 features.\nFitting estimator with 677 features.\nFitting estimator with 676 features.\nFitting estimator with 675 features.\nFitting estimator with 674 features.\nFitting estimator with 673 features.\nFitting estimator with 672 features.\nFitting estimator with 671 features.\nFitting estimator with 670 features.\nFitting estimator with 669 features.\nFitting estimator with 668 features.\nFitting estimator with 667 features.\nFitting estimator with 666 features.\nFitting estimator with 665 features.\nFitting estimator with 664 features.\nFitting estimator with 663 features.\nFitting estimator with 662 features.\nFitting estimator with 661 features.\nFitting estimator with 660 features.\nFitting estimator with 659 features.\nFitting estimator with 658 features.\nFitting estimator with 657 features.\nFitting estimator with 656 features.\nFitting estimator with 655 features.\nFitting estimator with 654 features.\nFitting estimator with 653 features.\nFitting estimator with 652 features.\nFitting estimator with 651 features.\nFitting estimator with 650 features.\nFitting estimator with 649 features.\nFitting estimator with 648 features.\nFitting estimator with 647 features.\nFitting estimator with 646 features.\nFitting estimator with 645 features.\nFitting estimator with 644 features.\nFitting estimator with 643 features.\nFitting estimator with 642 features.\nFitting estimator with 641 features.\nFitting estimator with 640 features.\nFitting estimator with 639 features.\nFitting estimator with 638 features.\nFitting estimator with 637 features.\nFitting estimator with 636 features.\nFitting estimator with 635 features.\nFitting estimator with 634 features.\nFitting estimator with 633 features.\nFitting estimator with 632 features.\nFitting estimator with 631 features.\nFitting estimator with 630 features.\nFitting estimator with 629 features.\nFitting estimator with 628 features.\nFitting estimator with 627 features.\nFitting estimator with 626 features.\nFitting estimator with 625 features.\nFitting estimator with 624 features.\nFitting estimator with 623 features.\nFitting estimator with 622 features.\nFitting estimator with 621 features.\nFitting estimator with 620 features.\nFitting estimator with 619 features.\nFitting estimator with 618 features.\nFitting estimator with 617 features.\nFitting estimator with 616 features.\nFitting estimator with 615 features.\nFitting estimator with 614 features.\nFitting estimator with 613 features.\nFitting estimator with 612 features.\nFitting estimator with 611 features.\nFitting estimator with 610 features.\nFitting estimator with 609 features.\nFitting estimator with 608 features.\nFitting estimator with 607 features.\nFitting estimator with 606 features.\nFitting estimator with 605 features.\nFitting estimator with 604 features.\nFitting estimator with 603 features.\nFitting estimator with 602 features.\nFitting estimator with 601 features.\nFitting estimator with 600 features.\nFitting estimator with 599 features.\nFitting estimator with 598 features.\nFitting estimator with 597 features.\nFitting estimator with 596 features.\nFitting estimator with 595 features.\nFitting estimator with 594 features.\nFitting estimator with 593 features.\nFitting estimator with 592 features.\nFitting estimator with 591 features.\nFitting estimator with 590 features.\nFitting estimator with 589 features.\nFitting estimator with 588 features.\nFitting estimator with 587 features.\nFitting estimator with 586 features.\nFitting estimator with 585 features.\nFitting estimator with 584 features.\nFitting estimator with 583 features.\nFitting estimator with 582 features.\nFitting estimator with 581 features.\nFitting estimator with 580 features.\nFitting estimator with 579 features.\nFitting estimator with 578 features.\nFitting estimator with 577 features.\nFitting estimator with 576 features.\nFitting estimator with 575 features.\nFitting estimator with 574 features.\nFitting estimator with 573 features.\nFitting estimator with 572 features.\nFitting estimator with 571 features.\nFitting estimator with 570 features.\nFitting estimator with 569 features.\nFitting estimator with 568 features.\nFitting estimator with 567 features.\nFitting estimator with 566 features.\nFitting estimator with 565 features.\nFitting estimator with 564 features.\nFitting estimator with 563 features.\nFitting estimator with 562 features.\nFitting estimator with 561 features.\nFitting estimator with 560 features.\nFitting estimator with 559 features.\nFitting estimator with 558 features.\nFitting estimator with 557 features.\nFitting estimator with 556 features.\nFitting estimator with 555 features.\nFitting estimator with 554 features.\nFitting estimator with 553 features.\nFitting estimator with 552 features.\nFitting estimator with 551 features.\nFitting estimator with 550 features.\nFitting estimator with 549 features.\nFitting estimator with 548 features.\nFitting estimator with 547 features.\nFitting estimator with 546 features.\nFitting estimator with 545 features.\nFitting estimator with 544 features.\nFitting estimator with 543 features.\nFitting estimator with 542 features.\nFitting estimator with 541 features.\nFitting estimator with 540 features.\nFitting estimator with 539 features.\nFitting estimator with 538 features.\nFitting estimator with 537 features.\nFitting estimator with 536 features.\nFitting estimator with 535 features.\nFitting estimator with 534 features.\nFitting estimator with 533 features.\nFitting estimator with 532 features.\nFitting estimator with 531 features.\nFitting estimator with 530 features.\nFitting estimator with 529 features.\nFitting estimator with 528 features.\nFitting estimator with 527 features.\nFitting estimator with 526 features.\nFitting estimator with 525 features.\nFitting estimator with 524 features.\nFitting estimator with 523 features.\nFitting estimator with 522 features.\nFitting estimator with 521 features.\nFitting estimator with 520 features.\nFitting estimator with 519 features.\nFitting estimator with 518 features.\nFitting estimator with 517 features.\nFitting estimator with 516 features.\nFitting estimator with 515 features.\nFitting estimator with 514 features.\nFitting estimator with 513 features.\nFitting estimator with 512 features.\nFitting estimator with 511 features.\nFitting estimator with 510 features.\nFitting estimator with 509 features.\nFitting estimator with 508 features.\nFitting estimator with 507 features.\nFitting estimator with 506 features.\nFitting estimator with 505 features.\nFitting estimator with 504 features.\nFitting estimator with 503 features.\nFitting estimator with 502 features.\nFitting estimator with 501 features.\nFitting estimator with 500 features.\nFitting estimator with 499 features.\nFitting estimator with 498 features.\nFitting estimator with 497 features.\nFitting estimator with 496 features.\nFitting estimator with 495 features.\nFitting estimator with 494 features.\nFitting estimator with 493 features.\nFitting estimator with 492 features.\nFitting estimator with 491 features.\nFitting estimator with 490 features.\nFitting estimator with 489 features.\nFitting estimator with 488 features.\nFitting estimator with 487 features.\nFitting estimator with 486 features.\nFitting estimator with 485 features.\nFitting estimator with 484 features.\nFitting estimator with 483 features.\nFitting estimator with 482 features.\nFitting estimator with 481 features.\nFitting estimator with 480 features.\nFitting estimator with 479 features.\nFitting estimator with 478 features.\nFitting estimator with 477 features.\nFitting estimator with 476 features.\nFitting estimator with 475 features.\nFitting estimator with 474 features.\nFitting estimator with 473 features.\nFitting estimator with 472 features.\nFitting estimator with 471 features.\nFitting estimator with 470 features.\nFitting estimator with 469 features.\nFitting estimator with 468 features.\nFitting estimator with 467 features.\nFitting estimator with 466 features.\nFitting estimator with 465 features.\nFitting estimator with 464 features.\nFitting estimator with 463 features.\nFitting estimator with 462 features.\nFitting estimator with 461 features.\nFitting estimator with 460 features.\nFitting estimator with 459 features.\nFitting estimator with 458 features.\nFitting estimator with 457 features.\nFitting estimator with 456 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\nFitting estimator with 880 features.\nFitting estimator with 879 features.\nFitting estimator with 878 features.\nFitting estimator with 877 features.\nFitting estimator with 876 features.\nFitting estimator with 875 features.\nFitting estimator with 874 features.\nFitting estimator with 873 features.\nFitting estimator with 872 features.\nFitting estimator with 871 features.\nFitting estimator with 870 features.\nFitting estimator with 869 features.\nFitting estimator with 868 features.\nFitting estimator with 867 features.\nFitting estimator with 866 features.\nFitting estimator with 865 features.\nFitting estimator with 864 features.\nFitting estimator with 863 features.\nFitting estimator with 862 features.\nFitting estimator with 861 features.\nFitting estimator with 860 features.\nFitting estimator with 859 features.\nFitting estimator with 858 features.\nFitting estimator with 857 features.\nFitting estimator with 856 features.\nFitting estimator with 855 features.\nFitting estimator with 854 features.\nFitting estimator with 853 features.\nFitting estimator with 852 features.\nFitting estimator with 851 features.\nFitting estimator with 850 features.\nFitting estimator with 849 features.\nFitting estimator with 848 features.\nFitting estimator with 847 features.\nFitting estimator with 846 features.\nFitting estimator with 845 features.\nFitting estimator with 844 features.\nFitting estimator with 843 features.\nFitting estimator with 842 features.\nFitting estimator with 841 features.\nFitting estimator with 840 features.\nFitting estimator with 839 features.\nFitting estimator with 838 features.\nFitting estimator with 837 features.\nFitting estimator with 836 features.\nFitting estimator with 835 features.\nFitting estimator with 834 features.\nFitting estimator with 833 features.\nFitting estimator with 832 features.\nFitting estimator with 831 features.\nFitting estimator with 830 features.\nFitting estimator with 829 features.\nFitting estimator with 828 features.\nFitting estimator with 827 features.\nFitting estimator with 826 features.\nFitting estimator with 825 features.\nFitting estimator with 824 features.\nFitting estimator with 823 features.\nFitting estimator with 822 features.\nFitting estimator with 821 features.\nFitting estimator with 820 features.\nFitting estimator with 819 features.\nFitting estimator with 818 features.\nFitting estimator with 817 features.\nFitting estimator with 816 features.\nFitting estimator with 815 features.\nFitting estimator with 814 features.\nFitting estimator with 813 features.\nFitting estimator with 812 features.\nFitting estimator with 811 features.\nFitting estimator with 810 features.\nFitting estimator with 809 features.\nFitting estimator with 808 features.\nFitting estimator with 807 features.\nFitting estimator with 806 features.\nFitting estimator with 805 features.\nFitting estimator with 804 features.\nFitting estimator with 803 features.\nFitting estimator with 802 features.\nFitting estimator with 801 features.\nFitting estimator with 800 features.\nFitting estimator with 799 features.\nFitting estimator with 798 features.\nFitting estimator with 797 features.\nFitting estimator with 796 features.\nFitting estimator with 795 features.\nFitting estimator with 794 features.\nFitting estimator with 793 features.\nFitting estimator with 792 features.\nFitting estimator with 791 features.\nFitting estimator with 790 features.\nFitting estimator with 789 features.\nFitting estimator with 788 features.\nFitting estimator with 787 features.\nFitting estimator with 786 features.\nFitting estimator with 785 features.\nFitting estimator with 784 features.\nFitting estimator with 783 features.\nFitting estimator with 782 features.\nFitting estimator with 781 features.\nFitting estimator with 780 features.\nFitting estimator with 779 features.\nFitting estimator with 778 features.\nFitting estimator with 777 features.\nFitting estimator with 776 features.\nFitting estimator with 775 features.\nFitting estimator with 774 features.\nFitting estimator with 773 features.\nFitting estimator with 772 features.\nFitting estimator with 771 features.\nFitting estimator with 770 features.\nFitting estimator with 769 features.\nFitting estimator with 768 features.\nFitting estimator with 767 features.\nFitting estimator with 766 features.\nFitting estimator with 765 features.\nFitting estimator with 764 features.\nFitting estimator with 763 features.\nFitting estimator with 762 features.\nFitting estimator with 761 features.\nFitting estimator with 760 features.\nFitting estimator with 759 features.\nFitting estimator with 758 features.\nFitting estimator with 757 features.\nFitting estimator with 756 features.\nFitting estimator with 755 features.\nFitting estimator with 754 features.\nFitting estimator with 753 features.\nFitting estimator with 752 features.\nFitting estimator with 751 features.\nFitting estimator with 750 features.\nFitting estimator with 749 features.\nFitting estimator with 748 features.\nFitting estimator with 747 features.\nFitting estimator with 746 features.\nFitting estimator with 745 features.\nFitting estimator with 744 features.\nFitting estimator with 743 features.\nFitting estimator with 742 features.\nFitting estimator with 741 features.\nFitting estimator with 740 features.\nFitting estimator with 739 features.\nFitting estimator with 738 features.\nFitting estimator with 737 features.\nFitting estimator with 736 features.\nFitting estimator with 735 features.\nFitting estimator with 734 features.\nFitting estimator with 733 features.\nFitting estimator with 732 features.\nFitting estimator with 731 features.\nFitting estimator with 730 features.\nFitting estimator with 729 features.\nFitting estimator with 728 features.\nFitting estimator with 727 features.\nFitting estimator with 726 features.\nFitting estimator with 725 features.\nFitting estimator with 724 features.\nFitting estimator with 723 features.\nFitting estimator with 722 features.\nFitting estimator with 721 features.\nFitting estimator with 720 features.\nFitting estimator with 719 features.\nFitting estimator with 718 features.\nFitting estimator with 717 features.\nFitting estimator with 716 features.\nFitting estimator with 715 features.\nFitting estimator with 714 features.\nFitting estimator with 713 features.\nFitting estimator with 712 features.\nFitting estimator with 711 features.\nFitting estimator with 710 features.\nFitting estimator with 709 features.\nFitting estimator with 708 features.\nFitting estimator with 707 features.\nFitting estimator with 706 features.\nFitting estimator with 705 features.\nFitting estimator with 704 features.\nFitting estimator with 703 features.\nFitting estimator with 702 features.\nFitting estimator with 701 features.\nFitting estimator with 700 features.\nFitting estimator with 699 features.\nFitting estimator with 698 features.\nFitting estimator with 697 features.\nFitting estimator with 696 features.\nFitting estimator with 695 features.\nFitting estimator with 694 features.\nFitting estimator with 693 features.\nFitting estimator with 692 features.\nFitting estimator with 691 features.\nFitting estimator with 690 features.\nFitting estimator with 689 features.\nFitting estimator with 688 features.\nFitting estimator with 687 features.\nFitting estimator with 686 features.\nFitting estimator with 685 features.\nFitting estimator with 684 features.\nFitting estimator with 683 features.\nFitting estimator with 682 features.\nFitting estimator with 681 features.\nFitting estimator with 680 features.\nFitting estimator with 679 features.\nFitting estimator with 678 features.\nFitting estimator with 677 features.\nFitting estimator with 676 features.\nFitting estimator with 675 features.\nFitting estimator with 674 features.\nFitting estimator with 673 features.\nFitting estimator with 672 features.\nFitting estimator with 671 features.\nFitting estimator with 670 features.\nFitting estimator with 669 features.\nFitting estimator with 668 features.\nFitting estimator with 667 features.\nFitting estimator with 666 features.\nFitting estimator with 665 features.\nFitting estimator with 664 features.\nFitting estimator with 663 features.\nFitting estimator with 662 features.\nFitting estimator with 661 features.\nFitting estimator with 660 features.\nFitting estimator with 659 features.\nFitting estimator with 658 features.\nFitting estimator with 657 features.\nFitting estimator with 656 features.\nFitting estimator with 655 features.\nFitting estimator with 654 features.\nFitting estimator with 653 features.\nFitting estimator with 652 features.\nFitting estimator with 651 features.\nFitting estimator with 650 features.\nFitting estimator with 649 features.\nFitting estimator with 648 features.\nFitting estimator with 647 features.\nFitting estimator with 646 features.\nFitting estimator with 645 features.\nFitting estimator with 644 features.\nFitting estimator with 643 features.\nFitting estimator with 642 features.\nFitting estimator with 641 features.\nFitting estimator with 640 features.\nFitting estimator with 639 features.\nFitting estimator with 638 features.\nFitting estimator with 637 features.\nFitting estimator with 636 features.\nFitting estimator with 635 features.\nFitting estimator with 634 features.\nFitting estimator with 633 features.\nFitting estimator with 632 features.\nFitting estimator with 631 features.\nFitting estimator with 630 features.\nFitting estimator with 629 features.\nFitting estimator with 628 features.\nFitting estimator with 627 features.\nFitting estimator with 626 features.\nFitting estimator with 625 features.\nFitting estimator with 624 features.\nFitting estimator with 623 features.\nFitting estimator with 622 features.\nFitting estimator with 621 features.\nFitting estimator with 620 features.\nFitting estimator with 619 features.\nFitting estimator with 618 features.\nFitting estimator with 617 features.\nFitting estimator with 616 features.\nFitting estimator with 615 features.\nFitting estimator with 614 features.\nFitting estimator with 613 features.\nFitting estimator with 612 features.\nFitting estimator with 611 features.\nFitting estimator with 610 features.\nFitting estimator with 609 features.\nFitting estimator with 608 features.\nFitting estimator with 607 features.\nFitting estimator with 606 features.\nFitting estimator with 605 features.\nFitting estimator with 604 features.\nFitting estimator with 603 features.\nFitting estimator with 602 features.\nFitting estimator with 601 features.\nFitting estimator with 600 features.\nFitting estimator with 599 features.\nFitting estimator with 598 features.\nFitting estimator with 597 features.\nFitting estimator with 596 features.\nFitting estimator with 595 features.\nFitting estimator with 594 features.\nFitting estimator with 593 features.\nFitting estimator with 592 features.\nFitting estimator with 591 features.\nFitting estimator with 590 features.\nFitting estimator with 589 features.\nFitting estimator with 588 features.\nFitting estimator with 587 features.\nFitting estimator with 586 features.\nFitting estimator with 585 features.\nFitting estimator with 584 features.\nFitting estimator with 583 features.\nFitting estimator with 582 features.\nFitting estimator with 581 features.\nFitting estimator with 580 features.\nFitting estimator with 579 features.\nFitting estimator with 578 features.\nFitting estimator with 577 features.\nFitting estimator with 576 features.\nFitting estimator with 575 features.\nFitting estimator with 574 features.\nFitting estimator with 573 features.\nFitting estimator with 572 features.\nFitting estimator with 571 features.\nFitting estimator with 570 features.\nFitting estimator with 569 features.\nFitting estimator with 568 features.\nFitting estimator with 567 features.\nFitting estimator with 566 features.\nFitting estimator with 565 features.\nFitting estimator with 564 features.\nFitting estimator with 563 features.\nFitting estimator with 562 features.\nFitting estimator with 561 features.\nFitting estimator with 560 features.\nFitting estimator with 559 features.\nFitting estimator with 558 features.\nFitting estimator with 557 features.\nFitting estimator with 556 features.\nFitting estimator with 555 features.\nFitting estimator with 554 features.\nFitting estimator with 553 features.\nFitting estimator with 552 features.\nFitting estimator with 551 features.\nFitting estimator with 550 features.\nFitting estimator with 549 features.\nFitting estimator with 548 features.\nFitting estimator with 547 features.\nFitting estimator with 546 features.\nFitting estimator with 545 features.\nFitting estimator with 544 features.\nFitting estimator with 543 features.\nFitting estimator with 542 features.\nFitting estimator with 541 features.\nFitting estimator with 540 features.\nFitting estimator with 539 features.\nFitting estimator with 538 features.\nFitting estimator with 537 features.\nFitting estimator with 536 features.\nFitting estimator with 535 features.\nFitting estimator with 534 features.\nFitting estimator with 533 features.\nFitting estimator with 532 features.\nFitting estimator with 531 features.\nFitting estimator with 530 features.\nFitting estimator with 529 features.\nFitting estimator with 528 features.\nFitting estimator with 527 features.\nFitting estimator with 526 features.\nFitting estimator with 525 features.\nFitting estimator with 524 features.\nFitting estimator with 523 features.\nFitting estimator with 522 features.\nFitting estimator with 521 features.\nFitting estimator with 520 features.\nFitting estimator with 519 features.\nFitting estimator with 518 features.\nFitting estimator with 517 features.\nFitting estimator with 516 features.\nFitting estimator with 515 features.\nFitting estimator with 514 features.\nFitting estimator with 513 features.\nFitting estimator with 512 features.\nFitting estimator with 511 features.\nFitting estimator with 510 features.\nFitting estimator with 509 features.\nFitting estimator with 508 features.\nFitting estimator with 507 features.\nFitting estimator with 506 features.\nFitting estimator with 505 features.\nFitting estimator with 504 features.\nFitting estimator with 503 features.\nFitting estimator with 502 features.\nFitting estimator with 501 features.\nFitting estimator with 500 features.\nFitting estimator with 499 features.\nFitting estimator with 498 features.\nFitting estimator with 497 features.\nFitting estimator with 496 features.\nFitting estimator with 495 features.\nFitting estimator with 494 features.\nFitting estimator with 493 features.\nFitting estimator with 492 features.\nFitting estimator with 491 features.\nFitting estimator with 490 features.\nFitting estimator with 489 features.\nFitting estimator with 488 features.\nFitting estimator with 487 features.\nFitting estimator with 486 features.\nFitting estimator with 485 features.\nFitting estimator with 484 features.\nFitting estimator with 483 features.\nFitting estimator with 482 features.\nFitting estimator with 481 features.\nFitting estimator with 480 features.\nFitting estimator with 479 features.\nFitting estimator with 478 features.\nFitting estimator with 477 features.\nFitting estimator with 476 features.\nFitting estimator with 475 features.\nFitting estimator with 474 features.\nFitting estimator with 473 features.\nFitting estimator with 472 features.\nFitting estimator with 471 features.\nFitting estimator with 470 features.\nFitting estimator with 469 features.\nFitting estimator with 468 features.\nFitting estimator with 467 features.\nFitting estimator with 466 features.\nFitting estimator with 465 features.\nFitting estimator with 464 features.\nFitting estimator with 463 features.\nFitting estimator with 462 features.\nFitting estimator with 461 features.\nFitting estimator with 460 features.\nFitting estimator with 459 features.\nFitting estimator with 458 features.\nFitting estimator with 457 features.\nFitting estimator with 456 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\nFitting estimator with 880 features.\nFitting estimator with 879 features.\nFitting estimator with 878 features.\nFitting estimator with 877 features.\nFitting estimator with 876 features.\nFitting estimator with 875 features.\nFitting estimator with 874 features.\nFitting estimator with 873 features.\nFitting estimator with 872 features.\nFitting estimator with 871 features.\nFitting estimator with 870 features.\nFitting estimator with 869 features.\nFitting estimator with 868 features.\nFitting estimator with 867 features.\nFitting estimator with 866 features.\nFitting estimator with 865 features.\nFitting estimator with 864 features.\nFitting estimator with 863 features.\nFitting estimator with 862 features.\nFitting estimator with 861 features.\nFitting estimator with 860 features.\nFitting estimator with 859 features.\nFitting estimator with 858 features.\nFitting estimator with 857 features.\nFitting estimator with 856 features.\nFitting estimator with 855 features.\nFitting estimator with 854 features.\nFitting estimator with 853 features.\nFitting estimator with 852 features.\nFitting estimator with 851 features.\nFitting estimator with 850 features.\nFitting estimator with 849 features.\nFitting estimator with 848 features.\nFitting estimator with 847 features.\nFitting estimator with 846 features.\nFitting estimator with 845 features.\nFitting estimator with 844 features.\nFitting estimator with 843 features.\nFitting estimator with 842 features.\nFitting estimator with 841 features.\nFitting estimator with 840 features.\nFitting estimator with 839 features.\nFitting estimator with 838 features.\nFitting estimator with 837 features.\nFitting estimator with 836 features.\nFitting estimator with 835 features.\nFitting estimator with 834 features.\nFitting estimator with 833 features.\nFitting estimator with 832 features.\nFitting estimator with 831 features.\nFitting estimator with 830 features.\nFitting estimator with 829 features.\nFitting estimator with 828 features.\nFitting estimator with 827 features.\nFitting estimator with 826 features.\nFitting estimator with 825 features.\nFitting estimator with 824 features.\nFitting estimator with 823 features.\nFitting estimator with 822 features.\nFitting estimator with 821 features.\nFitting estimator with 820 features.\nFitting estimator with 819 features.\nFitting estimator with 818 features.\nFitting estimator with 817 features.\nFitting estimator with 816 features.\nFitting estimator with 815 features.\nFitting estimator with 814 features.\nFitting estimator with 813 features.\nFitting estimator with 812 features.\nFitting estimator with 811 features.\nFitting estimator with 810 features.\nFitting estimator with 809 features.\nFitting estimator with 808 features.\nFitting estimator with 807 features.\nFitting estimator with 806 features.\nFitting estimator with 805 features.\nFitting estimator with 804 features.\nFitting estimator with 803 features.\nFitting estimator with 802 features.\nFitting estimator with 801 features.\nFitting estimator with 800 features.\nFitting estimator with 799 features.\nFitting estimator with 798 features.\nFitting estimator with 797 features.\nFitting estimator with 796 features.\nFitting estimator with 795 features.\nFitting estimator with 794 features.\nFitting estimator with 793 features.\nFitting estimator with 792 features.\nFitting estimator with 791 features.\nFitting estimator with 790 features.\nFitting estimator with 789 features.\nFitting estimator with 788 features.\nFitting estimator with 787 features.\nFitting estimator with 786 features.\nFitting estimator with 785 features.\nFitting estimator with 784 features.\nFitting estimator with 783 features.\nFitting estimator with 782 features.\nFitting estimator with 781 features.\nFitting estimator with 780 features.\nFitting estimator with 779 features.\nFitting estimator with 778 features.\nFitting estimator with 777 features.\nFitting estimator with 776 features.\nFitting estimator with 775 features.\nFitting estimator with 774 features.\nFitting estimator with 773 features.\nFitting estimator with 772 features.\nFitting estimator with 771 features.\nFitting estimator with 770 features.\nFitting estimator with 769 features.\nFitting estimator with 768 features.\nFitting estimator with 767 features.\nFitting estimator with 766 features.\nFitting estimator with 765 features.\nFitting estimator with 764 features.\nFitting estimator with 763 features.\nFitting estimator with 762 features.\nFitting estimator with 761 features.\nFitting estimator with 760 features.\nFitting estimator with 759 features.\nFitting estimator with 758 features.\nFitting estimator with 757 features.\nFitting estimator with 756 features.\nFitting estimator with 755 features.\nFitting estimator with 754 features.\nFitting estimator with 753 features.\nFitting estimator with 752 features.\nFitting estimator with 751 features.\nFitting estimator with 750 features.\nFitting estimator with 749 features.\nFitting estimator with 748 features.\nFitting estimator with 747 features.\nFitting estimator with 746 features.\nFitting estimator with 745 features.\nFitting estimator with 744 features.\nFitting estimator with 743 features.\nFitting estimator with 742 features.\nFitting estimator with 741 features.\nFitting estimator with 740 features.\nFitting estimator with 739 features.\nFitting estimator with 738 features.\nFitting estimator with 737 features.\nFitting estimator with 736 features.\nFitting estimator with 735 features.\nFitting estimator with 734 features.\nFitting estimator with 733 features.\nFitting estimator with 732 features.\nFitting estimator with 731 features.\nFitting estimator with 730 features.\nFitting estimator with 729 features.\nFitting estimator with 728 features.\nFitting estimator with 727 features.\nFitting estimator with 726 features.\nFitting estimator with 725 features.\nFitting estimator with 724 features.\nFitting estimator with 723 features.\nFitting estimator with 722 features.\nFitting estimator with 721 features.\nFitting estimator with 720 features.\nFitting estimator with 719 features.\nFitting estimator with 718 features.\nFitting estimator with 717 features.\nFitting estimator with 716 features.\nFitting estimator with 715 features.\nFitting estimator with 714 features.\nFitting estimator with 713 features.\nFitting estimator with 712 features.\nFitting estimator with 711 features.\nFitting estimator with 710 features.\nFitting estimator with 709 features.\nFitting estimator with 708 features.\nFitting estimator with 707 features.\nFitting estimator with 706 features.\nFitting estimator with 705 features.\nFitting estimator with 704 features.\nFitting estimator with 703 features.\nFitting estimator with 702 features.\nFitting estimator with 701 features.\nFitting estimator with 700 features.\nFitting estimator with 699 features.\nFitting estimator with 698 features.\nFitting estimator with 697 features.\nFitting estimator with 696 features.\nFitting estimator with 695 features.\nFitting estimator with 694 features.\nFitting estimator with 693 features.\nFitting estimator with 692 features.\nFitting estimator with 691 features.\nFitting estimator with 690 features.\nFitting estimator with 689 features.\nFitting estimator with 688 features.\nFitting estimator with 687 features.\nFitting estimator with 686 features.\nFitting estimator with 685 features.\nFitting estimator with 684 features.\nFitting estimator with 683 features.\nFitting estimator with 682 features.\nFitting estimator with 681 features.\nFitting estimator with 680 features.\nFitting estimator with 679 features.\nFitting estimator with 678 features.\nFitting estimator with 677 features.\nFitting estimator with 676 features.\nFitting estimator with 675 features.\nFitting estimator with 674 features.\nFitting estimator with 673 features.\nFitting estimator with 672 features.\nFitting estimator with 671 features.\nFitting estimator with 670 features.\nFitting estimator with 669 features.\nFitting estimator with 668 features.\nFitting estimator with 667 features.\nFitting estimator with 666 features.\nFitting estimator with 665 features.\nFitting estimator with 664 features.\nFitting estimator with 663 features.\nFitting estimator with 662 features.\nFitting estimator with 661 features.\nFitting estimator with 660 features.\nFitting estimator with 659 features.\nFitting estimator with 658 features.\nFitting estimator with 657 features.\nFitting estimator with 656 features.\nFitting estimator with 655 features.\nFitting estimator with 654 features.\nFitting estimator with 653 features.\nFitting estimator with 652 features.\nFitting estimator with 651 features.\nFitting estimator with 650 features.\nFitting estimator with 649 features.\nFitting estimator with 648 features.\nFitting estimator with 647 features.\nFitting estimator with 646 features.\nFitting estimator with 645 features.\nFitting estimator with 644 features.\nFitting estimator with 643 features.\nFitting estimator with 642 features.\nFitting estimator with 641 features.\nFitting estimator with 640 features.\nFitting estimator with 639 features.\nFitting estimator with 638 features.\nFitting estimator with 637 features.\nFitting estimator with 636 features.\nFitting estimator with 635 features.\nFitting estimator with 634 features.\nFitting estimator with 633 features.\nFitting estimator with 632 features.\nFitting estimator with 631 features.\nFitting estimator with 630 features.\nFitting estimator with 629 features.\nFitting estimator with 628 features.\nFitting estimator with 627 features.\nFitting estimator with 626 features.\nFitting estimator with 625 features.\nFitting estimator with 624 features.\nFitting estimator with 623 features.\nFitting estimator with 622 features.\nFitting estimator with 621 features.\nFitting estimator with 620 features.\nFitting estimator with 619 features.\nFitting estimator with 618 features.\nFitting estimator with 617 features.\nFitting estimator with 616 features.\nFitting estimator with 615 features.\nFitting estimator with 614 features.\nFitting estimator with 613 features.\nFitting estimator with 612 features.\nFitting estimator with 611 features.\nFitting estimator with 610 features.\nFitting estimator with 609 features.\nFitting estimator with 608 features.\nFitting estimator with 607 features.\nFitting estimator with 606 features.\nFitting estimator with 605 features.\nFitting estimator with 604 features.\nFitting estimator with 603 features.\nFitting estimator with 602 features.\nFitting estimator with 601 features.\nFitting estimator with 600 features.\nFitting estimator with 599 features.\nFitting estimator with 598 features.\nFitting estimator with 597 features.\nFitting estimator with 596 features.\nFitting estimator with 595 features.\nFitting estimator with 594 features.\nFitting estimator with 593 features.\nFitting estimator with 592 features.\nFitting estimator with 591 features.\nFitting estimator with 590 features.\nFitting estimator with 589 features.\nFitting estimator with 588 features.\nFitting estimator with 587 features.\nFitting estimator with 586 features.\nFitting estimator with 585 features.\nFitting estimator with 584 features.\nFitting estimator with 583 features.\nFitting estimator with 582 features.\nFitting estimator with 581 features.\nFitting estimator with 580 features.\nFitting estimator with 579 features.\nFitting estimator with 578 features.\nFitting estimator with 577 features.\nFitting estimator with 576 features.\nFitting estimator with 575 features.\nFitting estimator with 574 features.\nFitting estimator with 573 features.\nFitting estimator with 572 features.\nFitting estimator with 571 features.\nFitting estimator with 570 features.\nFitting estimator with 569 features.\nFitting estimator with 568 features.\nFitting estimator with 567 features.\nFitting estimator with 566 features.\nFitting estimator with 565 features.\nFitting estimator with 564 features.\nFitting estimator with 563 features.\nFitting estimator with 562 features.\nFitting estimator with 561 features.\nFitting estimator with 560 features.\nFitting estimator with 559 features.\nFitting estimator with 558 features.\nFitting estimator with 557 features.\nFitting estimator with 556 features.\nFitting estimator with 555 features.\nFitting estimator with 554 features.\nFitting estimator with 553 features.\nFitting estimator with 552 features.\nFitting estimator with 551 features.\nFitting estimator with 550 features.\nFitting estimator with 549 features.\nFitting estimator with 548 features.\nFitting estimator with 547 features.\nFitting estimator with 546 features.\nFitting estimator with 545 features.\nFitting estimator with 544 features.\nFitting estimator with 543 features.\nFitting estimator with 542 features.\nFitting estimator with 541 features.\nFitting estimator with 540 features.\nFitting estimator with 539 features.\nFitting estimator with 538 features.\nFitting estimator with 537 features.\nFitting estimator with 536 features.\nFitting estimator with 535 features.\nFitting estimator with 534 features.\nFitting estimator with 533 features.\nFitting estimator with 532 features.\nFitting estimator with 531 features.\nFitting estimator with 530 features.\nFitting estimator with 529 features.\nFitting estimator with 528 features.\nFitting estimator with 527 features.\nFitting estimator with 526 features.\nFitting estimator with 525 features.\nFitting estimator with 524 features.\nFitting estimator with 523 features.\nFitting estimator with 522 features.\nFitting estimator with 521 features.\nFitting estimator with 520 features.\nFitting estimator with 519 features.\nFitting estimator with 518 features.\nFitting estimator with 517 features.\nFitting estimator with 516 features.\nFitting estimator with 515 features.\nFitting estimator with 514 features.\nFitting estimator with 513 features.\nFitting estimator with 512 features.\nFitting estimator with 511 features.\nFitting estimator with 510 features.\nFitting estimator with 509 features.\nFitting estimator with 508 features.\nFitting estimator with 507 features.\nFitting estimator with 506 features.\nFitting estimator with 505 features.\nFitting estimator with 504 features.\nFitting estimator with 503 features.\nFitting estimator with 502 features.\nFitting estimator with 501 features.\nFitting estimator with 500 features.\nFitting estimator with 499 features.\nFitting estimator with 498 features.\nFitting estimator with 497 features.\nFitting estimator with 496 features.\nFitting estimator with 495 features.\nFitting estimator with 494 features.\nFitting estimator with 493 features.\nFitting estimator with 492 features.\nFitting estimator with 491 features.\nFitting estimator with 490 features.\nFitting estimator with 489 features.\nFitting estimator with 488 features.\nFitting estimator with 487 features.\nFitting estimator with 486 features.\nFitting estimator with 485 features.\nFitting estimator with 484 features.\nFitting estimator with 483 features.\nFitting estimator with 482 features.\nFitting estimator with 481 features.\nFitting estimator with 480 features.\nFitting estimator with 479 features.\nFitting estimator with 478 features.\nFitting estimator with 477 features.\nFitting estimator with 476 features.\nFitting estimator with 475 features.\nFitting estimator with 474 features.\nFitting estimator with 473 features.\nFitting estimator with 472 features.\nFitting estimator with 471 features.\nFitting estimator with 470 features.\nFitting estimator with 469 features.\nFitting estimator with 468 features.\nFitting estimator with 467 features.\nFitting estimator with 466 features.\nFitting estimator with 465 features.\nFitting estimator with 464 features.\nFitting estimator with 463 features.\nFitting estimator with 462 features.\nFitting estimator with 461 features.\nFitting estimator with 460 features.\nFitting estimator with 459 features.\nFitting estimator with 458 features.\nFitting estimator with 457 features.\nFitting estimator with 456 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\nFitting estimator with 880 features.\nFitting estimator with 879 features.\nFitting estimator with 878 features.\nFitting estimator with 877 features.\nFitting estimator with 876 features.\nFitting estimator with 875 features.\nFitting estimator with 874 features.\nFitting estimator with 873 features.\nFitting estimator with 872 features.\nFitting estimator with 871 features.\nFitting estimator with 870 features.\nFitting estimator with 869 features.\nFitting estimator with 868 features.\nFitting estimator with 867 features.\nFitting estimator with 866 features.\nFitting estimator with 865 features.\nFitting estimator with 864 features.\nFitting estimator with 863 features.\nFitting estimator with 862 features.\nFitting estimator with 861 features.\nFitting estimator with 860 features.\nFitting estimator with 859 features.\nFitting estimator with 858 features.\nFitting estimator with 857 features.\nFitting estimator with 856 features.\nFitting estimator with 855 features.\nFitting estimator with 854 features.\nFitting estimator with 853 features.\nFitting estimator with 852 features.\nFitting estimator with 851 features.\nFitting estimator with 850 features.\nFitting estimator with 849 features.\nFitting estimator with 848 features.\nFitting estimator with 847 features.\nFitting estimator with 846 features.\nFitting estimator with 845 features.\nFitting estimator with 844 features.\nFitting estimator with 843 features.\nFitting estimator with 842 features.\nFitting estimator with 841 features.\nFitting estimator with 840 features.\nFitting estimator with 839 features.\nFitting estimator with 838 features.\nFitting estimator with 837 features.\nFitting estimator with 836 features.\nFitting estimator with 835 features.\nFitting estimator with 834 features.\nFitting estimator with 833 features.\nFitting estimator with 832 features.\nFitting estimator with 831 features.\nFitting estimator with 830 features.\nFitting estimator with 829 features.\nFitting estimator with 828 features.\nFitting estimator with 827 features.\nFitting estimator with 826 features.\nFitting estimator with 825 features.\nFitting estimator with 824 features.\nFitting estimator with 823 features.\nFitting estimator with 822 features.\nFitting estimator with 821 features.\nFitting estimator with 820 features.\nFitting estimator with 819 features.\nFitting estimator with 818 features.\nFitting estimator with 817 features.\nFitting estimator with 816 features.\nFitting estimator with 815 features.\nFitting estimator with 814 features.\nFitting estimator with 813 features.\nFitting estimator with 812 features.\nFitting estimator with 811 features.\nFitting estimator with 810 features.\nFitting estimator with 809 features.\nFitting estimator with 808 features.\nFitting estimator with 807 features.\nFitting estimator with 806 features.\nFitting estimator with 805 features.\nFitting estimator with 804 features.\nFitting estimator with 803 features.\nFitting estimator with 802 features.\nFitting estimator with 801 features.\nFitting estimator with 800 features.\nFitting estimator with 799 features.\nFitting estimator with 798 features.\nFitting estimator with 797 features.\nFitting estimator with 796 features.\nFitting estimator with 795 features.\nFitting estimator with 794 features.\nFitting estimator with 793 features.\nFitting estimator with 792 features.\nFitting estimator with 791 features.\nFitting estimator with 790 features.\nFitting estimator with 789 features.\nFitting estimator with 788 features.\nFitting estimator with 787 features.\nFitting estimator with 786 features.\nFitting estimator with 785 features.\nFitting estimator with 784 features.\nFitting estimator with 783 features.\nFitting estimator with 782 features.\nFitting estimator with 781 features.\nFitting estimator with 780 features.\nFitting estimator with 779 features.\nFitting estimator with 778 features.\nFitting estimator with 777 features.\nFitting estimator with 776 features.\nFitting estimator with 775 features.\nFitting estimator with 774 features.\nFitting estimator with 773 features.\nFitting estimator with 772 features.\nFitting estimator with 771 features.\nFitting estimator with 770 features.\nFitting estimator with 769 features.\nFitting estimator with 768 features.\nFitting estimator with 767 features.\nFitting estimator with 766 features.\nFitting estimator with 765 features.\nFitting estimator with 764 features.\nFitting estimator with 763 features.\nFitting estimator with 762 features.\nFitting estimator with 761 features.\nFitting estimator with 760 features.\nFitting estimator with 759 features.\nFitting estimator with 758 features.\nFitting estimator with 757 features.\nFitting estimator with 756 features.\nFitting estimator with 755 features.\nFitting estimator with 754 features.\nFitting estimator with 753 features.\nFitting estimator with 752 features.\nFitting estimator with 751 features.\nFitting estimator with 750 features.\nFitting estimator with 749 features.\nFitting estimator with 748 features.\nFitting estimator with 747 features.\nFitting estimator with 746 features.\nFitting estimator with 745 features.\nFitting estimator with 744 features.\nFitting estimator with 743 features.\nFitting estimator with 742 features.\nFitting estimator with 741 features.\nFitting estimator with 740 features.\nFitting estimator with 739 features.\nFitting estimator with 738 features.\nFitting estimator with 737 features.\nFitting estimator with 736 features.\nFitting estimator with 735 features.\nFitting estimator with 734 features.\nFitting estimator with 733 features.\nFitting estimator with 732 features.\nFitting estimator with 731 features.\nFitting estimator with 730 features.\nFitting estimator with 729 features.\nFitting estimator with 728 features.\nFitting estimator with 727 features.\nFitting estimator with 726 features.\nFitting estimator with 725 features.\nFitting estimator with 724 features.\nFitting estimator with 723 features.\nFitting estimator with 722 features.\nFitting estimator with 721 features.\nFitting estimator with 720 features.\nFitting estimator with 719 features.\nFitting estimator with 718 features.\nFitting estimator with 717 features.\nFitting estimator with 716 features.\nFitting estimator with 715 features.\nFitting estimator with 714 features.\nFitting estimator with 713 features.\nFitting estimator with 712 features.\nFitting estimator with 711 features.\nFitting estimator with 710 features.\nFitting estimator with 709 features.\nFitting estimator with 708 features.\nFitting estimator with 707 features.\nFitting estimator with 706 features.\nFitting estimator with 705 features.\nFitting estimator with 704 features.\nFitting estimator with 703 features.\nFitting estimator with 702 features.\nFitting estimator with 701 features.\nFitting estimator with 700 features.\nFitting estimator with 699 features.\nFitting estimator with 698 features.\nFitting estimator with 697 features.\nFitting estimator with 696 features.\nFitting estimator with 695 features.\nFitting estimator with 694 features.\nFitting estimator with 693 features.\nFitting estimator with 692 features.\nFitting estimator with 691 features.\nFitting estimator with 690 features.\nFitting estimator with 689 features.\nFitting estimator with 688 features.\nFitting estimator with 687 features.\nFitting estimator with 686 features.\nFitting estimator with 685 features.\nFitting estimator with 684 features.\nFitting estimator with 683 features.\nFitting estimator with 682 features.\nFitting estimator with 681 features.\nFitting estimator with 680 features.\nFitting estimator with 679 features.\nFitting estimator with 678 features.\nFitting estimator with 677 features.\nFitting estimator with 676 features.\nFitting estimator with 675 features.\nFitting estimator with 674 features.\nFitting estimator with 673 features.\nFitting estimator with 672 features.\nFitting estimator with 671 features.\nFitting estimator with 670 features.\nFitting estimator with 669 features.\nFitting estimator with 668 features.\nFitting estimator with 667 features.\nFitting estimator with 666 features.\nFitting estimator with 665 features.\nFitting estimator with 664 features.\nFitting estimator with 663 features.\nFitting estimator with 662 features.\nFitting estimator with 661 features.\nFitting estimator with 660 features.\nFitting estimator with 659 features.\nFitting estimator with 658 features.\nFitting estimator with 657 features.\nFitting estimator with 656 features.\nFitting estimator with 655 features.\nFitting estimator with 654 features.\nFitting estimator with 653 features.\nFitting estimator with 652 features.\nFitting estimator with 651 features.\nFitting estimator with 650 features.\nFitting estimator with 649 features.\nFitting estimator with 648 features.\nFitting estimator with 647 features.\nFitting estimator with 646 features.\nFitting estimator with 645 features.\nFitting estimator with 644 features.\nFitting estimator with 643 features.\nFitting estimator with 642 features.\nFitting estimator with 641 features.\nFitting estimator with 640 features.\nFitting estimator with 639 features.\nFitting estimator with 638 features.\nFitting estimator with 637 features.\nFitting estimator with 636 features.\nFitting estimator with 635 features.\nFitting estimator with 634 features.\nFitting estimator with 633 features.\nFitting estimator with 632 features.\nFitting estimator with 631 features.\nFitting estimator with 630 features.\nFitting estimator with 629 features.\nFitting estimator with 628 features.\nFitting estimator with 627 features.\nFitting estimator with 626 features.\nFitting estimator with 625 features.\nFitting estimator with 624 features.\nFitting estimator with 623 features.\nFitting estimator with 622 features.\nFitting estimator with 621 features.\nFitting estimator with 620 features.\nFitting estimator with 619 features.\nFitting estimator with 618 features.\nFitting estimator with 617 features.\nFitting estimator with 616 features.\nFitting estimator with 615 features.\nFitting estimator with 614 features.\nFitting estimator with 613 features.\nFitting estimator with 612 features.\nFitting estimator with 611 features.\nFitting estimator with 610 features.\nFitting estimator with 609 features.\nFitting estimator with 608 features.\nFitting estimator with 607 features.\nFitting estimator with 606 features.\nFitting estimator with 605 features.\nFitting estimator with 604 features.\nFitting estimator with 603 features.\nFitting estimator with 602 features.\nFitting estimator with 601 features.\nFitting estimator with 600 features.\nFitting estimator with 599 features.\nFitting estimator with 598 features.\nFitting estimator with 597 features.\nFitting estimator with 596 features.\nFitting estimator with 595 features.\nFitting estimator with 594 features.\nFitting estimator with 593 features.\nFitting estimator with 592 features.\nFitting estimator with 591 features.\nFitting estimator with 590 features.\nFitting estimator with 589 features.\nFitting estimator with 588 features.\nFitting estimator with 587 features.\nFitting estimator with 586 features.\nFitting estimator with 585 features.\nFitting estimator with 584 features.\nFitting estimator with 583 features.\nFitting estimator with 582 features.\nFitting estimator with 581 features.\nFitting estimator with 580 features.\nFitting estimator with 579 features.\nFitting estimator with 578 features.\nFitting estimator with 577 features.\nFitting estimator with 576 features.\nFitting estimator with 575 features.\nFitting estimator with 574 features.\nFitting estimator with 573 features.\nFitting estimator with 572 features.\nFitting estimator with 571 features.\nFitting estimator with 570 features.\nFitting estimator with 569 features.\nFitting estimator with 568 features.\nFitting estimator with 567 features.\nFitting estimator with 566 features.\nFitting estimator with 565 features.\nFitting estimator with 564 features.\nFitting estimator with 563 features.\nFitting estimator with 562 features.\nFitting estimator with 561 features.\nFitting estimator with 560 features.\nFitting estimator with 559 features.\nFitting estimator with 558 features.\nFitting estimator with 557 features.\nFitting estimator with 556 features.\nFitting estimator with 555 features.\nFitting estimator with 554 features.\nFitting estimator with 553 features.\nFitting estimator with 552 features.\nFitting estimator with 551 features.\nFitting estimator with 550 features.\nFitting estimator with 549 features.\nFitting estimator with 548 features.\nFitting estimator with 547 features.\nFitting estimator with 546 features.\nFitting estimator with 545 features.\nFitting estimator with 544 features.\nFitting estimator with 543 features.\nFitting estimator with 542 features.\nFitting estimator with 541 features.\nFitting estimator with 540 features.\nFitting estimator with 539 features.\nFitting estimator with 538 features.\nFitting estimator with 537 features.\nFitting estimator with 536 features.\nFitting estimator with 535 features.\nFitting estimator with 534 features.\nFitting estimator with 533 features.\nFitting estimator with 532 features.\nFitting estimator with 531 features.\nFitting estimator with 530 features.\nFitting estimator with 529 features.\nFitting estimator with 528 features.\nFitting estimator with 527 features.\nFitting estimator with 526 features.\nFitting estimator with 525 features.\nFitting estimator with 524 features.\nFitting estimator with 523 features.\nFitting estimator with 522 features.\nFitting estimator with 521 features.\nFitting estimator with 520 features.\nFitting estimator with 519 features.\nFitting estimator with 518 features.\nFitting estimator with 517 features.\nFitting estimator with 516 features.\nFitting estimator with 515 features.\nFitting estimator with 514 features.\nFitting estimator with 513 features.\nFitting estimator with 512 features.\nFitting estimator with 511 features.\nFitting estimator with 510 features.\nFitting estimator with 509 features.\nFitting estimator with 508 features.\nFitting estimator with 507 features.\nFitting estimator with 506 features.\nFitting estimator with 505 features.\nFitting estimator with 504 features.\nFitting estimator with 503 features.\nFitting estimator with 502 features.\nFitting estimator with 501 features.\nFitting estimator with 500 features.\nFitting estimator with 499 features.\nFitting estimator with 498 features.\nFitting estimator with 497 features.\nFitting estimator with 496 features.\nFitting estimator with 495 features.\nFitting estimator with 494 features.\nFitting estimator with 493 features.\nFitting estimator with 492 features.\nFitting estimator with 491 features.\nFitting estimator with 490 features.\nFitting estimator with 489 features.\nFitting estimator with 488 features.\nFitting estimator with 487 features.\nFitting estimator with 486 features.\nFitting estimator with 485 features.\nFitting estimator with 484 features.\nFitting estimator with 483 features.\nFitting estimator with 482 features.\nFitting estimator with 481 features.\nFitting estimator with 480 features.\nFitting estimator with 479 features.\nFitting estimator with 478 features.\nFitting estimator with 477 features.\nFitting estimator with 476 features.\nFitting estimator with 475 features.\nFitting estimator with 474 features.\nFitting estimator with 473 features.\nFitting estimator with 472 features.\nFitting estimator with 471 features.\nFitting estimator with 470 features.\nFitting estimator with 469 features.\nFitting estimator with 468 features.\nFitting estimator with 467 features.\nFitting estimator with 466 features.\nFitting estimator with 465 features.\nFitting estimator with 464 features.\nFitting estimator with 463 features.\nFitting estimator with 462 features.\nFitting estimator with 461 features.\nFitting estimator with 460 features.\nFitting estimator with 459 features.\nFitting estimator with 458 features.\nFitting estimator with 457 features.\nFitting estimator with 456 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\n\n\n\n# Save the model\njoblib.dump(selector_lin_reg_cv, \"selector_lin_reg_cv.joblib\")\n\n['selector_lin_reg_cv.joblib']\n\n\nAs shown in the graph below, as the number of feature decreases, the validation loss tend to decrease.\n\ndef plot_n_feature_val_loss(selector, step, model_name=\"\", mark=None):\n    \"\"\"\n    Plot the validation loss (MSE) versus the number of features used.\n\n    Parameters:\n        selector: The RFECV selector containing cv_results_.\n        step (int): The step size used in feature elimination.\n        model_name (str): Name of the model for title annotation.\n        mark: Marker style for the plot.\n    \"\"\"\n    # Extract results.\n    n_features = np.array(range(1, len(selector.cv_results_[\"mean_test_score\"]) + 1))\n    validation_loss = -selector.cv_results_[\"mean_test_score\"]  # Convert to positive MSE.\n\n    # Plot.\n    plt.figure(figsize=(12, 6))\n    plt.plot(n_features * step, validation_loss, marker=mark, linestyle=\"-\", color=\"b\")\n    plt.title(f\"Number of Features vs Validation Loss (MSE) of {model_name}\")\n    plt.xlabel(\"Number of Features\")\n    plt.ylabel(\"Validation Loss (MSE)\")\n    plt.grid(True)\n    plt.show()\n\nplot_n_feature_val_loss(\n    selector_lin_reg_cv,\n    step=step_lin_reg,\n    model_name=\"Linear Regression (Cross-Validation)\",\n    mark=None\n)\n\n\n\n\n\n\n\n\n\ndef selector_result(selector, X_train_temp=X_train, y_train_temp=y_train,\n                    X_test_temp=X_test, y_test_temp=y_test,\n                    model_name=\"\", print_out=True, predict=False):\n    \"\"\"\n    Evaluate the selected model and print or return performance metrics.\n\n    Parameters:\n        selector: The RFECV selector object containing cv_results_ and estimator.\n        X_train_temp (DataFrame): Training feature set.\n        y_train_temp (Series): Training labels.\n        X_test_temp (DataFrame): Testing feature set.\n        y_test_temp (Series): Testing labels.\n        model_name (str): Name of the model for display purposes.\n        print_out (bool): If True, print the metrics.\n        predict (bool): If True, return predictions instead of metrics.\n\n    Returns:\n        Tuple: (train MSE, test MSE, train RMSE, test RMSE) if predict is False,\n               otherwise a list of predictions.\n    \"\"\"\n    features = X_train_temp.columns[selector.support_]\n\n    mse_bs_series = pd.Series(-selector.cv_results_[\"mean_test_score\"])\n\n    best_model = selector.estimator\n    best_model.fit(X_train_temp[features], y_train_temp)\n    best_model_mse_train = mean_squared_error(best_model.predict(X_train_temp[features]),\n                                              y_train_temp)\n    y_predict = best_model.predict(X_test_temp[features])\n    best_model_mse_test = mean_squared_error(best_model.predict(X_test_temp[features]),\n                                             y_test_temp)\n    best_model_rmse_train = np.sqrt(best_model_mse_train)\n    best_model_rmse_test = np.sqrt(best_model_mse_test)\n\n    if print_out:\n        print(f\"Optimal number of features for {model_name}: {selector.n_features_}\")\n        print(f\"Selected features for Linear {model_name}: {features}\")\n        print(\"Best Validation MSE:\", mse_bs_series.min())\n        print(\"-----------------------------\")\n        print(f\"Best {model_name} Model Train MSE: {best_model_mse_train}\")\n        print(f\"Best {model_name} Model Test MSE: {best_model_mse_test}\")\n        print(f\"Best {model_name} Model Train RMSE: {best_model_rmse_train}\")\n        print(f\"Best {model_name} Model Test RMSE: {best_model_rmse_test}\")\n\n    if not predict:\n        return best_model_mse_train, best_model_mse_test, best_model_rmse_train, best_model_rmse_test\n    else:\n        return list(y_predict)\n\n\n_ = selector_result(selector_lin_reg_cv, X_train, y_train, X_test, y_test, \"Linear Regression (Cross-Validation)\")\n\nOptimal number of features for Linear Regression (Cross-Validation): 5\nSelected features for Linear Linear Regression (Cross-Validation): Index(['TE3_day-1', 'TE1_day-4', 'TE2_day-4', 'DE5_day-4', 'DE6_day-4'], dtype='object')\nBest Validation MSE: 0.00010527116357773034\n-----------------------------\nBest Linear Regression (Cross-Validation) Model Train MSE: 0.00012614083944832163\nBest Linear Regression (Cross-Validation) Model Test MSE: 5.793372382508834e-05\nBest Linear Regression (Cross-Validation) Model Train RMSE: 0.011231243895861297\nBest Linear Regression (Cross-Validation) Model Test RMSE: 0.007611420618063906"
  },
  {
    "objectID": "nasdaq-prediction.html#tree-based-models-for-regression",
    "href": "nasdaq-prediction.html#tree-based-models-for-regression",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "3. Tree Based Models for Regression",
    "text": "3. Tree Based Models for Regression\nWe implement the same feature selection strategy for tree models\n\n3.1 Decision Tree\n\n3.1.1 Feature Selection\n\n# Time Series Cross-Validation\ntscv = TimeSeriesSplit(n_splits=3)\n\nstep_dt = 1\n\n# Initialize RFECV\nselector_dt = RFECV(\n    estimator=DecisionTreeRegressor(max_depth=10, random_state=42),\n    step=step_dt,  # Remove 1 feature at each step\n    cv=tscv,      # Time series split\n    verbose=1,\n    n_jobs=-1,\n    scoring='neg_mean_squared_error'  # Negative MSE for scoring\n)\n\n# Train and automatically select optimal number of features\nselector_dt.fit(X_train, y_train)\n\nFitting estimator with 880 features.\nFitting estimator with 879 features.\nFitting estimator with 878 features.\nFitting estimator with 877 features.\nFitting estimator with 876 features.\nFitting estimator with 875 features.\nFitting estimator with 874 features.\nFitting estimator with 873 features.\nFitting estimator with 872 features.\nFitting estimator with 871 features.\nFitting estimator with 870 features.\nFitting estimator with 869 features.\nFitting estimator with 868 features.\nFitting estimator with 867 features.\nFitting estimator with 866 features.\nFitting estimator with 865 features.\nFitting estimator with 864 features.\nFitting estimator with 863 features.\nFitting estimator with 862 features.\nFitting estimator with 861 features.\nFitting estimator with 860 features.\nFitting estimator with 859 features.\nFitting estimator with 858 features.\nFitting estimator with 857 features.\nFitting estimator with 856 features.\nFitting estimator with 855 features.\nFitting estimator with 854 features.\nFitting estimator with 853 features.\nFitting estimator with 852 features.\nFitting estimator with 851 features.\nFitting estimator with 850 features.\nFitting estimator with 849 features.\nFitting estimator with 848 features.\nFitting estimator with 847 features.\nFitting estimator with 846 features.\nFitting estimator with 845 features.\nFitting estimator with 844 features.\nFitting estimator with 843 features.\nFitting estimator with 842 features.\nFitting estimator with 841 features.\nFitting estimator with 840 features.\nFitting estimator with 839 features.\nFitting estimator with 838 features.\nFitting estimator with 837 features.\nFitting estimator with 836 features.\nFitting estimator with 835 features.\nFitting estimator with 834 features.\nFitting estimator with 833 features.\nFitting estimator with 832 features.\nFitting estimator with 831 features.\nFitting estimator with 830 features.\nFitting estimator with 829 features.\nFitting estimator with 828 features.\nFitting estimator with 827 features.\nFitting estimator with 826 features.\nFitting estimator with 825 features.\nFitting estimator with 824 features.\nFitting estimator with 823 features.\nFitting estimator with 822 features.\nFitting estimator with 821 features.\nFitting estimator with 820 features.\nFitting estimator with 819 features.\nFitting estimator with 818 features.\nFitting estimator with 817 features.\nFitting estimator with 816 features.\nFitting estimator with 815 features.\nFitting estimator with 814 features.\nFitting estimator with 813 features.\nFitting estimator with 812 features.\nFitting estimator with 811 features.\nFitting estimator with 810 features.\nFitting estimator with 809 features.\nFitting estimator with 808 features.\nFitting estimator with 807 features.\nFitting estimator with 806 features.\nFitting estimator with 805 features.\nFitting estimator with 804 features.\nFitting estimator with 803 features.\nFitting estimator with 802 features.\nFitting estimator with 801 features.\nFitting estimator with 800 features.\nFitting estimator with 799 features.\nFitting estimator with 798 features.\nFitting estimator with 797 features.\nFitting estimator with 796 features.\nFitting estimator with 795 features.\nFitting estimator with 794 features.\nFitting estimator with 793 features.\nFitting estimator with 792 features.\nFitting estimator with 791 features.\nFitting estimator with 790 features.\nFitting estimator with 789 features.\nFitting estimator with 788 features.\nFitting estimator with 787 features.\nFitting estimator with 786 features.\nFitting estimator with 785 features.\nFitting estimator with 784 features.\nFitting estimator with 783 features.\nFitting estimator with 782 features.\nFitting estimator with 781 features.\nFitting estimator with 780 features.\nFitting estimator with 779 features.\nFitting estimator with 778 features.\nFitting estimator with 777 features.\nFitting estimator with 776 features.\nFitting estimator with 775 features.\nFitting estimator with 774 features.\nFitting estimator with 773 features.\nFitting estimator with 772 features.\nFitting estimator with 771 features.\nFitting estimator with 770 features.\nFitting estimator with 769 features.\nFitting estimator with 768 features.\nFitting estimator with 767 features.\nFitting estimator with 766 features.\nFitting estimator with 765 features.\nFitting estimator with 764 features.\nFitting estimator with 763 features.\nFitting estimator with 762 features.\nFitting estimator with 761 features.\nFitting estimator with 760 features.\nFitting estimator with 759 features.\nFitting estimator with 758 features.\nFitting estimator with 757 features.\nFitting estimator with 756 features.\nFitting estimator with 755 features.\nFitting estimator with 754 features.\nFitting estimator with 753 features.\nFitting estimator with 752 features.\nFitting estimator with 751 features.\nFitting estimator with 750 features.\nFitting estimator with 749 features.\nFitting estimator with 748 features.\nFitting estimator with 747 features.\nFitting estimator with 746 features.\nFitting estimator with 745 features.\nFitting estimator with 744 features.\nFitting estimator with 743 features.\nFitting estimator with 742 features.\nFitting estimator with 741 features.\nFitting estimator with 740 features.\nFitting estimator with 739 features.\nFitting estimator with 738 features.\nFitting estimator with 737 features.\nFitting estimator with 736 features.\nFitting estimator with 735 features.\nFitting estimator with 734 features.\nFitting estimator with 733 features.\nFitting estimator with 732 features.\nFitting estimator with 731 features.\nFitting estimator with 730 features.\nFitting estimator with 729 features.\nFitting estimator with 728 features.\nFitting estimator with 727 features.\nFitting estimator with 726 features.\nFitting estimator with 725 features.\nFitting estimator with 724 features.\nFitting estimator with 723 features.\nFitting estimator with 722 features.\nFitting estimator with 721 features.\nFitting estimator with 720 features.\nFitting estimator with 719 features.\nFitting estimator with 718 features.\nFitting estimator with 717 features.\nFitting estimator with 716 features.\nFitting estimator with 715 features.\nFitting estimator with 714 features.\nFitting estimator with 713 features.\nFitting estimator with 712 features.\nFitting estimator with 711 features.\nFitting estimator with 710 features.\nFitting estimator with 709 features.\nFitting estimator with 708 features.\nFitting estimator with 707 features.\nFitting estimator with 706 features.\nFitting estimator with 705 features.\nFitting estimator with 704 features.\nFitting estimator with 703 features.\nFitting estimator with 702 features.\nFitting estimator with 701 features.\nFitting estimator with 700 features.\nFitting estimator with 699 features.\nFitting estimator with 698 features.\nFitting estimator with 697 features.\nFitting estimator with 696 features.\nFitting estimator with 695 features.\nFitting estimator with 694 features.\nFitting estimator with 693 features.\nFitting estimator with 692 features.\nFitting estimator with 691 features.\nFitting estimator with 690 features.\nFitting estimator with 689 features.\nFitting estimator with 688 features.\nFitting estimator with 687 features.\nFitting estimator with 686 features.\nFitting estimator with 685 features.\nFitting estimator with 684 features.\nFitting estimator with 683 features.\nFitting estimator with 682 features.\nFitting estimator with 681 features.\nFitting estimator with 680 features.\nFitting estimator with 679 features.\nFitting estimator with 678 features.\nFitting estimator with 677 features.\nFitting estimator with 676 features.\nFitting estimator with 675 features.\nFitting estimator with 674 features.\nFitting estimator with 673 features.\nFitting estimator with 672 features.\nFitting estimator with 671 features.\nFitting estimator with 670 features.\nFitting estimator with 669 features.\nFitting estimator with 668 features.\nFitting estimator with 667 features.\nFitting estimator with 666 features.\nFitting estimator with 665 features.\nFitting estimator with 664 features.\nFitting estimator with 663 features.\nFitting estimator with 662 features.\nFitting estimator with 661 features.\nFitting estimator with 660 features.\nFitting estimator with 659 features.\nFitting estimator with 658 features.\nFitting estimator with 657 features.\nFitting estimator with 656 features.\nFitting estimator with 655 features.\nFitting estimator with 654 features.\nFitting estimator with 653 features.\nFitting estimator with 652 features.\nFitting estimator with 651 features.\nFitting estimator with 650 features.\nFitting estimator with 649 features.\nFitting estimator with 648 features.\nFitting estimator with 647 features.\nFitting estimator with 646 features.\nFitting estimator with 645 features.\nFitting estimator with 644 features.\nFitting estimator with 643 features.\nFitting estimator with 642 features.\nFitting estimator with 641 features.\nFitting estimator with 640 features.\nFitting estimator with 639 features.\nFitting estimator with 638 features.\nFitting estimator with 637 features.\nFitting estimator with 636 features.\nFitting estimator with 635 features.\nFitting estimator with 634 features.\nFitting estimator with 633 features.\nFitting estimator with 632 features.\nFitting estimator with 631 features.\nFitting estimator with 630 features.\nFitting estimator with 629 features.\nFitting estimator with 628 features.\nFitting estimator with 627 features.\nFitting estimator with 626 features.\nFitting estimator with 625 features.\nFitting estimator with 624 features.\nFitting estimator with 623 features.\nFitting estimator with 622 features.\nFitting estimator with 621 features.\nFitting estimator with 620 features.\nFitting estimator with 619 features.\nFitting estimator with 618 features.\nFitting estimator with 617 features.\nFitting estimator with 616 features.\nFitting estimator with 615 features.\nFitting estimator with 614 features.\nFitting estimator with 613 features.\nFitting estimator with 612 features.\nFitting estimator with 611 features.\nFitting estimator with 610 features.\nFitting estimator with 609 features.\nFitting estimator with 608 features.\nFitting estimator with 607 features.\nFitting estimator with 606 features.\nFitting estimator with 605 features.\nFitting estimator with 604 features.\nFitting estimator with 603 features.\nFitting estimator with 602 features.\nFitting estimator with 601 features.\nFitting estimator with 600 features.\nFitting estimator with 599 features.\nFitting estimator with 598 features.\nFitting estimator with 597 features.\nFitting estimator with 596 features.\nFitting estimator with 595 features.\nFitting estimator with 594 features.\nFitting estimator with 593 features.\nFitting estimator with 592 features.\nFitting estimator with 591 features.\nFitting estimator with 590 features.\nFitting estimator with 589 features.\nFitting estimator with 588 features.\nFitting estimator with 587 features.\nFitting estimator with 586 features.\nFitting estimator with 585 features.\nFitting estimator with 584 features.\nFitting estimator with 583 features.\nFitting estimator with 582 features.\nFitting estimator with 581 features.\nFitting estimator with 580 features.\nFitting estimator with 579 features.\nFitting estimator with 578 features.\nFitting estimator with 577 features.\nFitting estimator with 576 features.\nFitting estimator with 575 features.\nFitting estimator with 574 features.\nFitting estimator with 573 features.\nFitting estimator with 572 features.\nFitting estimator with 571 features.\nFitting estimator with 570 features.\nFitting estimator with 569 features.\nFitting estimator with 568 features.\nFitting estimator with 567 features.\nFitting estimator with 566 features.\nFitting estimator with 565 features.\nFitting estimator with 564 features.\nFitting estimator with 563 features.\nFitting estimator with 562 features.\nFitting estimator with 561 features.\nFitting estimator with 560 features.\nFitting estimator with 559 features.\nFitting estimator with 558 features.\nFitting estimator with 557 features.\nFitting estimator with 556 features.\nFitting estimator with 555 features.\nFitting estimator with 554 features.\nFitting estimator with 553 features.\nFitting estimator with 552 features.\nFitting estimator with 551 features.\nFitting estimator with 550 features.\nFitting estimator with 549 features.\nFitting estimator with 548 features.\nFitting estimator with 547 features.\nFitting estimator with 546 features.\nFitting estimator with 545 features.\nFitting estimator with 544 features.\nFitting estimator with 543 features.\nFitting estimator with 542 features.\nFitting estimator with 541 features.\nFitting estimator with 540 features.\nFitting estimator with 539 features.\nFitting estimator with 538 features.\nFitting estimator with 537 features.\nFitting estimator with 536 features.\nFitting estimator with 535 features.\nFitting estimator with 534 features.\nFitting estimator with 533 features.\nFitting estimator with 532 features.\nFitting estimator with 531 features.\nFitting estimator with 530 features.\nFitting estimator with 529 features.\nFitting estimator with 528 features.\nFitting estimator with 527 features.\nFitting estimator with 526 features.\nFitting estimator with 525 features.\nFitting estimator with 524 features.\nFitting estimator with 523 features.\nFitting estimator with 522 features.\nFitting estimator with 521 features.\nFitting estimator with 520 features.\nFitting estimator with 519 features.\nFitting estimator with 518 features.\nFitting estimator with 517 features.\nFitting estimator with 516 features.\nFitting estimator with 515 features.\nFitting estimator with 514 features.\nFitting estimator with 513 features.\nFitting estimator with 512 features.\nFitting estimator with 511 features.\nFitting estimator with 510 features.\nFitting estimator with 509 features.\nFitting estimator with 508 features.\nFitting estimator with 507 features.\nFitting estimator with 506 features.\nFitting estimator with 505 features.\nFitting estimator with 504 features.\nFitting estimator with 503 features.\nFitting estimator with 502 features.\nFitting estimator with 501 features.\nFitting estimator with 500 features.\nFitting estimator with 499 features.\nFitting estimator with 498 features.\nFitting estimator with 497 features.\nFitting estimator with 496 features.\nFitting estimator with 495 features.\nFitting estimator with 494 features.\nFitting estimator with 493 features.\nFitting estimator with 492 features.\nFitting estimator with 491 features.\nFitting estimator with 490 features.\nFitting estimator with 489 features.\nFitting estimator with 488 features.\nFitting estimator with 487 features.\nFitting estimator with 486 features.\nFitting estimator with 485 features.\nFitting estimator with 484 features.\nFitting estimator with 483 features.\nFitting estimator with 482 features.\nFitting estimator with 481 features.\nFitting estimator with 480 features.\nFitting estimator with 479 features.\nFitting estimator with 478 features.\nFitting estimator with 477 features.\nFitting estimator with 476 features.\nFitting estimator with 475 features.\nFitting estimator with 474 features.\nFitting estimator with 473 features.\nFitting estimator with 472 features.\nFitting estimator with 471 features.\nFitting estimator with 470 features.\nFitting estimator with 469 features.\nFitting estimator with 468 features.\nFitting estimator with 467 features.\nFitting estimator with 466 features.\nFitting estimator with 465 features.\nFitting estimator with 464 features.\nFitting estimator with 463 features.\nFitting estimator with 462 features.\nFitting estimator with 461 features.\nFitting estimator with 460 features.\nFitting estimator with 459 features.\nFitting estimator with 458 features.\nFitting estimator with 457 features.\nFitting estimator with 456 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\n\n\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=DecisionTreeRegressor(max_depth=10, random_state=42), n_jobs=-1,\n      scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RFECVRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=DecisionTreeRegressor(max_depth=10, random_state=42), n_jobs=-1,\n      scoring='neg_mean_squared_error', verbose=1)estimator: DecisionTreeRegressorDecisionTreeRegressor(max_depth=10, random_state=42)DecisionTreeRegressorDecisionTreeRegressor(max_depth=10, random_state=42)\n\n\n\n# Save the model\njoblib.dump(selector_dt, \"selector_dt.joblib\")\n\n['selector_dt.joblib']\n\n\n\nplot_n_feature_val_loss(selector_dt, step=step_dt, model_name='Decision Tree', mark=None)\n\n\n\n\n\n\n\n\n\n_ = selector_result(selector_dt, X_train, y_train, X_test, y_test, \"Decision Tree\")\n\nOptimal number of features for Decision Tree: 2\nSelected features for Linear Decision Tree: Index(['ROC_10_day-1', 'NASDAQ-F_day-3'], dtype='object')\nBest Validation MSE: 0.00013017646633121612\n-----------------------------\nBest Decision Tree Model Train MSE: 9.389818141190975e-05\nBest Decision Tree Model Test MSE: 5.319664601451538e-05\nBest Decision Tree Model Train RMSE: 0.00969010739940016\nBest Decision Tree Model Test RMSE: 0.0072936030886329\n\n\n\n\n3.1.2 Hyperparameter Optimization\n\n# Get selected features\nX_train_selected_dt = X_train.iloc[:, selector_dt.support_]\n\n# Hyperparameter optimization\nparam_grid_dt = {\n    'max_depth': [2, 3, 4, 6, 8, 10, None],\n    'min_samples_split': [2, 3, 4, 6, 8, 10],\n}\ngrid_search_dt = GridSearchCV(\n    estimator=DecisionTreeRegressor(random_state=42),\n    param_grid=param_grid_dt,\n    cv=tscv,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    verbose=1,\n)\ngrid_search_dt.fit(X_train_selected_dt, y_train)\n\nFitting 3 folds for each of 42 candidates, totalling 126 fits\n\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=DecisionTreeRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [2, 3, 4, 6, 8, 10, None],\n                         'min_samples_split': [2, 3, 4, 6, 8, 10]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=DecisionTreeRegressor(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [2, 3, 4, 6, 8, 10, None],\n                         'min_samples_split': [2, 3, 4, 6, 8, 10]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=42)DecisionTreeRegressorDecisionTreeRegressor(random_state=42)\n\n\n\ndef two_hyperparameter_visualization(grid_search, hp1, hp2, model_name,\n                                     criterion='mean_test_score',\n                                     criterion_name='RMSE'):\n    # Extract results\n    results = grid_search.cv_results_\n    mean_scores = np.sqrt(-results[criterion])  # Convert to positive RMSE\n    param_1 = grid_search.param_grid[hp1]\n    param_2 = grid_search.param_grid[hp2]\n\n    # Reshape scores into a matrix\n    scores_matrix = np.reshape(mean_scores, (len(param_1), len(param_2)))\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(\n        scores_matrix,\n        annot=True,\n        fmt=\".3f\",\n        xticklabels=param_1,\n        yticklabels=param_2,\n        cmap=\"viridis\",\n    )\n    plt.xlabel(hp1)\n    plt.ylabel(hp2)\n    plt.title(\n        f\"{model_name} Decision Tree Hyperparameter Tuning Results \"\n        f\"({criterion_name})\"\n    )\n    plt.show()\n\n\ntwo_hyperparameter_visualization(\n    grid_search_dt, 'max_depth', 'min_samples_split', 'Decision Tree'\n)\n\n\n\n\n\n\n\n\n\n# Evaluate the final model\nbest_model_dt_final = grid_search_dt.best_estimator_\nbest_model_dt_final.fit(\n    X_train[X.columns[selector_dt.support_]], y_train\n)\n\n# Save the model\njoblib.dump(best_model_dt_final, 'best_model_dt_final.joblib')\n\n\ndef grid_search_result(grid_search, selector, final_model, model_name='', print_out=True):\n    best_model_mse_train_final = mean_squared_error(\n        final_model.predict(X_train[X.columns[selector.support_]]), y_train\n    )\n    best_model_mse_test_final = mean_squared_error(\n        final_model.predict(X_test[X.columns[selector.support_]]), y_test\n    )\n    best_model_rmse_train_final = np.sqrt(best_model_mse_train_final)\n    best_model_rmse_test_final = np.sqrt(best_model_mse_test_final)\n\n    if print_out:\n        print(f\"{model_name} Best hyperparameters:\", grid_search.best_params_)\n        print(f\"{model_name} Best Validation MSE:\", -grid_search.best_score_)\n        print(\"-----------------------------\")\n        print(f\"{model_name} Best Model Train MSE:\", best_model_mse_train_final)\n        print(f\"{model_name} Best Model Test MSE:\", best_model_mse_test_final)\n        print(f\"{model_name} Best Model Train RMSE:\", best_model_rmse_train_final)\n        print(f\"{model_name} Best Model Test RMSE:\", best_model_rmse_test_final)\n\n    return (\n        best_model_mse_train_final,\n        best_model_mse_test_final,\n        best_model_rmse_train_final,\n        best_model_rmse_test_final,\n    )\n\n\n_ = grid_search_result(grid_search_dt, selector_dt, best_model_dt_final, 'Decision Tree')\n\nDecision Tree Best hyperparameters: {'max_depth': 3, 'min_samples_split': 3}\nDecision Tree Best Validation MSE: 0.00010894315706034298\n-----------------------------\nDecision Tree Best Model Train MSE: 0.00011617992780627547\nDecision Tree Best Model Test MSE: 5.0685852201722405e-05\nDecision Tree Best Model Train RMSE: 0.010778679316422558\nDecision Tree Best Model Test RMSE: 0.007119399707961508\n\n\n\n\n\n3.2 Random Forest\n\n3.2.1 Feature Selection\nBecause of the limited computing resource we have, we remove 5 features at each step in the RFE process.\n\n# Time Series Cross-Validation\ntscv = TimeSeriesSplit(n_splits=3)\n\nstep_rf = 5\n\n# Initialize RFECV, select some hyperparameters\nselector_rf = RFECV(\n    estimator=RandomForestRegressor(\n        n_estimators=30,       # Fewer trees for speed (but not too few)\n        max_depth=10,          # Limit tree depth to avoid overfitting\n        max_samples=0.6,       # Use 60% of data per tree (balance speed and diversity)\n        min_samples_split=10,  # Prevent splits on small nodes\n        min_samples_leaf=15,   # Require larger leaves\n        max_features='log2',   # log2(800) ≈ 9 features per split (faster than 'sqrt')\n        n_jobs=-1,             # Use all CPU cores\n        random_state=42,       # Reproducibility\n        bootstrap=True,        # Bootstrap sampling for diversity\n    ),\n    step=step_rf,             # Remove 5 features at each step\n    cv=tscv,                  # Time series split\n    verbose=1,\n    scoring='neg_mean_squared_error'  # Negative MSE for scoring\n)\n\n# Train and automatically select optimal number of features\nselector_rf.fit(X_train, y_train)\n\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\n\n\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=RandomForestRegressor(max_depth=10, max_features='log2',\n                                      max_samples=0.6, min_samples_leaf=15,\n                                      min_samples_split=10, n_estimators=30,\n                                      n_jobs=-1, random_state=42),\n      scoring='neg_mean_squared_error', step=5, verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RFECVRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=RandomForestRegressor(max_depth=10, max_features='log2',\n                                      max_samples=0.6, min_samples_leaf=15,\n                                      min_samples_split=10, n_estimators=30,\n                                      n_jobs=-1, random_state=42),\n      scoring='neg_mean_squared_error', step=5, verbose=1)estimator: RandomForestRegressorRandomForestRegressor(max_depth=10, max_features='log2', max_samples=0.6,\n                      min_samples_leaf=15, min_samples_split=10,\n                      n_estimators=30, n_jobs=-1, random_state=42)RandomForestRegressorRandomForestRegressor(max_depth=10, max_features='log2', max_samples=0.6,\n                      min_samples_leaf=15, min_samples_split=10,\n                      n_estimators=30, n_jobs=-1, random_state=42)\n\n\n\n# Save the model\njoblib.dump(selector_rf, 'selector_rf.joblib')\n\n['selector_rf.joblib']\n\n\n\nplot_n_feature_val_loss(selector_rf, step=step_rf, model_name='Random Forest', mark='x')\n\n\n\n\n\n\n\n\n\nselector_result(selector_rf, X_train, y_train, X_test, y_test, 'Random Forest')\n\nOptimal number of features for Random Forest: 535\nSelected features for Linear Random Forest: Index(['ROC_20_day-1', 'ROC_20_day-1^2', 'EMA_10_day-1', 'EMA_10_day-1^2',\n       'EMA_20_day-1', 'EMA_20_day-1^2', 'EMA_50_day-1', 'EMA_50_day-1^2',\n       'DTB4WK_day-1', 'DTB4WK_day-1^2',\n       ...\n       'Dollar index-F_day-5^2', 'Dollar index_day-5', 'Dollar index_day-5^2',\n       'wheat-F_day-5', 'wheat-F_day-5^2', 'XAG_day-5', 'XAG_day-5^2',\n       'XAU_day-5', 'XAU_day-5^2', 'day_of_week_day-5'],\n      dtype='object', length=535)\nBest Validation MSE: 0.00010155466999754498\n-----------------------------\nBest Random Forest Model Train MSE: 0.00010452561376682597\nBest Random Forest Model Test MSE: 5.417641101158336e-05\nBest Random Forest Model Train RMSE: 0.010223776883658307\nBest Random Forest Model Test RMSE: 0.007360462690047641\n\n\n(0.00010452561376682597,\n 5.417641101158336e-05,\n 0.010223776883658307,\n 0.007360462690047641)\n\n\n\n\n3.2.2 Hyperparameter Optimization\n\n# Get selected features\nX_train_selected_rf = X_train.iloc[:, selector_rf.support_]\n\n# Hyperparameter optimization\nparam_grid_rf = {\n    'n_estimators': [50, 100, 150],      # Fewer trees than default (balance speed and stability)\n    'max_depth': [5, 10, 15],            # Shallower trees to prevent overfitting\n    'min_samples_split': [10, 20, 30],   # Larger splits for noisy data\n    'min_samples_leaf': [5, 10, 15],     # Larger leaves for regularization\n    'max_features': ['log2', 0.05],      # Fewer features per split (critical for high dimensions)\n    'bootstrap': [True],               # Bootstrap sampling for robustness\n    'max_samples': [0.6, 0.8],           # Subsample data to increase diversity\n}\ngrid_search_rf = GridSearchCV(\n    estimator=RandomForestRegressor(n_jobs=-1),\n    param_grid=param_grid_rf,\n    cv=tscv,\n    scoring='neg_mean_squared_error',\n    verbose=1,\n)\ngrid_search_rf.fit(X_train_selected_rf, y_train)\n\nFitting 3 folds for each of 324 candidates, totalling 972 fits\n\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=RandomForestRegressor(n_jobs=-1),\n             param_grid={'bootstrap': [True], 'max_depth': [5, 10, 15],\n                         'max_features': ['log2', 0.05],\n                         'max_samples': [0.6, 0.8],\n                         'min_samples_leaf': [5, 10, 15],\n                         'min_samples_split': [10, 20, 30],\n                         'n_estimators': [50, 100, 150]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=RandomForestRegressor(n_jobs=-1),\n             param_grid={'bootstrap': [True], 'max_depth': [5, 10, 15],\n                         'max_features': ['log2', 0.05],\n                         'max_samples': [0.6, 0.8],\n                         'min_samples_leaf': [5, 10, 15],\n                         'min_samples_split': [10, 20, 30],\n                         'n_estimators': [50, 100, 150]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: RandomForestRegressorRandomForestRegressor(n_jobs=-1)RandomForestRegressorRandomForestRegressor(n_jobs=-1)\n\n\nHere’s the optimized parameters for random forest.\n\n# Evaluate the final model\nbest_model_rf_final = grid_search_rf.best_estimator_\nbest_model_rf_final.fit(X_train[X.columns[selector_rf.support_]], y_train)\n\n# Save the model\njoblib.dump(best_model_rf_final, 'best_model_rf_final.joblib')\n\n_ = grid_search_result(grid_search_rf, selector_rf, best_model_rf_final, 'Random Forest')\n\nRandom Forest Best hyperparameters: {'bootstrap': True, 'max_depth': 5, 'max_features': 'log2', 'max_samples': 0.8, 'min_samples_leaf': 15, 'min_samples_split': 10, 'n_estimators': 50}\nRandom Forest Best Validation MSE: 0.00010222025743517132\n-----------------------------\nRandom Forest Best Model Train MSE: 0.00011301699070842933\nRandom Forest Best Model Test MSE: 5.164476926179037e-05\nRandom Forest Best Model Train RMSE: 0.010630944958395248\nRandom Forest Best Model Test RMSE: 0.0071864295211036735\n\n\n\n\n\n3.3 XGBoost\n\n3.3.1 Feature Selection\n\n# Time Series Cross-Validation\ntscv = TimeSeriesSplit(n_splits=3)\n\nstep_xgb = 5\n\n# Initialize RFECV\nselector_xgb = RFECV(\n    estimator=XGBRegressor(\n        n_estimators=50,  # Number of trees (balance speed and performance)\n        max_depth=5,  # Limit tree depth to prevent overfitting\n        subsample=0.8,  # Use 80% of samples per tree (introduce diversity)\n        min_child_weight=3,  # Minimum sum of instance weights (controls splits)\n        tree_method='hist',  # Histogram-based tree method (faster than 'exact')\n        device='cuda',  # Use GPU for training (if available)\n        random_state=42,  # Reproducibility\n        n_jobs=-1  # Use all CPU cores for parallel processing\n    ),\n    step=step_xgb,  # Remove 5 features at each step\n    cv=tscv,  # Time series split\n    verbose=1,\n    scoring='neg_mean_squared_error'  # Negative MSE for scoring\n)\n\n# Train and automatically select optimal number of features\nselector_xgb.fit(X_train, y_train)\n\n\n\nFitting estimator with 880 features.\n\n\nC:\\Users\\zln92\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n\n\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\nFitting estimator with 880 features.\nFitting estimator with 875 features.\nFitting estimator with 870 features.\nFitting estimator with 865 features.\nFitting estimator with 860 features.\nFitting estimator with 855 features.\nFitting estimator with 850 features.\nFitting estimator with 845 features.\nFitting estimator with 840 features.\nFitting estimator with 835 features.\nFitting estimator with 830 features.\nFitting estimator with 825 features.\nFitting estimator with 820 features.\nFitting estimator with 815 features.\nFitting estimator with 810 features.\nFitting estimator with 805 features.\nFitting estimator with 800 features.\nFitting estimator with 795 features.\nFitting estimator with 790 features.\nFitting estimator with 785 features.\nFitting estimator with 780 features.\nFitting estimator with 775 features.\nFitting estimator with 770 features.\nFitting estimator with 765 features.\nFitting estimator with 760 features.\nFitting estimator with 755 features.\nFitting estimator with 750 features.\nFitting estimator with 745 features.\nFitting estimator with 740 features.\nFitting estimator with 735 features.\nFitting estimator with 730 features.\nFitting estimator with 725 features.\nFitting estimator with 720 features.\nFitting estimator with 715 features.\nFitting estimator with 710 features.\nFitting estimator with 705 features.\nFitting estimator with 700 features.\nFitting estimator with 695 features.\nFitting estimator with 690 features.\nFitting estimator with 685 features.\nFitting estimator with 680 features.\nFitting estimator with 675 features.\nFitting estimator with 670 features.\nFitting estimator with 665 features.\nFitting estimator with 660 features.\nFitting estimator with 655 features.\nFitting estimator with 650 features.\nFitting estimator with 645 features.\nFitting estimator with 640 features.\nFitting estimator with 635 features.\nFitting estimator with 630 features.\nFitting estimator with 625 features.\nFitting estimator with 620 features.\nFitting estimator with 615 features.\nFitting estimator with 610 features.\nFitting estimator with 605 features.\nFitting estimator with 600 features.\nFitting estimator with 595 features.\nFitting estimator with 590 features.\nFitting estimator with 585 features.\nFitting estimator with 580 features.\nFitting estimator with 575 features.\nFitting estimator with 570 features.\nFitting estimator with 565 features.\nFitting estimator with 560 features.\nFitting estimator with 555 features.\nFitting estimator with 550 features.\nFitting estimator with 545 features.\nFitting estimator with 540 features.\nFitting estimator with 535 features.\nFitting estimator with 530 features.\nFitting estimator with 525 features.\nFitting estimator with 520 features.\nFitting estimator with 515 features.\nFitting estimator with 510 features.\nFitting estimator with 505 features.\nFitting estimator with 500 features.\nFitting estimator with 495 features.\nFitting estimator with 490 features.\nFitting estimator with 485 features.\nFitting estimator with 480 features.\nFitting estimator with 475 features.\nFitting estimator with 470 features.\nFitting estimator with 465 features.\nFitting estimator with 460 features.\nFitting estimator with 455 features.\nFitting estimator with 450 features.\nFitting estimator with 445 features.\nFitting estimator with 440 features.\nFitting estimator with 435 features.\nFitting estimator with 430 features.\nFitting estimator with 425 features.\nFitting estimator with 420 features.\nFitting estimator with 415 features.\nFitting estimator with 410 features.\nFitting estimator with 405 features.\nFitting estimator with 400 features.\nFitting estimator with 395 features.\nFitting estimator with 390 features.\nFitting estimator with 385 features.\nFitting estimator with 380 features.\nFitting estimator with 375 features.\nFitting estimator with 370 features.\nFitting estimator with 365 features.\nFitting estimator with 360 features.\nFitting estimator with 355 features.\nFitting estimator with 350 features.\nFitting estimator with 345 features.\nFitting estimator with 340 features.\nFitting estimator with 335 features.\nFitting estimator with 330 features.\nFitting estimator with 325 features.\nFitting estimator with 320 features.\nFitting estimator with 315 features.\nFitting estimator with 310 features.\nFitting estimator with 305 features.\nFitting estimator with 300 features.\nFitting estimator with 295 features.\nFitting estimator with 290 features.\nFitting estimator with 285 features.\nFitting estimator with 280 features.\nFitting estimator with 275 features.\nFitting estimator with 270 features.\nFitting estimator with 265 features.\nFitting estimator with 260 features.\nFitting estimator with 255 features.\nFitting estimator with 250 features.\nFitting estimator with 245 features.\nFitting estimator with 240 features.\nFitting estimator with 235 features.\nFitting estimator with 230 features.\nFitting estimator with 225 features.\nFitting estimator with 220 features.\nFitting estimator with 215 features.\nFitting estimator with 210 features.\nFitting estimator with 205 features.\nFitting estimator with 200 features.\nFitting estimator with 195 features.\nFitting estimator with 190 features.\nFitting estimator with 185 features.\nFitting estimator with 180 features.\nFitting estimator with 175 features.\nFitting estimator with 170 features.\nFitting estimator with 165 features.\nFitting estimator with 160 features.\nFitting estimator with 155 features.\nFitting estimator with 150 features.\nFitting estimator with 145 features.\nFitting estimator with 140 features.\nFitting estimator with 135 features.\nFitting estimator with 130 features.\nFitting estimator with 125 features.\nFitting estimator with 120 features.\nFitting estimator with 115 features.\nFitting estimator with 110 features.\nFitting estimator with 105 features.\nFitting estimator with 100 features.\nFitting estimator with 95 features.\nFitting estimator with 90 features.\nFitting estimator with 85 features.\nFitting estimator with 80 features.\nFitting estimator with 75 features.\nFitting estimator with 70 features.\nFitting estimator with 65 features.\nFitting estimator with 60 features.\nFitting estimator with 55 features.\nFitting estimator with 50 features.\nFitting estimator with 45 features.\nFitting estimator with 40 features.\nFitting estimator with 35 features.\nFitting estimator with 30 features.\nFitting estimator with 25 features.\nFitting estimator with 20 features.\nFitting estimator with 15 features.\nFitting estimator with 10 features.\nFitting estimator with 5 features.\n\n\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=XGBRegressor(base_score=None, booster=None, callbacks=None,\n                             colsample_bylevel=None, colsample_bynode=None,\n                             colsample_bytree=None, device='cuda',\n                             early_stopping_rounds=None,\n                             enable_categorical=False, eval_metric=None,\n                             feature_types=None, gamma=None, grow_policy=None,\n                             importan...teraction_constraints=None,\n                             learning_rate=None, max_bin=None,\n                             max_cat_threshold=None, max_cat_to_onehot=None,\n                             max_delta_step=None, max_depth=5, max_leaves=None,\n                             min_child_weight=3, missing=nan,\n                             monotone_constraints=None, multi_strategy=None,\n                             n_estimators=50, n_jobs=-1, num_parallel_tree=None,\n                             random_state=42, ...),\n      scoring='neg_mean_squared_error', step=5, verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RFECVRFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n      estimator=XGBRegressor(base_score=None, booster=None, callbacks=None,\n                             colsample_bylevel=None, colsample_bynode=None,\n                             colsample_bytree=None, device='cuda',\n                             early_stopping_rounds=None,\n                             enable_categorical=False, eval_metric=None,\n                             feature_types=None, gamma=None, grow_policy=None,\n                             importan...teraction_constraints=None,\n                             learning_rate=None, max_bin=None,\n                             max_cat_threshold=None, max_cat_to_onehot=None,\n                             max_delta_step=None, max_depth=5, max_leaves=None,\n                             min_child_weight=3, missing=nan,\n                             monotone_constraints=None, multi_strategy=None,\n                             n_estimators=50, n_jobs=-1, num_parallel_tree=None,\n                             random_state=42, ...),\n      scoring='neg_mean_squared_error', step=5, verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=3, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=50, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=3, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=50, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\n# Save the model\njoblib.dump(selector_xgb, 'selector_xgb.joblib')\n\n['selector_xgb.joblib']\n\n\n\nplot_n_feature_val_loss(selector_xgb, step=step_xgb, model_name='XGBoost', mark='x')\n\n\n\n\n\n\n\n\n\nselector_result(selector_xgb, X_train, y_train, X_test, y_test, 'XGBoost')\n\nOptimal number of features for XGBoost: 1\nSelected features for Linear XGBoost: Index(['ROC_20_day-1'], dtype='object')\nBest Validation MSE: 0.00012259275374271517\n-----------------------------\nBest XGBoost Model Train MSE: 0.00010600596254067053\nBest XGBoost Model Test MSE: 5.943372552324083e-05\nBest XGBoost Model Train RMSE: 0.010295919703487907\nBest XGBoost Model Test RMSE: 0.007709327177078479\n\n\n(0.00010600596254067053,\n 5.943372552324083e-05,\n 0.010295919703487907,\n 0.007709327177078479)\n\n\n\n\n3.3.2 Hyperparameter Optimization\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n\n# Time Series Cross-Validation\ntscv = TimeSeriesSplit(n_splits=3)\n\n# Initialize GridSearchCV\nparam_grid_xgb = {\n    'n_estimators': [50, 100],  # Number of trees\n    'max_depth': [3, 5, 7],  # Maximum tree depth\n    'subsample': [0.8, 1.0],  # Fraction of samples per tree\n    'colsample_bytree': [0.6, 1.0],  # Fraction of features per tree\n    'gamma': [0, 0.1, 0.2],  # Minimum loss reduction for a split\n    'min_child_weight': [3, 5],  # Minimum sum of instance weights\n}\ngrid_search_xgb = GridSearchCV(\n    estimator=XGBRegressor(\n        tree_method='hist',  # Use GPU for training\n        device='cuda',  # Specify GPU device\n        random_state=42,\n        n_jobs=-1,  # Enable CPU parallelism (optional, if GPU is not available)\n    ),\n    param_grid=param_grid_xgb,\n    cv=tscv,  # Time series split\n    scoring='neg_mean_squared_error',  # Negative MSE for scoring\n    verbose=2,  # Print progress\n    n_jobs=-1,  # Use all CPU cores\n)\n\n# Fit the model\ngrid_search_xgb.fit(X_train, y_train)\n\nFitting 3 folds for each of 144 candidates, totalling 432 fits\n\n\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device='cuda',\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, i...\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=-1, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.1, 0.2],\n                         'max_depth': [3, 5, 7], 'min_child_weight': [3, 5],\n                         'n_estimators': [50, 100], 'subsample': [0.8, 1.0]},\n             scoring='neg_mean_squared_error', verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device='cuda',\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, i...\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=-1, num_parallel_tree=None,\n                                    random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.1, 0.2],\n                         'max_depth': [3, 5, 7], 'min_child_weight': [3, 5],\n                         'n_estimators': [50, 100], 'subsample': [0.8, 1.0]},\n             scoring='neg_mean_squared_error', verbose=2)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)\n\n\n\n# Evaluate the final model\nbest_model_xgb_final = grid_search_xgb.best_estimator_\nbest_model_xgb_final.fit(X_train[X.columns[selector_xgb.support_]], y_train)\n\n# Save the model\njoblib.dump(best_model_xgb_final, 'best_model_xgb_final.joblib')\n\n['best_model_xgb_final.joblib']\n\n\n\n_ = grid_search_result(grid_search_xgb, selector_xgb, best_model_xgb_final, 'XGBoost')\n\nXGBoost Best hyperparameters: {'colsample_bytree': 0.6, 'gamma': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}\nXGBoost Best Validation MSE: 0.00010268245284882257\n-----------------------------\nXGBoost Best Model Train MSE: 0.00012705996490768268\nXGBoost Best Model Test MSE: 5.086330269091728e-05\nXGBoost Best Model Train RMSE: 0.011272087868167222\nXGBoost Best Model Test RMSE: 0.00713185128076275"
  },
  {
    "objectID": "nasdaq-prediction.html#neural-network-models-for-regression",
    "href": "nasdaq-prediction.html#neural-network-models-for-regression",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "4. Neural Network Models for Regression",
    "text": "4. Neural Network Models for Regression\nBecause of the limited computing resource, we didn’t implement feature or hyperparameter optimization for Neural Network models. These models are just exploratory attempts.\n\n4.1 Multilayer Perceptron\n\nMLP_reg = Sequential([\n    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n    Dense(128, activation='relu'),  # Hidden layer\n    Dense(1)  # Output layer (linear activation)\n])\n\n# Compile the model\nMLP_reg.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n\nsplit_date = int(0.75 * len(X_train))  # 60% training (0.75*80%), 20% validation\nX_train_split, X_val = X_train[:split_date], X_train[split_date:]\ny_train_split, y_val = y_train[:split_date], y_train[split_date:]\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss', \n    patience=20, \n    restore_best_weights=True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    patience=5,\n    min_lr=1e-6,\n    verbose=0,\n)\n\ncallbacks = [\n    early_stopping,\n    reduce_lr,\n]\n\n# Train the model\nhistory = MLP_reg.fit(\n    X_train_split,\n    y_train_split,\n    epochs=100,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    callbacks=callbacks,\n)\n\nC:\\Users\\zln92\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\nEpoch 1/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - loss: 48.4190 - val_loss: 0.0687 - learning_rate: 0.0100\n\nEpoch 2/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0013 - val_loss: 0.0241 - learning_rate: 0.0100\n\nEpoch 3/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0010 - val_loss: 0.0222 - learning_rate: 0.0100\n\nEpoch 4/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 7.7949e-04 - val_loss: 0.0210 - learning_rate: 0.0100\n\nEpoch 5/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.6575e-04 - val_loss: 0.0201 - learning_rate: 0.0100\n\nEpoch 6/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 4.1016e-04 - val_loss: 0.0195 - learning_rate: 0.0100\n\nEpoch 7/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2.6531e-04 - val_loss: 0.0188 - learning_rate: 0.0100\n\nEpoch 8/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 2.0890e-04 - val_loss: 0.0184 - learning_rate: 0.0100\n\nEpoch 9/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.7962e-04 - val_loss: 0.0179 - learning_rate: 0.0100\n\nEpoch 10/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4781e-04 - val_loss: 0.0176 - learning_rate: 0.0100\n\nEpoch 11/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2998e-04 - val_loss: 0.0174 - learning_rate: 0.0100\n\nEpoch 12/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3270e-04 - val_loss: 0.0173 - learning_rate: 0.0100\n\nEpoch 13/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3352e-04 - val_loss: 0.0172 - learning_rate: 0.0100\n\nEpoch 14/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3359e-04 - val_loss: 0.0171 - learning_rate: 0.0100\n\nEpoch 15/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3150e-04 - val_loss: 0.0171 - learning_rate: 0.0100\n\nEpoch 16/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3089e-04 - val_loss: 0.0171 - learning_rate: 0.0100\n\nEpoch 17/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2469e-04 - val_loss: 0.0171 - learning_rate: 0.0100\n\nEpoch 18/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3430e-04 - val_loss: 0.0170 - learning_rate: 0.0100\n\nEpoch 19/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2950e-04 - val_loss: 0.0170 - learning_rate: 0.0100\n\nEpoch 20/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2746e-04 - val_loss: 0.0170 - learning_rate: 0.0100\n\nEpoch 21/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2412e-04 - val_loss: 0.0170 - learning_rate: 0.0100\n\nEpoch 22/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4509e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n\nEpoch 23/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3784e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n\nEpoch 24/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3388e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n\nEpoch 25/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3646e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n\nEpoch 26/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.2867e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n\nEpoch 27/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.4292e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n\nEpoch 28/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.2472e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n\nEpoch 29/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2470e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n\nEpoch 30/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2030e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n\nEpoch 31/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4047e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n\nEpoch 32/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4166e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n\nEpoch 33/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3643e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n\nEpoch 34/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2316e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n\nEpoch 35/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2334e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n\nEpoch 36/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3595e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n\nEpoch 37/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4116e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 38/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3190e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 39/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3455e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 40/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3248e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 41/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3989e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 42/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3399e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 43/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.2894e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\nEpoch 44/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3307e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06\n\n\n\n\n\nMLP_reg.save('MLP_reg.keras')\n\n\nplt.title('Training and Validation Loss of MLP for Regression')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Evaluate the model\ndef result_nn(model, model_name='', X_tr=X_train, y_tr=y_train, X_te=X_test, y_te=y_test, print_out=True, predict=False):\n    train_MSE = model.evaluate(X_tr, y_tr)\n    test_MSE = model.evaluate(X_te, y_te)\n    train_RMSE = np.sqrt(train_MSE)\n    test_RMSE = np.sqrt(test_MSE)\n    y_test_predict = model.predict(X_te)\n    if print_out:\n        print(f'{model_name} Train MSE: {train_MSE}')\n        print(f'{model_name} Test MSE: {test_MSE}')\n        print(f'{model_name} Train RMSE: {train_RMSE}')\n        print(f'{model_name} Test RMSE: {test_RMSE}')\n\n    if not predict:\n        return train_MSE, test_MSE, train_RMSE, test_RMSE\n    else:\n        return list(y_test_predict)\n\n\n_ = result_nn(MLP_reg, 'MLP (Regression)')\n\n\n49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 8.8159e-04 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0018 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n\nMLP (Regression) Train MSE: 0.00435810349881649\n\nMLP (Regression) Test MSE: 0.001041724462993443\n\nMLP (Regression) Train RMSE: 0.06601593367374645\n\nMLP (Regression) Test RMSE: 0.03227575658281991\n\n\n\n\n\n\n4.2 Long Short Term Memory (LSTM) Network\n\n4.2.1 Additional data transformation\nWe need to modify the data shape to fit into the LSTM network.\n\n# Choose the target variable\ntarget_col = 'Close_ROC'\n\ndf_nn = df_raw.drop(columns=['Date', 'Name'], errors='ignore')\ndf_nn = df_nn.dropna()\n\n# Scale all features using StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_nn)\n\n# We use the previous 5 days data to predict\nsequence_length = 5\n\n\n# Create sequences (X) and target (y)\nX_nn, y_nn = [], []\nfor i in range(len(df_nn) - sequence_length):\n    X_nn.append(df_nn.iloc[i:i+sequence_length].values)\n    y_nn.append(df_nn[target_col].iloc[i+sequence_length])\n\n# Convert to numpy arrays\nX_nn = np.array(X_nn)\ny_nn = np.array(y_nn)\n\n# For example, split the data into training (60%), validation (20%), and test (20%) sets\ntrain_size = int(0.60 * len(X_nn))\nval_size = int(0.20 * len(X_nn))\ntest_split = int(0.80 * len(X_nn))\n\nX_train_nn, y_train_nn = X_nn[:train_size], y_nn[:train_size]\nX_val_nn, y_val_nn = X_nn[train_size:train_size+val_size], y_nn[train_size:train_size+val_size]\nX_test_nn, y_test_nn = X_nn[test_split:], y_nn[test_split:]\n\nprint(\"X_train shape:\", X_train_nn.shape)  # (num_train_samples, window_size, num_features)\nprint(\"y_train shape:\", y_train_nn.shape)\n\nX_train shape: (1157, 5, 88)\ny_train shape: (1157,)\n\n\n\n\n4.2.2 Training\n\nlstm_reg = Sequential([\n    LSTM(\n        128,\n        return_sequences=False,\n        input_shape=(X_train_nn.shape[1], X_train_nn.shape[2]),\n        dropout=0.5,\n        recurrent_dropout=0.5,\n    ),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(1),\n])\n\nlstm_reg.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n)\n\nhistory = lstm_reg.fit(\n    X_train_nn,\n    y_train_nn,\n    validation_data=(X_val_nn, y_val_nn),\n    epochs=100,\n    batch_size=32,\n    callbacks=[early_stopping],\n    verbose=1,\n)\n\n# Save the trained model\nlstm_reg.save('lstm_reg.keras')\n\nEpoch 1/100\n\n\nC:\\Users\\zln92\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 4s 17ms/step - loss: 0.6221 - val_loss: 0.0699\n\nEpoch 2/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.3012 - val_loss: 0.0381\n\nEpoch 3/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.2395 - val_loss: 0.0055\n\nEpoch 4/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.1609 - val_loss: 8.2306e-04\n\nEpoch 5/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0949 - val_loss: 1.4668e-04\n\nEpoch 6/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0803 - val_loss: 1.2618e-04\n\nEpoch 7/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0571 - val_loss: 1.1901e-04\n\nEpoch 8/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0468 - val_loss: 2.3850e-04\n\nEpoch 9/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0439 - val_loss: 2.5668e-04\n\nEpoch 10/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0344 - val_loss: 3.3398e-04\n\nEpoch 11/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0267 - val_loss: 3.1353e-04\n\nEpoch 12/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0273 - val_loss: 1.1845e-04\n\nEpoch 13/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0197 - val_loss: 1.3941e-04\n\nEpoch 14/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0177 - val_loss: 1.5279e-04\n\nEpoch 15/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 0.0172 - val_loss: 1.5038e-04\n\nEpoch 16/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0121 - val_loss: 2.1746e-04\n\nEpoch 17/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0108 - val_loss: 1.5716e-04\n\nEpoch 18/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0088 - val_loss: 1.7635e-04\n\nEpoch 19/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0076 - val_loss: 1.4064e-04\n\nEpoch 20/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0082 - val_loss: 1.6360e-04\n\nEpoch 21/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0055 - val_loss: 1.7268e-04\n\nEpoch 22/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0073 - val_loss: 2.0646e-04\n\nEpoch 23/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0053 - val_loss: 1.3289e-04\n\nEpoch 24/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0046 - val_loss: 1.6827e-04\n\nEpoch 25/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0043 - val_loss: 1.6713e-04\n\nEpoch 26/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0039 - val_loss: 1.3411e-04\n\nEpoch 27/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0038 - val_loss: 1.5198e-04\n\nEpoch 28/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0033 - val_loss: 1.4898e-04\n\nEpoch 29/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0028 - val_loss: 1.1839e-04\n\nEpoch 30/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0026 - val_loss: 1.4790e-04\n\nEpoch 31/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0022 - val_loss: 1.3562e-04\n\nEpoch 32/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0024 - val_loss: 1.3838e-04\n\nEpoch 33/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0021 - val_loss: 1.2952e-04\n\nEpoch 34/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0019 - val_loss: 1.2973e-04\n\nEpoch 35/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0019 - val_loss: 1.3039e-04\n\nEpoch 36/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0015 - val_loss: 1.1754e-04\n\nEpoch 37/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0019 - val_loss: 1.1735e-04\n\nEpoch 38/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0017 - val_loss: 1.1970e-04\n\nEpoch 39/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0016 - val_loss: 1.2148e-04\n\nEpoch 40/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 0.0012 - val_loss: 1.2273e-04\n\nEpoch 41/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0013 - val_loss: 1.2091e-04\n\nEpoch 42/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0014 - val_loss: 1.2954e-04\n\nEpoch 43/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0012 - val_loss: 1.2313e-04\n\nEpoch 44/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 9.2076e-04 - val_loss: 1.1762e-04\n\nEpoch 45/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 9.8157e-04 - val_loss: 1.2080e-04\n\nEpoch 46/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 7.5778e-04 - val_loss: 1.2152e-04\n\nEpoch 47/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 8.6865e-04 - val_loss: 1.2130e-04\n\nEpoch 48/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 8.2736e-04 - val_loss: 1.1941e-04\n\nEpoch 49/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 8.1362e-04 - val_loss: 1.2545e-04\n\nEpoch 50/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 8.3782e-04 - val_loss: 1.1824e-04\n\nEpoch 51/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 9.2354e-04 - val_loss: 1.1838e-04\n\nEpoch 52/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.8213e-04 - val_loss: 1.1737e-04\n\nEpoch 53/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.9316e-04 - val_loss: 1.1770e-04\n\nEpoch 54/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 7.9941e-04 - val_loss: 1.1710e-04\n\nEpoch 55/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.0735e-04 - val_loss: 1.1709e-04\n\nEpoch 56/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 5.3108e-04 - val_loss: 1.1969e-04\n\nEpoch 57/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 6.1958e-04 - val_loss: 1.1820e-04\n\nEpoch 58/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 7.5449e-04 - val_loss: 1.1930e-04\n\nEpoch 59/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.4645e-04 - val_loss: 1.2019e-04\n\nEpoch 60/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 4.7346e-04 - val_loss: 1.1966e-04\n\nEpoch 61/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 3.9326e-04 - val_loss: 1.1716e-04\n\nEpoch 62/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 5.5395e-04 - val_loss: 1.2005e-04\n\nEpoch 63/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 6.4767e-04 - val_loss: 1.1854e-04\n\nEpoch 64/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 4.5624e-04 - val_loss: 1.1788e-04\n\nEpoch 65/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 4.4687e-04 - val_loss: 1.1709e-04\n\nEpoch 66/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.9228e-04 - val_loss: 1.1749e-04\n\nEpoch 67/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 3.8819e-04 - val_loss: 1.1715e-04\n\nEpoch 68/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.5908e-04 - val_loss: 1.1854e-04\n\nEpoch 69/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.3763e-04 - val_loss: 1.1755e-04\n\nEpoch 70/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 3.7734e-04 - val_loss: 1.1709e-04\n\nEpoch 71/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.5650e-04 - val_loss: 1.1760e-04\n\nEpoch 72/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.1977e-04 - val_loss: 1.1709e-04\n\nEpoch 73/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 3.7925e-04 - val_loss: 1.1741e-04\n\nEpoch 74/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.5705e-04 - val_loss: 1.1708e-04\n\nEpoch 75/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.0423e-04 - val_loss: 1.1726e-04\n\nEpoch 76/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 4.2551e-04 - val_loss: 1.1716e-04\n\nEpoch 77/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.1666e-04 - val_loss: 1.1729e-04\n\nEpoch 78/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.1278e-04 - val_loss: 1.1805e-04\n\nEpoch 79/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.0009e-04 - val_loss: 1.1710e-04\n\nEpoch 80/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.4286e-04 - val_loss: 1.2090e-04\n\nEpoch 81/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.4950e-04 - val_loss: 1.1722e-04\n\nEpoch 82/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.5682e-04 - val_loss: 1.1727e-04\n\nEpoch 83/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.3842e-04 - val_loss: 1.1708e-04\n\nEpoch 84/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 3.6073e-04 - val_loss: 1.1724e-04\n\nEpoch 85/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.4341e-04 - val_loss: 1.1784e-04\n\nEpoch 86/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.2171e-04 - val_loss: 1.1711e-04\n\nEpoch 87/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.0149e-04 - val_loss: 1.1723e-04\n\nEpoch 88/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.8024e-04 - val_loss: 1.1805e-04\n\nEpoch 89/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.2784e-04 - val_loss: 1.1708e-04\n\nEpoch 90/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.3941e-04 - val_loss: 1.1712e-04\n\nEpoch 91/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.8911e-04 - val_loss: 1.1848e-04\n\nEpoch 92/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.5088e-04 - val_loss: 1.1739e-04\n\nEpoch 93/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 2.2310e-04 - val_loss: 1.1789e-04\n\nEpoch 94/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.8202e-04 - val_loss: 1.1739e-04\n\nEpoch 95/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 2.1815e-04 - val_loss: 1.1801e-04\n\nEpoch 96/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.9053e-04 - val_loss: 1.1784e-04\n\nEpoch 97/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 2.1361e-04 - val_loss: 1.1913e-04\n\nEpoch 98/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 2.0350e-04 - val_loss: 1.1918e-04\n\nEpoch 99/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 2.1561e-04 - val_loss: 1.2011e-04\n\nEpoch 100/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.8266e-04 - val_loss: 1.1839e-04\n\n\n\n\n\nplt.title('Training and Validation Loss of LSTM for Regression')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n_ = result_nn(\n    lstm_reg,\n    'LSTM (Regression)',\n    X_tr=X_train_nn,\n    y_tr=y_train_nn,\n    X_te=X_test_nn,\n    y_te=y_test_nn,\n)\n\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.6815e-04\n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 6.2900e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 1s 53ms/step\n\nLSTM (Regression) Train MSE: 0.0001305853365920484\n\nLSTM (Regression) Test MSE: 5.119918205309659e-05\n\nLSTM (Regression) Train RMSE: 0.01142739412954889\n\nLSTM (Regression) Test RMSE: 0.007155360371993614\n\n\n\n\n\n\n\n4.3 Convolutional Neural Network (CNN)\n\nnum_features = X_train_nn.shape[2]  # number of columns in original dataset\n\nCNN1D_reg = Sequential([\n    Conv1D(\n        filters=64,\n        kernel_size=5,\n        activation='relu',\n        padding='same',\n        input_shape=(sequence_length, num_features),\n    ),\n    MaxPooling1D(pool_size=2),\n    Conv1D(\n        filters=32,\n        kernel_size=5,\n        activation='relu',\n        padding='same',\n    ),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(50, activation='relu'),\n    Dense(1),\n])\n\nCNN1D_reg.compile(optimizer=Adam(learning_rate=0.1), loss='mse')\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n)\n\nhistory = CNN1D_reg.fit(\n    X_train_nn,\n    y_train_nn,\n    validation_data=(X_val_nn, y_val_nn),\n    epochs=100,\n    batch_size=32,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1,\n)\n\nCNN1D_reg.save('CNN1D_reg.keras')\n\nEpoch 1/100\n\n\nC:\\Users\\zln92\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - loss: 1447449344.0000 - val_loss: 6036.1299 - learning_rate: 0.1000\n\nEpoch 2/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 819.7986 - val_loss: 72.9835 - learning_rate: 0.1000\n\nEpoch 3/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 11.2172 - val_loss: 0.7078 - learning_rate: 0.1000\n\nEpoch 4/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.1930 - val_loss: 0.4386 - learning_rate: 0.1000\n\nEpoch 5/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0319 - val_loss: 0.1091 - learning_rate: 0.1000\n\nEpoch 6/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0172 - val_loss: 0.0432 - learning_rate: 0.1000\n\nEpoch 7/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0208 - val_loss: 0.2549 - learning_rate: 0.1000\n\nEpoch 8/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0197 - val_loss: 0.0231 - learning_rate: 0.1000\n\nEpoch 9/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0278 - val_loss: 0.0348 - learning_rate: 0.1000\n\nEpoch 10/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0258 - val_loss: 0.2173 - learning_rate: 0.1000\n\nEpoch 11/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0241 - val_loss: 0.3775 - learning_rate: 0.1000\n\nEpoch 12/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0492 - val_loss: 0.6089 - learning_rate: 0.1000\n\nEpoch 13/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0616 - val_loss: 0.4884 - learning_rate: 0.1000\n\nEpoch 14/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0375 - val_loss: 0.0468 - learning_rate: 0.0100\n\nEpoch 15/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0174 - val_loss: 0.0760 - learning_rate: 0.0100\n\nEpoch 16/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0150 - val_loss: 0.0345 - learning_rate: 0.0100\n\nEpoch 17/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0159 - val_loss: 0.0963 - learning_rate: 0.0100\n\nEpoch 18/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0129 - val_loss: 0.0694 - learning_rate: 0.0100\n\nEpoch 19/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0125 - val_loss: 0.0744 - learning_rate: 1.0000e-03\n\nEpoch 20/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0126 - val_loss: 0.0772 - learning_rate: 1.0000e-03\n\nEpoch 21/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0134 - val_loss: 0.0679 - learning_rate: 1.0000e-03\n\nEpoch 22/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0129 - val_loss: 0.0854 - learning_rate: 1.0000e-03\n\nEpoch 23/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0139 - val_loss: 0.0701 - learning_rate: 1.0000e-03\n\nEpoch 24/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0131 - val_loss: 0.0674 - learning_rate: 1.0000e-04\n\nEpoch 25/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0131 - val_loss: 0.0684 - learning_rate: 1.0000e-04\n\nEpoch 26/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0128 - val_loss: 0.0665 - learning_rate: 1.0000e-04\n\nEpoch 27/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0132 - val_loss: 0.0673 - learning_rate: 1.0000e-04\n\nEpoch 28/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0130 - val_loss: 0.0680 - learning_rate: 1.0000e-04\n\n\n\n\n\nplt.title('Training and Validation Loss of 1D CNN for Regression')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n_ = result_nn(\n    CNN1D_reg,\n    '1D CNN (Regression)',\n    X_tr=X_train_nn,\n    y_tr=y_train_nn,\n    X_te=X_test_nn,\n    y_te=y_test_nn,\n)\n\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0319 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0471 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n\n1D CNN (Regression) Train MSE: 0.01970905438065529\n\n1D CNN (Regression) Test MSE: 0.07369030267000198\n\n1D CNN (Regression) Train RMSE: 0.14038893966639712\n\n1D CNN (Regression) Test RMSE: 0.27145957833534257\n\n\n\n\n\n\n4.4 CNN LSTM\n\nCNN1D_lstm_reg = Sequential([\n    Conv1D(\n        filters=64,\n        kernel_size=5,\n        activation='relu',\n        padding='same',\n        input_shape=(sequence_length, num_features),\n    ),\n    MaxPooling1D(pool_size=2),\n    Conv1D(\n        filters=32,\n        kernel_size=5,\n        activation='relu',\n        padding='same',\n    ),\n    MaxPooling1D(pool_size=2),\n    LSTM(32, return_sequences=False),\n    Dense(50, activation='relu'),\n    Dense(1),\n])\n\nCNN1D_lstm_reg.compile(optimizer=Adam(learning_rate=0.1), loss='mse')\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    restore_best_weights=True,\n)\n\nhistory = CNN1D_lstm_reg.fit(\n    X_train_nn,\n    y_train_nn,\n    validation_data=(X_val_nn, y_val_nn),\n    epochs=100,\n    batch_size=32,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1,\n)\n\nCNN1D_lstm_reg.save('CNN1D_lstm_reg.keras')\n\n\nEpoch 1/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - loss: 0.8922 - val_loss: 0.0024 - learning_rate: 0.1000\n\nEpoch 2/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.0021 - val_loss: 1.2201e-04 - learning_rate: 0.1000\n\nEpoch 3/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.5326e-04 - val_loss: 1.1884e-04 - learning_rate: 0.1000\n\nEpoch 4/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.5140e-04 - val_loss: 1.2202e-04 - learning_rate: 0.1000\n\nEpoch 5/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2620e-04 - val_loss: 1.2179e-04 - learning_rate: 0.1000\n\nEpoch 6/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4153e-04 - val_loss: 1.3648e-04 - learning_rate: 0.1000\n\nEpoch 7/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4528e-04 - val_loss: 1.1876e-04 - learning_rate: 0.1000\n\nEpoch 8/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2227e-04 - val_loss: 1.1712e-04 - learning_rate: 0.0100\n\nEpoch 9/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.1566e-04 - val_loss: 1.1713e-04 - learning_rate: 0.0100\n\nEpoch 10/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.5773e-04 - val_loss: 1.1710e-04 - learning_rate: 0.0100\n\nEpoch 11/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2773e-04 - val_loss: 1.1764e-04 - learning_rate: 0.0100\n\nEpoch 12/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2657e-04 - val_loss: 1.1710e-04 - learning_rate: 0.0100\n\nEpoch 13/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3492e-04 - val_loss: 1.1714e-04 - learning_rate: 1.0000e-03\n\nEpoch 14/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2028e-04 - val_loss: 1.1716e-04 - learning_rate: 1.0000e-03\n\nEpoch 15/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3301e-04 - val_loss: 1.1712e-04 - learning_rate: 1.0000e-03\n\nEpoch 16/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3516e-04 - val_loss: 1.1722e-04 - learning_rate: 1.0000e-03\n\nEpoch 17/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.4031e-04 - val_loss: 1.1730e-04 - learning_rate: 1.0000e-03\n\nEpoch 18/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4178e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04\n\nEpoch 19/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2104e-04 - val_loss: 1.1730e-04 - learning_rate: 1.0000e-04\n\nEpoch 20/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3550e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04\n\nEpoch 21/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.2281e-04 - val_loss: 1.1727e-04 - learning_rate: 1.0000e-04\n\nEpoch 22/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3376e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04\n\nEpoch 23/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3211e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05\n\nEpoch 24/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3694e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05\n\nEpoch 25/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2585e-04 - val_loss: 1.1728e-04 - learning_rate: 1.0000e-05\n\nEpoch 26/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.3536e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05\n\nEpoch 27/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.3663e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05\n\nEpoch 28/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.2884e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06\n\nEpoch 29/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 1.3420e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06\n\nEpoch 30/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.2918e-04 - val_loss: 1.1728e-04 - learning_rate: 1.0000e-06\n\nEpoch 31/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.5119e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06\n\nEpoch 32/100\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.3169e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06\n\n\n\n\n\nplt.title('Training and Validation Loss of CNN LSTM for Regression')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.yscale('log')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n_ = result_nn(\n    CNN1D_lstm_reg,\n    'CNN LSTM (Regression)',\n    X_tr=X_train_nn,\n    y_tr=y_train_nn,\n    X_te=X_test_nn,\n    y_te=y_test_nn,\n)\n\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6813e-04\n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6.2670e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step\n\nCNN LSTM (Regression) Train MSE: 0.00013050153211224824\n\nCNN LSTM (Regression) Test MSE: 5.0988739531021565e-05\n\nCNN LSTM (Regression) Train RMSE: 0.011423726717330393\n\nCNN LSTM (Regression) Test RMSE: 0.007140639994497802"
  },
  {
    "objectID": "nasdaq-prediction.html#model-comparison-and-discussion",
    "href": "nasdaq-prediction.html#model-comparison-and-discussion",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "5. Model Comparison and Discussion",
    "text": "5. Model Comparison and Discussion\n\n5.1 Comparison of RMSE\nFirst, we compare the training and testing RMSE of the 8 models we’ve trained. We use RMSE as criterion.\n\n# Load saved models for comparison\nlin_reg_c = joblib.load('selector_lin_reg_cv.joblib')\ndt_c = joblib.load('selector_dt.joblib')\nrf_c = joblib.load('selector_rf.joblib')\nxgb_c = joblib.load('selector_xgb.joblib')\nmlp_c = load_model('MLP_reg.keras')\nlstm_c = load_model('lstm_reg.keras')\ncnn_c = load_model('CNN1D_reg.keras')\ncnn_lstm_c = load_model('CNN1D_lstm_reg.keras')\n\nmodel_list = [\n    'baseline',\n    lin_reg_c,\n    dt_c,\n    rf_c,\n    xgb_c,\n    mlp_c,\n    lstm_c,\n    cnn_c,\n    cnn_lstm_c,\n]\n\nmodel_names = [\n    'Baseline',\n    'Linear Regression',\n    'Decision Tree',\n    'Random Forest',\n    'XGBoost',\n    'MLP',\n    'LSTM',\n    '1D CNN',\n    'CNN LSTM',\n]\n\n\ndf_model_comparison = pd.DataFrame(\n    columns=[\n        'model',\n        'model_name',\n        'train_MSE',\n        'test_MSE',\n        'train_RMSE',\n        'test_RMSE',\n        'y_test_predict',\n    ]\n)\n\ndf_model_comparison['model'] = model_list\ndf_model_comparison['model_name'] = model_names\n\ndf_model_comparison.iloc[0, 2:6] = (\n    baseline_mse_train_final,\n    baseline_mse_test_final,\n    baseline_rmse_train_final,\n    baseline_rmse_test_final,\n)\ndf_model_comparison.iat[0, 6] = baseline_test_predict\n\nfor i in range(1, 5):\n    df_model_comparison.iloc[i, 2:6] = selector_result(\n        df_model_comparison.loc[i, 'model'], print_out=False\n    )\n    df_model_comparison.iat[i, 6] = selector_result(\n        df_model_comparison.loc[i, 'model'], print_out=False, predict=True\n    )\n\ndf_model_comparison.iloc[5, 2:6] = result_nn(\n    df_model_comparison.loc[5, 'model'], print_out=False\n)\ndf_model_comparison.iat[5, 6] = result_nn(\n    df_model_comparison.loc[5, 'model'], print_out=False, predict=True\n)\n\nfor i in range(6, 9):\n    df_model_comparison.iloc[i, 2:6] = result_nn(\n        df_model_comparison.loc[i, 'model'],\n        model_name='',\n        X_tr=X_train_nn,\n        y_tr=y_train_nn,\n        X_te=X_test_nn,\n        y_te=y_test_nn,\n        print_out=False,\n    )\n    df_model_comparison.iat[i, 6] = result_nn(\n        df_model_comparison.loc[i, 'model'],\n        model_name='',\n        X_tr=X_train_nn,\n        y_tr=y_train_nn,\n        X_te=X_test_nn,\n        y_te=y_test_nn,\n        print_out=False,\n        predict=True,\n    )\n\ndf_model_comparison['y_test_predict'] = df_model_comparison['y_test_predict'].apply(\n    lambda x: np.ravel(x)\n)\n\ndf_model_comparison\n\n\n49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 8.8159e-04 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0018 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n\n49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 8.8159e-04\n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0018 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6815e-04  \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6.2900e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6815e-04\n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6.2900e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step \n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0319  \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0471 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.0319 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0471 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6813e-04  \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 6.2670e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6813e-04 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 6.2670e-05 \n\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n\n\n\n\n\n\n\n\n\n\n\nmodel\nmodel_name\ntrain_MSE\ntest_MSE\ntrain_RMSE\ntest_RMSE\ny_test_predict\n\n\n\n\n0\nbaseline\nBaseline\n0.000127\n0.000052\n0.011283\n0.007178\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n\n\n1\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size...\nLinear Regression\n0.000126\n0.000058\n0.011231\n0.007611\n[9.748103946616224e-05, 0.000582024958710533, ...\n\n\n2\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size...\nDecision Tree\n0.000094\n0.000053\n0.00969\n0.007294\n[0.0003911573186231354, 0.0003911573186231354,...\n\n\n3\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size...\nRandom Forest\n0.000105\n0.000054\n0.010224\n0.00736\n[0.0019121735784212019, 0.0010096476163310333,...\n\n\n4\nRFECV(cv=TimeSeriesSplit(gap=0, max_train_size...\nXGBoost\n0.000106\n0.000059\n0.010296\n0.007709\n[-0.004676743, 0.002464143, 0.0020128984, 0.00...\n\n\n5\n&lt;Sequential name=sequential, built=True&gt;\nMLP\n0.004358\n0.001042\n0.066016\n0.032276\n[0.00056827633, 0.00056827633, 0.00056827633, ...\n\n\n6\n&lt;Sequential name=sequential_1, built=True&gt;\nLSTM\n0.000131\n0.000051\n0.011427\n0.007155\n[0.00019381603, 0.00019381603, 0.00019381603, ...\n\n\n7\n&lt;Sequential name=sequential_2, built=True&gt;\n1D CNN\n0.019709\n0.07369\n0.140389\n0.27146\n[0.15607905, 0.14068294, 0.13164973, 0.1305968...\n\n\n8\n&lt;Sequential name=sequential_3, built=True&gt;\nCNN LSTM\n0.000131\n0.000051\n0.011424\n0.007141\n[0.00035510492, 0.00035510492, 0.00035510492, ...\n\n\n\n\n\n\n\nWe didn’t find a proper method to set random states for neural network models, so you may get different results if you run the code again.\nFrom the bar graph below, we can observe that there’s no model that is significantly better than the baseline model.\nSome testing RMSE is lower than the traing RMSE. The reason might be that our testing data not obtained by randomly sampling, and the testing data is substantially different from the training data, according to our UMAP analysis in section 1.3.1.\nFrom the table above, we can see that the predicted values some models including the neural network models are constants. They may just predict the rate of change to be the daily average rate of change. So, these model fails to grasp the increasing or decreasing trend of the Nasdaq Index.\n\n# Melt the dataframe to have train_MSE and test_MSE in one column\ndf_melted = df_model_comparison.melt(\n    id_vars=['model_name'],\n    value_vars=['train_RMSE', 'test_RMSE'],\n    var_name='RMSE_Type',\n    value_name='RMSE_Value',\n)\n\n# Set the figure size\nplt.figure(figsize=(12, 6))\n\n# Plot the bars side by side using hue\nax = sns.barplot(\n    x='model_name', y='RMSE_Value', hue='RMSE_Type', data=df_melted\n)\n\n# Add value labels on each bar\nfor p in ax.patches:\n    ax.annotate(\n        f'{p.get_height():.5f}',\n        (p.get_x() + p.get_width() / 2., p.get_height()),\n        ha='center',\n        va='bottom',\n        fontsize=10,\n        color='black',\n        rotation=0,\n    )\n\n# Add labels and title\nplt.xlabel('Model')\nplt.ylabel('RMSE')\nplt.title('Train and Test RMSE for Each Model')\nplt.xticks(rotation=0)\nplt.yscale('log')\nplt.legend()\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2 Predicting Nasdaq Index\n\n# Keep only 'Close' and 'Close_day-1' columns in df_compare\ndf_compare = df.iloc[split_index:][['Close', 'Close_day-1']]\n\nfor i in range(9):\n    model_name = df_model_comparison.loc[i, 'model_name']\n    y_test_predict = df_model_comparison.loc[i, 'y_test_predict']\n    df_compare[model_name] = df_compare['Close_day-1'] * (1 + y_test_predict)\n\ndf_compare\n\n\n\n\n\n\n\n\nClose\nClose_day-1\nBaseline\nLinear Regression\nDecision Tree\nRandom Forest\nXGBoost\nMLP\nLSTM\n1D CNN\nCNN LSTM\n\n\n\n\n1598\n4750.209961\n4736.160156\n4736.160156\n4736.621842\n4738.012740\n4745.216516\n4714.010275\n4738.851577\n4737.078186\n5475.375552\n4737.842082\n\n\n1599\n4809.879883\n4750.209961\n4750.209961\n4752.974702\n4752.068040\n4755.005999\n4761.915311\n4752.909366\n4751.130715\n5418.483444\n4751.896877\n\n\n1600\n4760.689941\n4809.879883\n4809.879883\n4814.637871\n4811.761303\n4810.711296\n4819.561444\n4812.613197\n4810.812203\n5443.099283\n4811.587989\n\n\n1601\n4737.330078\n4760.689941\n4760.689941\n4765.626611\n4762.552120\n4763.370205\n4807.241778\n4763.395302\n4761.612726\n5382.421176\n4762.380579\n\n\n1602\n4717.680176\n4737.330078\n4737.330078\n4738.565476\n4739.183119\n4740.332310\n4706.732521\n4740.022164\n4738.248335\n5437.549106\n4739.012420\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1979\n6750.049805\n6789.120117\n6789.120117\n6763.667137\n6791.775731\n6785.791067\n6793.182125\n6792.978175\n6790.436081\n9504.084121\n6791.531100\n\n\n1980\n6750.939941\n6750.049805\n6750.049805\n6723.597959\n6752.690136\n6748.439763\n6762.724141\n6753.885660\n6751.358196\n9488.322709\n6752.446913\n\n\n1981\n6757.600098\n6750.939941\n6750.939941\n6725.801424\n6753.580621\n6741.231789\n6756.923441\n6754.776302\n6752.248505\n9552.616779\n6753.337365\n\n\n1982\n6737.870117\n6757.600098\n6757.600098\n6730.846650\n6760.243383\n6756.024509\n6767.336201\n6761.440244\n6758.909953\n9578.126533\n6759.999887\n\n\n1983\n6706.209961\n6737.870117\n6737.870117\n6712.351766\n6740.505684\n6726.680086\n6732.851619\n6741.699051\n6739.176147\n9479.838308\n6740.262900\n\n\n\n\n386 rows × 11 columns\n\n\n\nWe can see that most model follows closely to the true value, while some model substantially deviate from the true value.\n\ntest_dates = df_raw.loc[y_test.index, 'Date']\n\nplt.figure(figsize=(12, 6))\n\nfor column in df_compare.columns:\n    if column not in ['Close_day-1']:\n        label = \"True Nasdaq Index\" if column == \"Close\" else column\n        plt.plot(test_dates, df_compare[column], label=label)\n\nplt.legend()\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\nplt.title(\"Comparison of True Nasdaq Index and Other Models\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.3 Investment Simulation\n\n# ---------------------------------------------------------------\n# Compute the true daily return using the 'Close' and 'Close_day-1' columns.\n# Daily return is defined as (Close / Close_day-1) - 1.\n# ---------------------------------------------------------------\ndf_compare['y_test'] = df_compare['Close'] / df_compare['Close_day-1'] - 1\n\n# ---------------------------------------------------------------\n# Calculate the cumulative return for the baseline (Invest & Hold) strategy.\n# Invest & Hold: simply holding the asset, so cumulative return is the\n# cumulative product of (1 + true daily return).\n# ---------------------------------------------------------------\ninvest_hold_return = (1 + df_compare['y_test']).cumprod()\n\n# ---------------------------------------------------------------\n# Define a function to calculate cumulative strategy returns.\n# For each day, if the predicted return (y_pred) is &gt;= 0, we take a long\n# position (+1); otherwise, a short position (-1). The strategy's daily\n# return is then: position * true daily return.\n# ---------------------------------------------------------------\ndef calc_strategy_returns(y_true, y_pred):\n    # Determine position: long if predicted return &gt;= 0, else short.\n    position = np.where(y_pred &gt;= 0, 1, -1)\n    # Calculate daily strategy returns.\n    daily_returns = position * y_true\n    # Compute cumulative returns as the cumulative product of (1 + daily return).\n    cum_returns = (1 + pd.Series(daily_returns, index=y_true.index)).cumprod()\n    return cum_returns, daily_returns\n\n# ---------------------------------------------------------------\n# Create a dictionary to hold cumulative return series for each model.\n# Loop over each model in df_model_comparison and compute its strategy returns.\n# ---------------------------------------------------------------\nstrategy_returns = {}\nfor i in range(len(df_model_comparison)):\n    model_name = df_model_comparison.loc[i, 'model_name']\n    # Extract predicted returns for the test set and convert them to a Series \n    # with the same index as df_compare.\n    y_pred = pd.Series(\n        df_model_comparison.loc[i, 'y_test_predict'], index=df_compare.index\n    )\n    # Calculate the strategy cumulative returns using the true returns and predicted returns.\n    strategy, _ = calc_strategy_returns(df_compare['y_test'], y_pred)\n    strategy_returns[model_name] = strategy\n\n# ---------------------------------------------------------------\n# Print the final cumulative return for the Invest & Hold strategy and\n# each model-based strategy.\n# ---------------------------------------------------------------\nprint(\n    \"Invest & Hold Final Cumulative Return: {:.4f}\".format(\n        invest_hold_return.iloc[-1]\n    )\n)\nfor model_name, cum_return in strategy_returns.items():\n    print(f\"{model_name} Strategy Final Cumulative Return: {cum_return.iloc[-1]:.4f}\")\n\n# ---------------------------------------------------------------\n# Plot the cumulative return curves for comparison.\n# The Invest & Hold curve is plotted in a solid line.\n# Each model strategy is plotted with a dashed line.\n# ---------------------------------------------------------------\nplt.figure(figsize=(12, 6))\nplt.plot(\n    test_dates, invest_hold_return, label=\"Invest & Hold\", linewidth=2\n)\n# Plot each model strategy except for the Baseline (since it's equivalent to Invest & Hold).\nfor model_name, cum_return in strategy_returns.items():\n    if model_name != 'Baseline':\n        plt.plot(\n            test_dates,\n            cum_return,\n            label=f\"{model_name} Strategy\",\n            linestyle=\"--\"\n        )\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Return\")\nplt.title(\"Comparison of Strategy Cumulative Returns\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nInvest & Hold Final Cumulative Return: 1.4160\nBaseline Strategy Final Cumulative Return: 1.4160\nLinear Regression Strategy Final Cumulative Return: 0.8317\nDecision Tree Strategy Final Cumulative Return: 1.3419\nRandom Forest Strategy Final Cumulative Return: 0.8919\nXGBoost Strategy Final Cumulative Return: 1.2267\nMLP Strategy Final Cumulative Return: 1.3785\nLSTM Strategy Final Cumulative Return: 1.4160\n1D CNN Strategy Final Cumulative Return: 1.4160\nCNN LSTM Strategy Final Cumulative Return: 1.4160\n\n\n\n\n\n\n\n\n\nWe can see that no model is significantly better than the simple “invest and hold” strategy. The neural network models closely follows the baseline strategy because their predictions for rate of change are positive constants."
  },
  {
    "objectID": "nasdaq-prediction.html#conclusion",
    "href": "nasdaq-prediction.html#conclusion",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nAlthough we didn’t find any model that successfully predict the Nasdaq index, we showed that, using this dataset, we cannot use the linear regression, tree based models, and neural network models with the hyperparameter we’ve tried to predict Nasdaq index.\nPredicting the financial market and make profit is not an easy task, but machine learning and deep learning is promising tool to do it. We can explore other available data, models, optimization methods to make successful prediction in the future."
  },
  {
    "objectID": "nasdaq-prediction.html#reference",
    "href": "nasdaq-prediction.html#reference",
    "title": "Nasdaq Index Prediction and Investment Simulation",
    "section": "Reference",
    "text": "Reference\n\nCNNpred: CNN-based stock market prediction using a diverse set of variables https://archive.ics.uci.edu/dataset/554/cnnpred+cnn+based+stock+market+prediction+using+a+diverse+set+of+variables\nThe idea of RFE and Time Series Cross-Validation are coming from ChatGPT and DeepSeek. The investment simulation part is modified from ChatGPT. Used ChatGPT to check the PEP8 format.\nMartingale (probability theory) https://en.wikipedia.org/wiki/Martingale_(probability_theory)\nRecursive feature elimination with cross-validation https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n\n   Created in Deepnote"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "📄 Download PDF Version"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine\nB.S. in Applied and Computational Mathematics (Data Science Concentration)\nSeptember 2020 – December 2025\nGPA: 3.845 | Dean’s Honor List"
  },
  {
    "objectID": "cv.html#awards-fellowship",
    "href": "cv.html#awards-fellowship",
    "title": "CV",
    "section": "Awards & Fellowship",
    "text": "Awards & Fellowship\n\nUROP Fellow for Summer 2025, UC Irvine | July 2025\nThird Prize, 18th “Tomorrow’s Little Scientists” Award Event | November 2018\nSecond Prize, 38th Beijing Youth Science and Technology Innovation Contest | March 2018\nFirst Prize, 20th Hong Kong Youth Science and Technology Innovation Competition | April 2018\nShu Ping Scholarship | November 2017"
  },
  {
    "objectID": "cv.html#skills-languages",
    "href": "cv.html#skills-languages",
    "title": "CV",
    "section": "Skills & Languages",
    "text": "Skills & Languages\nData Analysis: Excel, SQL, Python (Pandas, NumPy, Matplotlib, Seaborn, etc.), R, Tableau, and MATLAB\nMachine Learning: Keras, PyTorch, Scikit-learn; familiar with machine/deep/reinforcement learning, and model optimization methods\nProject Management: Goal decomposition, progress-tracking, and cross-functional collaboration (Using tools: Dingding and Lark)\nSocial Media Content Production: Copywriting, graphic design and layout (Xiumi, CDR, Gaoding Design, and Photoshop), video production (CapCut and VideoStudio), and live streaming (OBS, and live room setup)\nBilingual Communications: Fluent in English and Chinese"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "CV",
    "section": "Research Experience",
    "text": "Research Experience\n\nOptimizing LLM and RL Methods to Explore Open Problems in Combinatorics: A Case Study on the “No-Three-in-Line” Problem\nMarch 2025 – Ongoing\n\nImplemented MCTS to explore the largest and smallest complete sets, and optimized the algorithm’s time complexity through algorithmic improvements and Numba acceleration (boosting performance from ~7 iter/s to 6,000+ iter/s)\nFor n=53……101, the algorithm finds 1.68n points, which outperforms the previous best lower bound: ((1.5-ε)n)\nOngoing: Exploring the smallest geometric dominating set and complete set. Developing Graph Search algorithm\nOngoing: Investigating the LLM-based methods for code optimization to explore the open problems in combinatorics\n\n\n\nOptimizing Large Language Models (LLM) for Academic Support: A Framework for Knowledge Graph (KG) Embedding and Cheat Sheet Generation\nSeptember 2024 – June 2025\n\nDesigned a benchmark to evaluate the performance of LLM on college-level academic problems\nIntegrate KG for Retrieval-augmented Generation into LLM and assess the accuracy improvement on the benchmark\n\n\n\nInstantaneous Electromagnetic Water Heater Design and Multiphysics Field Finite Element Optimization\nAugust 2016 – October 2017\n\nDesigned the structure and circuitry of the induction water heater with better energy efficiency and temperature stability; coded a program for temperature control and user interaction on Arduino; optimized the energy efficiency by applying thermodynamic and electromagnetic models and conducting multi-physics field finite element simulations on ANSYS\nApplied and accepted as Patent Number: ZL201621267831.6"
  },
  {
    "objectID": "cv.html#project-experience",
    "href": "cv.html#project-experience",
    "title": "CV",
    "section": "Project Experience",
    "text": "Project Experience\n\nNasdaq Index Prediction and Investment Simulation\nMarch 2025\n🔗 Project Link\nObjective: Predict future Nasdaq Index by machine learning and neural network models\n\nPreprocessed the dataset (1984 samples and 84 features, including technical indicators, futures contracts, etc.) by feature augmentation, conversion, filling and dropping null values, and time-series train-test split\nTrained four machine learning models (Linear Regression, Decision Tree, Random Forest, and XGBoost) and four deep learning models (MLP, LSTM, 1DCNN, CNN-LSTM); and used Grid Search to optimize hyperparameters\nImplemented Recursive Feature Elimination and Time-Series Cross-Validation to select features\nEvaluated and compared the models and implemented an investment simulation with these models\n\n\n\nCNN and Vision Transformer Pre-trained Models for Facial Emotion Recognition\nMay 2024\n🔗 Project Link\nObjective: Building CNN and Vision Transformer networks to recognize facial emotions from scratch\n\nPreprocessed the FER-2013 dataset with 35,887 grayscale images (48x48 pixels) of 7 categories, addressed class imbalance by resampling, and implemented data augmentation by rotating and resizing the images\nDesigned and optimized the architecture of CNN and Vision Transformer networks, optimized the learning rate, increased the number of layers and used gradual unfreezing to solve underfitting, and incorporated max pooling (for CNN), batch normalizations, dropout layers, and weight decay to overcome overfitting, achieving accuracy score of 0.62 and 0.66\nThe accuracy scores of the CNN and Vision Transformer models are 0.62 and 0.66, respectively. The happy category has the highest accuracy, 0.83. The models also predict well on images outside the dataset, proving their generalization capability\n\n\n\nMarket Data Analysis for Airbnb in Dublin\nApril 2024\n🔗 Project Link\nObjective: Understanding the user profile and optimizing the searching-to-booking conversion rate\nTools used: Tableau, Python (Pandas, NumPy, Sklearn, XGBoost, Matplotlib, Seaborn)\n\nPreprocessed the raw dataset (2 tables, 43k rows, and 24 columns) by dropping or filling null values and outliers, converting data types, merging tables and rows, and engineering features\nUsed Plotly, Geopandas, Matplotlib, etc. to depict users’ demographics, visualizing insights into the booking process\nTrained 4 predictive tree models including XGBoost, combined them with the voting classifier, and used cross-validation to optimize parameters; tested the model to predict users’ stages in the conversion process (searched, inquired, accepted, and booked), getting an accuracy score of 0.82, 228% higher than guessing randomly\nSuggested business strategies with a higher potential return on investment\n\n\n\nAnalysis on Airline Dataset\nSeptember 2023\n🔗 Project Link\nObjective: Analyzed flight data to understand the demographics of passengers and predict flight status\nTools used: Python (Pandas, Sklearn, Altair)\n\nPreprocessed the raw data (98k rows, 15 columns) using pandas, and encoded categorical data with the label encoder\nVisualized passengers’ profiles using Altair to gain insights into their characteristics including nationalities, ages, etc.\nTried to predict the flight status (“on time”, “delayed”, or “canceled”) with logistic regression and random forest models"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work Experience",
    "text": "Work Experience\n\nEasyTransfer, Beijing, China\nSocial Media Marketing Project Leader and User Growth Analyst\nFebruary 2023 – May 2023 | 40hrs/week\n\nOptimized marketing strategy by using funnel model and analyzing user data, and initiated the entire process of WeChat user acquisition SOP from scratch, increasing users by 25%\nCame up with monthly planning and task allocation strategies across diverse social media channels, compiled weekly data reports of social media projects, and analyzed content and user data through social media analytics platforms, achieving a 50% overall subscription growth rate; 3 accounts grew from 0 to over 100 subscriptions with “hit” posts in a week\nSupported live broadcast backend operations and live room setup, collected viewer feedback, and reviewed and analyzed live broadcast data to improve live performance, achieving the highest view of over 1,000 and GMV of over 1,200 CNY\n\n\n\nYiche, Beijing, China\nUser Growth Analyst Intern\nOctober 2022 – January 2023 | 40hrs/week\n\nCollected user behavior data from the SCRM system and conducted data analysis and A/B tests to optimize the creator growth process, achieving a peak of 50 new creators per day and a conversion rate of over 50%\nExpanded user acquisition channels across various social media platforms to foster the growth of UGC creators base\nManaged 50 WeChat groups (about 5,000 people) in the Yiche community, conducted group message writing, pushing, and optimizing, and answered user inquiries on the mobile app and website, achieving over 70% community engagement rate and 30% weekly growth rate\n\n\n\nHou Baoyan Education, Beijing, China\nSocial Media Marketing Intern\nAugust 2022 – October 2022 | 40hrs/week\n\nAssisted with short video planning, filming, editing, and publishing, and optimized the video production process by analyzing content data, achieving over 1,000 new followers from an original of 1,300 followers in a month on Bilibili (a video platform)\nExecuted new user acquisition through free material sharing on major social media platforms, achieving an average lead generation rate of 20%, and assisted with target customer research to provide segment insights for sales increase\n\n\n\nPanopath, Shanghai, China (Hybrid)\nEvent Planning Intern\nAugust 2020 – March 2021 | 10hrs/week\n\nPlanned and executed online and in-person social activities to foster subscription of social media accounts and followers’ loyalty, participated in brainstorming, designed event process, wrote copy, managed social media groups, and promoted activities; conducted surveys and post-event reviews, achieved the highest 100 subscription growth by single event\nManaged Beijing branch of “Easiversity”, a study space project during pandemic, conducted recruitment and community activities including event process design, promotional copywriting, catering preparation, guest reception, hosting, and controlling the event, with 20 participants attended; increased Panopath’s visibility in Beijing"
  },
  {
    "objectID": "cv.html#student-club-experience",
    "href": "cv.html#student-club-experience",
    "title": "CV",
    "section": "Student Club Experience",
    "text": "Student Club Experience\n\nUCI Math Student Federation\nPresident\nJuly 2024 - June 2025\nPublic Relations, Board Member\nSeptember 2023 - June 2024\n\nCommunicated with sponsors to keep the club well-funded\nInitiated the “Club Alumni Directory” program to foster networking opportunities among current members and ex-members\nManaged recruitment and social media operations\n\n\n\nBeijing No. 15 High School Science and Technology Club\nPresident\nSeptember 2017 – September 2018\n\nManaged task allocation, daily operation, promotion, and recruitment; expanded the club from 20 to 60 members\nOrganized and executed club and school-level activities, including a school-level “Creative Fair” with 200 participants"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Nasdaq Index Prediction and Investment Simulation\nMarch 2025\nObjective: Predict future Nasdaq Index using machine learning and neural network models.\nView Notebook\n\nPreprocessed the dataset (1984 samples, 84 features) including technical indicators, futures contracts, etc. through feature augmentation, conversion, null value handling, and time-series train-test split.\n\nTrained four machine learning models (Linear Regression, Decision Tree, Random Forest, XGBoost) and four deep learning models (MLP, LSTM, 1D CNN, CNN-LSTM), with Grid Search for hyperparameter tuning.\n\nImplemented Recursive Feature Elimination and Time-Series Cross-Validation for feature selection.\n\nEvaluated and compared models and implemented an investment simulation with these models.\n\n\n\nCNN and Vision Transformer Pre-trained Models for Facial Emotion Recognition\nMay 2024\nObjective: Build CNN and Vision Transformer networks to recognize facial emotions from scratch.\nView Report View Slides\n\nPreprocessed the FER-2013 dataset (35,887 grayscale images, 48x48 pixels, 7 categories). Addressed class imbalance with resampling and used data augmentation (rotation, resizing).\n\nDesigned and optimized CNN and Vision Transformer architectures. Applied learning rate schedules, gradual unfreezing, max pooling (CNN), batch normalization, dropout, and weight decay to address under/overfitting.\n\nAchieved accuracy scores of 0.62 (CNN) and 0.66 (ViT). The “happy” category had the highest accuracy (0.83). Models generalized well on images outside the dataset.\n\n\n\nMarket Data Analysis for Airbnb in Dublin\nApril 2024\nObjective: Understand user profiles and optimize the search-to-booking conversion rate.\nTools: Tableau, Python (Pandas, NumPy, Sklearn, XGBoost, Matplotlib, Seaborn)\nView Notebook\n\nPreprocessed the dataset (2 tables, 43k rows, 24 columns): handled nulls/outliers, converted data types, merged tables, engineered features.\n\nVisualized demographics and booking behavior using Plotly, GeoPandas, and Matplotlib.\n\nTrained predictive models (including XGBoost), combined them with a voting classifier, and applied cross-validation. Achieved accuracy of 0.82 (228% higher than random guessing).\n\nSuggested business strategies with higher potential ROI.\n\n\n\nAnalysis on Airline Dataset\nSeptember 2023\nObjective: Analyze flight data to understand passenger demographics and predict flight status.\nTools: Python (Pandas, Sklearn, Altair)\n- Preprocessed dataset (98k rows, 15 columns) using pandas; encoded categorical data with label encoding.\n- Visualized passenger profiles (nationality, age, etc.) using Altair.\n- Predicted flight status (“on time,” “delayed,” “canceled”) with logistic regression and random forest models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luoning Zhang",
    "section": "",
    "text": "I am a fourth-year undergraduate student at the University of California, Irvine, majoring in Computational and Applied Mathematics with a concentration in Data Science.\nI have strong interests in Reinforcement Learning, Deep Learning, and their applications to computer vision, natural language processing, code superoptimization, automated driving, and AI for mathematics (AI4Math), with relevant research and project experience in some of these areas.\nMy goal is to leverage computer algorithms and automation technologies to free humanity from repetitive physical and mental labor, allowing us to focus on creativity and imagination, and ultimately to accelerate scientific progress and human well-being.\nContact me: luoninz1@uci.edu | Here’s my Curriculum Vitae (CV)"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Luoning Zhang",
    "section": "Interests",
    "text": "Interests\n\nReinforcement Learning\n\nDeep Learning\n\nAutomated Driving\n\nCode Superoptimization\nAI4Math"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Luoning Zhang",
    "section": "Skills",
    "text": "Skills\n\nProgramming: Python, R, MATLAB, C\n\nMachine Learning: PyTorch, Keras, Scikit-learn; familiar with deep learning, reinforcement learning, and hyperparameter optimization\n\nData Analysis & Scientific Computing: Pandas, NumPy, SciPy, Numba, Matplotlib, Seaborn, SQL, Tableau, Excel\n\nTools: Git, LaTeX\n\nDesign & Media Production: CorelDRAW, Photoshop, Canva, CapCut, VideoStudio, OBS\n\nLanguages: Fluent in English and Chinese"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Luoning Zhang",
    "section": "Education",
    "text": "Education\n\nUniversity of California, Irvine\nB.S. in Applied and Computational Mathematics (Data Science Concentration)\nSeptember 2020 – December 2025\nGPA: 3.85 (Upper-division GPA: 4.00)\nDean’s Honor List"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Optimizing LLM and RL Methods to Explore Open Problems in Combinatorics\nMarch 2025 – Ongoing\n- Implemented Monte Carlo Tree Search (MCTS) to explore the largest and smallest complete sets, and optimized the algorithm’s time complexity through algorithmic improvements and Numba acceleration (boosting performance from ~7 iter/s to 6,000+ iter/s).\n- For n = 53 … 101, the algorithm finds 1.68n points, which outperforms the previous best lower bound of ((1.5 - ε) n).\n- Ongoing: Exploring the smallest geometric dominating set and complete set; developing graph search algorithms.\n- Ongoing: Investigating LLM-based methods for code optimization to explore open problems in combinatorics.\n\n\nOptimizing Large Language Models (LLM) for Academic Support: A Framework for Knowledge Graph (KG) Embedding and Cheat Sheet Generation\nSeptember 2024 – Ongoing\nSymposium Poster (Interim Presentation)\n\nDesigned a benchmark to evaluate the performance of LLMs on college-level academic problems.\n\nIntegrated knowledge graphs (KG) for Retrieval-augmented Generation into LLMs and assessed accuracy improvements on the benchmark.\n\n\n\nInstantaneous Electromagnetic Water Heater Design and Multiphysics Field Finite Element Optimization\nAugust 2016 – October 2017\n- Designed the structure and circuitry of an induction water heater with improved energy efficiency and temperature stability.\n- Coded a program for temperature control and user interaction on Arduino.\n- Optimized energy efficiency by applying thermodynamic and electromagnetic models and conducting multiphysics finite element simulations in ANSYS.\n- Applied and accepted as Patent Number: ZL201621267831.6."
  },
  {
    "objectID": "publications-patents.html",
    "href": "publications-patents.html",
    "title": "Publications & Patents",
    "section": "",
    "text": "Work in progress: Optimizing LLM and RL Methods to Explore Open Problems in Combinatorics."
  },
  {
    "objectID": "publications-patents.html#publications",
    "href": "publications-patents.html#publications",
    "title": "Publications & Patents",
    "section": "",
    "text": "Work in progress: Optimizing LLM and RL Methods to Explore Open Problems in Combinatorics."
  },
  {
    "objectID": "publications-patents.html#patents",
    "href": "publications-patents.html#patents",
    "title": "Publications & Patents",
    "section": "Patents",
    "text": "Patents\n\nUtility Model Patent: Instant Induction Water Heater with Automatic Temperature Control.\nPatent Number: CN 2016 2 1267831.6.\nApplication Date: Nov. 23, 2016.\nPublication Date: May 24, 2017.\nPatentee: Luoning Zhang."
  },
  {
    "objectID": "courseworks.html",
    "href": "courseworks.html",
    "title": "Courseworks",
    "section": "",
    "text": "Courses Taken:\n\nMATH 9 – Introduction to Programming for Numerical Analysis\n\nMATH 10 – Introduction to Programming for Data Science\n\nMATH 105A–B – Numerical Analysis (two-course sequence)\n\nMATH 110A–B – Optimization (two-course sequence)\n\nMATH 176 – Mathematics of Finance\n\nMATH 178 – Mathematical Machine Learning\n\nCOMPSCI 172B – Neural Networks and Deep Learning\n\nEquivalent to Graduate Course COMPSCI 274C\n\n\nSTATS 110 – Statistical Methods for Data Analysis I\n\nEquivalent to Graduate Course STATS 201"
  },
  {
    "objectID": "dublin-airbnb-analysis.html",
    "href": "dublin-airbnb-analysis.html",
    "title": "Datathon Project-UCI MSF Team",
    "section": "",
    "text": "Members: Songhan Hu, Jiaye Liu, Luoning Zhang Dataset: Market Analysis in Dublin"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#data-cleaning-and-pre-processing",
    "href": "dublin-airbnb-analysis.html#data-cleaning-and-pre-processing",
    "title": "Datathon Project-UCI MSF Team",
    "section": "Data Cleaning and Pre-processing",
    "text": "Data Cleaning and Pre-processing\n\nimport pandas as pd\nimport optuna\nfrom sklearn.svm import SVC\nimport geopandas as gpd\nimport pycountry\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\n\n\ndf1 = pd.read_csv('contacts.tsv', delimiter='\\t',parse_dates=['ds_checkin','ds_checkout'])\ndf2 = pd.read_csv('searches.tsv', delimiter='\\t',parse_dates=['ds','ds_checkin','ds_checkout'],)\n\n\ndf_s = df2.copy()\n\ndf_s['filter_room_types'] = df_s['filter_room_types'].apply(lambda x: [room.strip() for room in x.split(',')] if isinstance(x, str) else ['unknown'])\ndf_s['filter_neighborhoods'] = df_s['filter_neighborhoods'].apply(lambda x: [neighborhood.strip() for neighborhood in x.split(',')] if isinstance(x, str) else ['unknown'])\ndf_s['filter_room_types'] = df_s['filter_room_types'].apply(lambda x: set(x))\ndf_s['filter_neighborhoods'] = df_s['filter_neighborhoods'].apply(lambda x: set(x))\n\ndf_s['num_room_types'] = df_s['filter_room_types'].apply(lambda x: 0 if x == {'unknown'} else len(x))\ndf_s['day_diff'] = (df_s['ds_checkin'] - df_s['ds']).dt.days\n\n\ndf_s['num_neighborhoods'] = df_s['filter_neighborhoods'].apply(lambda x: 0 if x == {'unknown'} else len(x))\n\nfor i in df_s.index:\n    df_s.at[i, 'filter_room_types'] = [item for item in df_s.at[i, 'filter_room_types'] if item]\n    df_s.at[i, 'filter_neighborhoods'] = [item for item in df_s.at[i, 'filter_neighborhoods'] if item]\n\ndf_s['no_check_date'] = [1 if x else 0 for x in df_s['ds_checkin'].isna()]\n\ndf_s['ds'] = pd.to_datetime(df_s['ds'])\ndf_s['ds_checkin'] = pd.to_datetime(df_s['ds_checkin'])\ndf_s['ds_checkout'] = pd.to_datetime(df_s['ds_checkout'])\ndf_s['no_price_preference'] = [1 if x else 0 for x in df_s['filter_price_max'].isna()]\n\ndf_s['no_cap'] = 0\ndf_s.loc[df_s['filter_price_max'] &gt; 10000000, 'no_cap'] = 1\n\n\ndeal_user = set(df1['id_guest'][~df1['ts_booking_at'].isna()]).intersection(set(df_s['id_user']))\n\ndf_s['is_finally_deal'] = [1 if x in deal_user else 0 for x in df_s['id_user']]\n\ndf_s['search_index'] = df_s.groupby('id_user').cumcount() + 1\n\n\npivot_columns = ['ds_checkin', 'ds_checkout', 'n_searches', 'n_nights',\n                 'n_guests_min', 'n_guests_max', 'origin_country', 'filter_price_min',\n                 'filter_price_max', 'filter_room_types', 'filter_neighborhoods',\n                 'no_check_date', 'no_price_preference', 'no_cap']\n\ndf_s_pivot = pd.DataFrame()\n\nmax_index = df_s['search_index'].max()\nfor i in range(1, max_index + 1):\n    temp_df = df_s[df_s['search_index'] == i]\n    pivoted = temp_df.set_index('id_user')[pivot_columns].add_prefix(f'search_{i}_')\n    if df_s_pivot.empty:\n        df_s_pivot = pivoted\n    else:\n        df_s_pivot = pd.concat([df_s_pivot, pivoted], axis=1)\n\ndf_s_pivot.reset_index(inplace=True)\ndf_s_pivot.head()\n\n/var/folders/cl/vxfw4d0s681922_9crphysvm0000gn/T/ipykernel_64085/1384273355.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df_s_pivot.reset_index(inplace=True)\n\n\n\n\n\n\n\n\n\nid_user\nsearch_1_ds_checkin\nsearch_1_ds_checkout\nsearch_1_n_searches\nsearch_1_n_nights\nsearch_1_n_guests_min\nsearch_1_n_guests_max\nsearch_1_origin_country\nsearch_1_filter_price_min\nsearch_1_filter_price_max\n...\nsearch_81_n_guests_min\nsearch_81_n_guests_max\nsearch_81_origin_country\nsearch_81_filter_price_min\nsearch_81_filter_price_max\nsearch_81_filter_room_types\nsearch_81_filter_neighborhoods\nsearch_81_no_check_date\nsearch_81_no_price_preference\nsearch_81_no_cap\n\n\n\n\n0\n0000af0a-6f26-4233-9832-27efbfb36148\n2014-10-09\n2014-10-12\n16\n3.0\n2\n2\nIE\n0.0\n67.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n000cd9d3-e05b-4016-9e09-34a6f8ba2fc5\nNaT\nNaT\n1\nNaN\n1\n1\nGB\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n001c04f0-5a94-4ee0-bf5d-3591265256de\nNaT\nNaT\n1\nNaN\n1\n1\nIE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n004e88da-930b-4209-886f-b58f90fdc67e\n2014-10-03\n2014-10-05\n7\n2.0\n5\n5\nSE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n00623353-97d2-43c9-ac02-1adfecf1aca6\n2014-11-01\n2014-11-09\n6\n8.0\n1\n1\nIE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 1135 columns\n\n\n\n\ndef prepare_feature(df, group_col, agg_col, new_col_name, fill_value=0):\n    \"\"\" Helper function to prepare features by aggregating and filling missing values. \"\"\"\n    feature = df.groupby(group_col)[agg_col].median().reset_index()\n    feature.columns = [group_col, new_col_name]\n    feature[new_col_name] = feature[new_col_name].fillna(fill_value)\n    return feature\n\n# Define the DataFrame df_s and df_s_pivot properly above this snippet\nn_nights_mid_df = prepare_feature(df_s, 'id_user', 'n_nights', 'n_night_mid')\nn_nights_var_df = prepare_feature(df_s, 'id_user', 'n_nights', 'n_night_var', fill_value=0)\nn_guest_min_mid_df = prepare_feature(df_s, 'id_user', 'n_guests_min', 'n_guests_min_mid')\nn_guest_max_mid_df = prepare_feature(df_s, 'id_user', 'n_guests_max', 'n_guests_max_mid', fill_value=16)\nfilter_price_min_mid_df = prepare_feature(df_s, 'id_user', 'filter_price_min', 'filter_price_min_mid')\nfilter_price_max_mid_df = prepare_feature(df_s, 'id_user', 'filter_price_max', 'filter_price_max_mid', fill_value=1073741823)\nn_room_type_mid_df = prepare_feature(df_s, 'id_user', 'num_room_types', 'num_room_types')\nn_neigh_type_mid_df = prepare_feature(df_s, 'id_user', 'num_neighborhoods', 'num_neighborhoods')\nn_day_diff_mid_df = prepare_feature(df_s, 'id_user', 'day_diff', 'day_diff')\n\ndf_ = pd.concat([df_s_pivot['id_user'], pd.get_dummies(df_s_pivot['search_1_origin_country'])], axis=1)\n\nprint(n_nights_mid_df.head(), n_guest_min_mid_df.head())\n\n                                id_user  n_night_mid\n0  0000af0a-6f26-4233-9832-27efbfb36148          3.0\n1  00058bcf-8950-4481-a977-d08b42d1fce5          0.0\n2  000ab7fb-dbac-414f-9080-88f265e2243e          2.0\n3  000b7ff7-47ea-48d3-9b09-1edd150acb02          0.0\n4  000c5621-b577-465d-be91-75254d75cc68         19.0                                 id_user  n_guests_min_mid\n0  0000af0a-6f26-4233-9832-27efbfb36148               2.0\n1  00058bcf-8950-4481-a977-d08b42d1fce5               1.0\n2  000ab7fb-dbac-414f-9080-88f265e2243e               2.0\n3  000b7ff7-47ea-48d3-9b09-1edd150acb02               1.0\n4  000c5621-b577-465d-be91-75254d75cc68               1.5\n\n\n\nsearch_counts = df_s.groupby('id_user')['search_index'].max().reset_index().rename(columns={'search_index': 'num_of_searches'})\naccept_user = set(df1.loc[df1['ts_accepted_at'].notna(), 'id_guest']).intersection(set(df_s['id_user']))\ninquiry_user = set(df1['id_guest']).intersection(set(df_s['id_user']))\n\nfeature_dfs = [n_nights_mid_df, n_nights_var_df, n_guest_max_mid_df, n_guest_min_mid_df, filter_price_min_mid_df,\n               filter_price_max_mid_df, n_room_type_mid_df, n_neigh_type_mid_df, n_day_diff_mid_df]\nuser_search_data = search_counts\n\nfor df in feature_dfs:\n    user_search_data = pd.merge(user_search_data, df, on='id_user', how='left')\n\nuser_search_data['is_booking'] = user_search_data['id_user'].apply(lambda x: 1 if x in deal_user else 0)\nuser_search_data['is_accept'] = user_search_data['id_user'].apply(lambda x: 1 if x in accept_user else 0)\nuser_search_data['is_inquiry'] = user_search_data['id_user'].apply(lambda x: 1 if x in inquiry_user else 0)\n\nuser_search_data['type_order'] = user_search_data['is_inquiry'] + user_search_data['is_accept'] + user_search_data['is_booking']\nuser_search_data.drop(['is_booking', 'is_accept', 'is_inquiry'], axis=1, inplace=True)\n\nif 'df_' in locals():\n    user_search_data = pd.merge(user_search_data, df_, on='id_user', how='left')\n\nuser_search_data\n\n\n\n\n\n\n\n\nid_user\nnum_of_searches\nn_night_mid\nn_night_var\nn_guests_max_mid\nn_guests_min_mid\nfilter_price_min_mid\nfilter_price_max_mid\nnum_room_types\nnum_neighborhoods\n...\nTZ\nUA\nUG\nUS\nUY\nUZ\nVE\nVN\nZA\nZW\n\n\n\n\n0\n0000af0a-6f26-4233-9832-27efbfb36148\n3\n3.0\n3.0\n2.0\n2.0\n0.0\n6.700000e+01\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n00058bcf-8950-4481-a977-d08b42d1fce5\n1\n0.0\n0.0\n1.0\n1.0\n0.0\n7.000000e+01\n2.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n000ab7fb-dbac-414f-9080-88f265e2243e\n1\n2.0\n2.0\n2.0\n2.0\n0.0\n1.073742e+09\n1.0\n0.0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n000b7ff7-47ea-48d3-9b09-1edd150acb02\n1\n0.0\n0.0\n1.0\n1.0\n0.0\n1.073742e+09\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n000c5621-b577-465d-be91-75254d75cc68\n2\n19.0\n19.0\n1.5\n1.5\n0.0\n7.400000e+01\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18600\nfff2a379-46a1-4e09-9515-05d2cd26ceaa\n1\n3.0\n3.0\n2.0\n2.0\n0.0\n1.073742e+09\n2.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18601\nfff3b53b-09fd-4681-9b79-4bc07360488f\n1\n1.0\n1.0\n2.0\n2.0\n0.0\n1.770000e+02\n2.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n18602\nfffa2e94-f426-405f-ab11-6f6602731ec8\n1\n0.0\n0.0\n1.0\n1.0\n0.0\n1.073742e+09\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18603\nfffea166-9432-43a7-8b1b-09d6f30c1c07\n4\n6.5\n6.5\n2.0\n2.0\n0.0\n3.400000e+01\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18604\nffffb713-5497-4c20-b157-31356dec6e0e\n1\n0.0\n0.0\n1.0\n1.0\n0.0\n1.073742e+09\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n18605 rows × 143 columns"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#visualization",
    "href": "dublin-airbnb-analysis.html#visualization",
    "title": "Datathon Project-UCI MSF Team",
    "section": "Visualization",
    "text": "Visualization\n\nFunnel Model\n\nsns.set()\n\nnp_stage = user_search_data.groupby('type_order').count()['id_user'].to_numpy()\nfor i in [2,1,0]:\n    np_stage[i] = np_stage[i] + np_stage[i+1]\n\ndata = dict(\n    number=np_stage,\n    stage=[\"Searched\", \"Inquired\", \"Be accepted\", \"Booked\"])\nfig = px.funnel(data, x='number', y='stage')\nfig.show()\n\n                                                \n\n\n\n\nOrigin Country\n\ndef get_alpha3_code(alpha2_code):\n    try:\n        return pycountry.countries.get(alpha_2=alpha2_code).alpha_3\n    except AttributeError:\n        return None\n\ncountry_counts = df_s['origin_country'].value_counts().reset_index()\ncountry_counts.columns = ['CountryCode', 'Count']\ncountry_counts['iso_a3'] = country_counts['CountryCode'].apply(get_alpha3_code)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld = world.merge(country_counts, how='left', left_on='iso_a3', right_on='iso_a3')\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nworld.boundary.plot(ax=ax)\nworld.plot(column='Count', ax=ax, legend=True,\n           legend_kwds={'label': \"Country Frequency\"},\n           cmap='OrRd', missing_kwds={'color': 'lightgrey'})\nplt.show()\n\n/var/folders/cl/vxfw4d0s681922_9crphysvm0000gn/T/ipykernel_63988/334731399.py:11: FutureWarning:\n\nThe geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n\n\n\n\n\n\n\n\n\n\n\n# There is a user from Antarctica!!\ncountry_counts[country_counts['iso_a3'] == \"ATA\"]\n\n\n\n\n\n\n\n\nCountryCode\nCount\niso_a3\n\n\n\n\n130\nAQ\n1\nATA\n\n\n\n\n\n\n\n\n\nDifference between Searching Date and Checkin Date\n\nplt.figure(figsize=(10, 6))\n\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 0]['day_diff'], bw_adjust=0.5, fill=True, label='Search')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 1]['day_diff'], bw_adjust=0.5, fill=True, label='Inquiry')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 2]['day_diff'], bw_adjust=0.5, fill=True, label='Accepted')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 3]['day_diff'], bw_adjust=0.5, fill=True, label='Book')\nplt.title('Distribution of the Difference between Searching Date and Checkin Date')\nplt.xlabel('Days')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNumber of Searches for Each Unique User\n\nplt.figure(figsize=(10, 6))\n\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 0]['num_of_searches'], bw_adjust=0.5, fill=True, label='Search')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 1]['num_of_searches'], bw_adjust=0.5, fill=True, label='Inquiry')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 2]['num_of_searches'], bw_adjust=0.5, fill=True, label='Accepted')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 3]['num_of_searches'], bw_adjust=0.5, fill=True, label='Book')\nplt.title('Distribution of umber of Searches for Each Unique User')\nplt.xlabel('Count')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\n\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 0]['num_of_searches'], clip=(0,10), bw_adjust=0.2, fill=True, label='Search')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 1]['num_of_searches'], clip=(0,10), bw_adjust=0.2, fill=True, label='Inquiry')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 2]['num_of_searches'], clip=(0,10), bw_adjust=0.2, fill=True, label='Accepted')\nsns.kdeplot(user_search_data[user_search_data['type_order'] == 3]['num_of_searches'], clip=(0,10), bw_adjust=0.2, fill=True, label='Book')\nplt.title('Distribution of umber of Searches for Each Unique User')\nplt.xlabel('Count')\nplt.ylabel('Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#see-here-decided-why-to-use-median-and-tree-model",
    "href": "dublin-airbnb-analysis.html#see-here-decided-why-to-use-median-and-tree-model",
    "title": "Datathon Project-UCI MSF Team",
    "section": "See here decided why to use median and tree model",
    "text": "See here decided why to use median and tree model\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\nfig.suptitle('Box Plots to Identify Outliers in Numerical Columns', fontsize=16)\n\nnum_cols = ['n_searches', 'n_nights', 'filter_price_min', 'filter_price_max']\nfor ax, col in zip(axes.flatten(), num_cols):\n    df_s.boxplot(column=col, ax=ax)\n    ax.set_title(col)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#model-training",
    "href": "dublin-airbnb-analysis.html#model-training",
    "title": "Datathon Project-UCI MSF Team",
    "section": "Model Training",
    "text": "Model Training\nWe first use XGBoost, cross validation to find the best parameter.\n\nX = user_search_data.copy().drop(['id_user'], axis=1)\ny = X.pop(\"type_order\")\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2)\n\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 5, 40),\n        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 4),\n        'subsample': trial.suggest_float('subsample', 0.2, 1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1),\n        'lambda': trial.suggest_float('lambda', 1e-8, 1.0),  # L2 regularization\n        'alpha': trial.suggest_float('alpha', 1e-8, 1.0),  # L1 regularization\n        'use_label_encoder': False,  # Avoid a deprecation warning\n    }\n    xgb_model = XGBClassifier(**params, eval_metric='mlogloss', verbosity=0)\n\n    cv = cross_val_score(xgb_model, X, y, cv=4, scoring='accuracy').mean()\n    return cv\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100, timeout=1800, show_progress_bar=True)\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('Value: ', trial.value)\nprint('Params: ')\nfor key, value in trial.params.items():\n    print(f'    {key}: {value}')\n\n[I 2024-04-13 20:48:27,128] A new study created in memory with name: no-name-c61da181-f6f0-4406-a28e-2ca6ef57cfcc\n\n\n\n\n\n[I 2024-04-13 20:49:31,625] Trial 0 finished with value: 0.813813466274415 and parameters: {'n_estimators': 1337, 'learning_rate': 0.047229613231243694, 'max_depth': 18, 'min_child_weight': 1.5004360596379311, 'subsample': 0.48720956244704877, 'colsample_bytree': 0.24118622095320275, 'lambda': 0.5198374651782831, 'alpha': 0.6542260507192192}. Best is trial 0 with value: 0.813813466274415.\n[I 2024-04-13 20:51:45,982] Trial 1 finished with value: 0.8203711033583511 and parameters: {'n_estimators': 1624, 'learning_rate': 0.006477410403064766, 'max_depth': 27, 'min_child_weight': 1.9391492954946157, 'subsample': 0.26393362218029454, 'colsample_bytree': 0.958667550622444, 'lambda': 0.7275514654420072, 'alpha': 0.24795684861624576}. Best is trial 1 with value: 0.8203711033583511.\n[I 2024-04-13 20:54:00,094] Trial 2 finished with value: 0.8025263915728882 and parameters: {'n_estimators': 1822, 'learning_rate': 0.04379408103268792, 'max_depth': 19, 'min_child_weight': 3.2587794436607522, 'subsample': 0.6397393146522905, 'colsample_bytree': 0.9778316529309103, 'lambda': 0.5242213883065793, 'alpha': 0.640957442042659}. Best is trial 1 with value: 0.8203711033583511.\n[I 2024-04-13 20:55:31,997] Trial 3 finished with value: 0.7984413988023544 and parameters: {'n_estimators': 1625, 'learning_rate': 0.09525925387395676, 'max_depth': 20, 'min_child_weight': 3.8200242269078672, 'subsample': 0.5442124924293351, 'colsample_bytree': 0.657405813353553, 'lambda': 0.1247179047579049, 'alpha': 0.8382256793834322}. Best is trial 1 with value: 0.8203711033583511.\n[I 2024-04-13 20:57:10,276] Trial 4 finished with value: 0.8130075000281931 and parameters: {'n_estimators': 1280, 'learning_rate': 0.01682254047988452, 'max_depth': 24, 'min_child_weight': 2.2456629893841953, 'subsample': 0.4245461662781999, 'colsample_bytree': 0.9693097176818797, 'lambda': 0.6012631974370846, 'alpha': 0.5096914842848129}. Best is trial 1 with value: 0.8203711033583511.\n[I 2024-04-13 20:59:07,689] Trial 5 finished with value: 0.8002688587759212 and parameters: {'n_estimators': 1859, 'learning_rate': 0.061011727815679616, 'max_depth': 40, 'min_child_weight': 2.5507746123075745, 'subsample': 0.2686234674222428, 'colsample_bytree': 0.9036252567648639, 'lambda': 0.40555021926099183, 'alpha': 0.7614547968380192}. Best is trial 1 with value: 0.8203711033583511.\n[I 2024-04-13 20:59:58,396] Trial 6 finished with value: 0.824509663137006 and parameters: {'n_estimators': 552, 'learning_rate': 0.009244780950413032, 'max_depth': 21, 'min_child_weight': 1.1776475763509335, 'subsample': 0.5787484399490634, 'colsample_bytree': 0.6717104435517728, 'lambda': 0.27308200188135845, 'alpha': 0.6604682731983871}. Best is trial 6 with value: 0.824509663137006.\n[I 2024-04-13 21:00:50,344] Trial 7 finished with value: 0.8154797399314824 and parameters: {'n_estimators': 1133, 'learning_rate': 0.07299712057147484, 'max_depth': 13, 'min_child_weight': 1.3292628334912076, 'subsample': 0.8438719229660459, 'colsample_bytree': 0.2002516250948327, 'lambda': 0.8198157691830962, 'alpha': 0.8928324142818284}. Best is trial 6 with value: 0.824509663137006.\n[I 2024-04-13 21:03:00,417] Trial 8 finished with value: 0.8154260458230398 and parameters: {'n_estimators': 1534, 'learning_rate': 0.017041429123773714, 'max_depth': 30, 'min_child_weight': 1.722301834175352, 'subsample': 0.656351410385265, 'colsample_bytree': 0.5333034234046892, 'lambda': 0.10003377531260546, 'alpha': 0.5853008152606514}. Best is trial 6 with value: 0.824509663137006.\n[I 2024-04-13 21:03:43,984] Trial 9 finished with value: 0.8259607790593393 and parameters: {'n_estimators': 686, 'learning_rate': 0.0009626545964325102, 'max_depth': 15, 'min_child_weight': 0.8950946674929849, 'subsample': 0.653682987578901, 'colsample_bytree': 0.9653556818615103, 'lambda': 0.9981782746720972, 'alpha': 0.8084151115559389}. Best is trial 9 with value: 0.8259607790593393.\n[I 2024-04-13 21:03:58,621] Trial 10 finished with value: 0.826068352149419 and parameters: {'n_estimators': 534, 'learning_rate': 0.029566664080238888, 'max_depth': 6, 'min_child_weight': 0.5493692401340649, 'subsample': 0.9582245810596254, 'colsample_bytree': 0.7785256985419187, 'lambda': 0.9163152035026848, 'alpha': 0.26196592563605225}. Best is trial 10 with value: 0.826068352149419.\n[I 2024-04-13 21:04:11,245] Trial 11 finished with value: 0.8261220924761601 and parameters: {'n_estimators': 530, 'learning_rate': 0.030412496626778224, 'max_depth': 5, 'min_child_weight': 0.5009248161992941, 'subsample': 0.9670656084409862, 'colsample_bytree': 0.7708009548973506, 'lambda': 0.9982146279800529, 'alpha': 0.2380032301548375}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:04:31,200] Trial 12 finished with value: 0.8250471819501645 and parameters: {'n_estimators': 867, 'learning_rate': 0.032734589881235905, 'max_depth': 5, 'min_child_weight': 0.5352160584504654, 'subsample': 0.9953504502364621, 'colsample_bytree': 0.7988319275869853, 'lambda': 0.9832896170125175, 'alpha': 0.21798153039974583}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:04:50,877] Trial 13 finished with value: 0.8249934416234233 and parameters: {'n_estimators': 871, 'learning_rate': 0.03333523971613389, 'max_depth': 5, 'min_child_weight': 0.5819147172295058, 'subsample': 0.9592633112246078, 'colsample_bytree': 0.807339787548702, 'lambda': 0.8160869169231629, 'alpha': 0.022940498384892316}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:05:17,692] Trial 14 finished with value: 0.8234884698286022 and parameters: {'n_estimators': 551, 'learning_rate': 0.03023121180316528, 'max_depth': 10, 'min_child_weight': 1.0492797175238322, 'subsample': 0.8058336751399967, 'colsample_bytree': 0.5322391452470353, 'lambda': 0.8713844462828828, 'alpha': 0.3076091161667134}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:05:58,489] Trial 15 finished with value: 0.8082776117821906 and parameters: {'n_estimators': 934, 'learning_rate': 0.06462866610737526, 'max_depth': 9, 'min_child_weight': 2.732941476903637, 'subsample': 0.8279724926318875, 'colsample_bytree': 0.7560034708481365, 'lambda': 0.6708791399556301, 'alpha': 0.38989071871013525}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:06:26,387] Trial 16 finished with value: 0.8241871980674096 and parameters: {'n_estimators': 695, 'learning_rate': 0.023893451508534832, 'max_depth': 9, 'min_child_weight': 0.8047267398489057, 'subsample': 0.8865787380701572, 'colsample_bytree': 0.4895347101413101, 'lambda': 0.8909494046771829, 'alpha': 0.05934635947850925}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:07:40,460] Trial 17 finished with value: 0.8135986205131969 and parameters: {'n_estimators': 1037, 'learning_rate': 0.04424298317396684, 'max_depth': 37, 'min_child_weight': 0.5557605600982021, 'subsample': 0.7473014965520811, 'colsample_bytree': 0.34538123082588207, 'lambda': 0.7451447802738025, 'alpha': 0.14116553608857246}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:08:25,855] Trial 18 finished with value: 0.8100512736561429 and parameters: {'n_estimators': 667, 'learning_rate': 0.056354576952966924, 'max_depth': 14, 'min_child_weight': 1.9175694242757688, 'subsample': 0.9266279983348323, 'colsample_bytree': 0.7124872198711556, 'lambda': 0.39553012835088786, 'alpha': 0.3748087445659024}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:08:39,630] Trial 19 finished with value: 0.8216072579737196 and parameters: {'n_estimators': 505, 'learning_rate': 0.07807390321146618, 'max_depth': 5, 'min_child_weight': 1.447761828581032, 'subsample': 0.7411336604528174, 'colsample_bytree': 0.8482182716247123, 'lambda': 0.9387511733773851, 'alpha': 0.4393780417462038}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:09:33,807] Trial 20 finished with value: 0.8134372724326521 and parameters: {'n_estimators': 772, 'learning_rate': 0.036428337416606144, 'max_depth': 33, 'min_child_weight': 2.986317642055114, 'subsample': 0.36216400097035273, 'colsample_bytree': 0.6045081342768153, 'lambda': 0.6542072354177585, 'alpha': 0.11405944484416197}. Best is trial 11 with value: 0.8261220924761601.\n[I 2024-04-13 21:10:35,815] Trial 21 finished with value: 0.8262295384659185 and parameters: {'n_estimators': 678, 'learning_rate': 0.00027262230602636075, 'max_depth': 15, 'min_child_weight': 1.0116302454830888, 'subsample': 0.7237819360653299, 'colsample_bytree': 0.8696735760725043, 'lambda': 0.9979845742208219, 'alpha': 0.19502713141165656}. Best is trial 21 with value: 0.8262295384659185.\n[I 2024-04-13 21:11:11,389] Trial 22 finished with value: 0.8244022171472476 and parameters: {'n_estimators': 510, 'learning_rate': 0.020788198953459194, 'max_depth': 11, 'min_child_weight': 0.8780630344106966, 'subsample': 0.7548131898342476, 'colsample_bytree': 0.8599940681890501, 'lambda': 0.907393538757365, 'alpha': 0.1913947587815238}. Best is trial 21 with value: 0.8262295384659185.\n[I 2024-04-13 21:11:40,793] Trial 23 finished with value: 0.8266596343984679 and parameters: {'n_estimators': 778, 'learning_rate': 0.009306603424580433, 'max_depth': 8, 'min_child_weight': 1.06465509758388, 'subsample': 0.9072207944572264, 'colsample_bytree': 0.7658441822251906, 'lambda': 0.783959485397532, 'alpha': 0.30582332667893436}. Best is trial 23 with value: 0.8266596343984679.\n[I 2024-04-13 21:12:58,877] Trial 24 finished with value: 0.8247784340981599 and parameters: {'n_estimators': 786, 'learning_rate': 0.000899010873623342, 'max_depth': 16, 'min_child_weight': 1.1711545565925352, 'subsample': 0.8626388874125682, 'colsample_bytree': 0.8746125769801567, 'lambda': 0.7905707125061482, 'alpha': 0.3379222447871222}. Best is trial 23 with value: 0.8266596343984679.\n[I 2024-04-13 21:13:44,056] Trial 25 finished with value: 0.824939701296682 and parameters: {'n_estimators': 1023, 'learning_rate': 0.011620423872540342, 'max_depth': 8, 'min_child_weight': 1.5731654000013582, 'subsample': 0.8888144477272626, 'colsample_bytree': 0.7354268068603133, 'lambda': 0.999866701875694, 'alpha': 0.4629631308606137}. Best is trial 23 with value: 0.8266596343984679.\n[I 2024-04-13 21:14:31,938] Trial 26 finished with value: 0.8242947133846159 and parameters: {'n_estimators': 635, 'learning_rate': 0.011948647446816604, 'max_depth': 12, 'min_child_weight': 0.8917237020680562, 'subsample': 0.7069706700284674, 'colsample_bytree': 0.6806805770029137, 'lambda': 0.8420137513945466, 'alpha': 0.1579317861953075}. Best is trial 23 with value: 0.8266596343984679.\n[I 2024-04-13 21:15:51,394] Trial 27 finished with value: 0.8145124024955662 and parameters: {'n_estimators': 814, 'learning_rate': 0.023518283467701503, 'max_depth': 17, 'min_child_weight': 2.1531941423259515, 'subsample': 0.9951758044390351, 'colsample_bytree': 0.9101319895013484, 'lambda': 0.7454106511469829, 'alpha': 0.29914243167348653}. Best is trial 23 with value: 0.8266596343984679.\n[I 2024-04-13 21:16:49,961] Trial 28 finished with value: 0.8274658201816083 and parameters: {'n_estimators': 1171, 'learning_rate': 0.004693099965535389, 'max_depth': 8, 'min_child_weight': 1.2895967986038595, 'subsample': 0.7804638105059425, 'colsample_bytree': 0.6081963765934802, 'lambda': 0.9255099991762942, 'alpha': 0.06406101879012513}. Best is trial 28 with value: 0.8274658201816083.\n[I 2024-04-13 21:18:28,451] Trial 29 finished with value: 0.8258532175238343 and parameters: {'n_estimators': 1444, 'learning_rate': 0.0038715750889392595, 'max_depth': 13, 'min_child_weight': 1.697845224896466, 'subsample': 0.7983637803530047, 'colsample_bytree': 0.4022918223666994, 'lambda': 0.6511410294605691, 'alpha': 0.08047711382215175}. Best is trial 28 with value: 0.8274658201816083.\nBest trial:\nValue:  0.8274658201816083\nParams: \n    n_estimators: 1171\n    learning_rate: 0.004693099965535389\n    max_depth: 8\n    min_child_weight: 1.2895967986038595\n    subsample: 0.7804638105059425\n    colsample_bytree: 0.6081963765934802\n    lambda: 0.9255099991762942\n    alpha: 0.06406101879012513\n\n\n\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train, y_train)\ny_pred = xgb_classifier.predict(X_valid)\nxgb_accuracy_score = accuracy_score(y_valid, y_pred)\nprint(\"XGBClassifier Accuracy:\", xgb_accuracy_score)\n\nXGBClassifier Accuracy: 0.8212846009137329\n\n\nComparing with other models, we use the combined model called voting classifier to identify the stage of people.\n\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\nrf_y_pred = rf_classifier.predict(X_valid)\nrf_accuracy_score = accuracy_score(y_valid, rf_y_pred)\nrf_accuracy_score\n\n0.8105348024724537\n\n\n\nmlp_classifier = MLPClassifier()\nmlp_classifier.fit(X_train, y_train)\nmlp_y_pred = mlp_classifier.predict(X_valid)\nmlp_accuracy_score = accuracy_score(y_valid, mlp_y_pred)\nmlp_accuracy_score\n\n0.806503628056974\n\n\n\nsvm_classifier = SVC(probability=True)\nsvm_classifier.fit(X_train, y_train)\nsvm_y_pred = svm_classifier.predict(X_valid)\nsvm_accuracy_score = accuracy_score(y_valid, svm_y_pred)\nsvm_accuracy_score\n\n0.8210158559527009\n\n\nVoting Classifier\n\nclassifiers = [('random_forest', rf_classifier),\n    ('svm', svm_classifier),\n    ('mlp', mlp_classifier),\n    ('xgb', xgb_classifier)\n]\n\nvoting_clf = VotingClassifier(estimators=classifiers, voting='hard')\nvoting_clf.fit(X_train, y_train)\nvoting_pred = voting_clf.predict(X_valid)\nvoting_accuracy_score = accuracy_score(y_valid, voting_pred)\nvoting_accuracy_score\n\n0.8237033055630207\n\n\n\nxgb_classifier.feature_importances_\n\narray([0.03214042, 0.02455774, 0.02288991, 0.01597637, 0.0157053 ,\n       0.01522262, 0.01760654, 0.01486355, 0.01796535, 0.0191166 ,\n       0.        , 0.01155449, 0.        , 0.        , 0.        ,\n       0.01604011, 0.01683889, 0.01560377, 0.        , 0.        ,\n       0.        , 0.        , 0.01549766, 0.        , 0.01013466,\n       0.02900742, 0.        , 0.        , 0.01473845, 0.00728089,\n       0.0157551 , 0.01849182, 0.        , 0.        , 0.01439588,\n       0.00661679, 0.        , 0.        , 0.        , 0.01467162,\n       0.01698269, 0.01810054, 0.        , 0.        , 0.        ,\n       0.        , 0.00872591, 0.0160712 , 0.        , 0.        ,\n       0.01640791, 0.01694654, 0.01688639, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.0136373 , 0.        ,\n       0.01868004, 0.01573102, 0.01299768, 0.        , 0.03188633,\n       0.01033509, 0.        , 0.01145769, 0.        , 0.        ,\n       0.01650339, 0.        , 0.00598313, 0.00983884, 0.        ,\n       0.01450301, 0.        , 0.        , 0.        , 0.00576115,\n       0.        , 0.        , 0.00706745, 0.01864195, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.00629945, 0.        , 0.        , 0.01314489, 0.01169234,\n       0.        , 0.        , 0.        , 0.01755105, 0.01236612,\n       0.        , 0.01578866, 0.        , 0.        , 0.00903834,\n       0.        , 0.01211253, 0.        , 0.01390858, 0.        ,\n       0.        , 0.01319115, 0.01226211, 0.01494829, 0.01316986,\n       0.01216134, 0.01945747, 0.00908214, 0.00818385, 0.        ,\n       0.        , 0.        , 0.00898265, 0.        , 0.01442079,\n       0.01242575, 0.        , 0.01064345, 0.        , 0.01584867,\n       0.        , 0.        , 0.        , 0.        , 0.01750329,\n       0.        ], dtype=float32)"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#decision-boundaries",
    "href": "dublin-airbnb-analysis.html#decision-boundaries",
    "title": "Datathon Project-UCI MSF Team",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\nThat is a really complex model\n\n# Extract the two features and the target variable\nX = user_search_data[['num_of_searches', 'day_diff']]\ny = user_search_data['type_order']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\n# Create a mesh to plot in\nx_min, x_max = X['num_of_searches'].min() - 1, X['num_of_searches'].max() + 1\ny_min, y_max = X['day_diff'].min() - 1, X['day_diff'].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\n# Predict on the mesh\nZ = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\nplt.xlabel('num_of_searches')\nplt.ylabel('day_diff')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title('Decision Boundary with XGBoost')\nplt.show()"
  },
  {
    "objectID": "dublin-airbnb-analysis.html#conclusion",
    "href": "dublin-airbnb-analysis.html#conclusion",
    "title": "Datathon Project-UCI MSF Team",
    "section": "Conclusion",
    "text": "Conclusion\n\nMarketing campaign should be performed around 25 days before the busy season for traveling\nEncourage users to do more searching\nOur model can predict the user’s “drop out” stage, therefore we can make accurate action to avoid this.\n\n   Created in Deepnote"
  }
]