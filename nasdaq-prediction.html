<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Nasdaq Index Prediction and Investment Simulation – Luoning Zhang</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Luoning Zhang</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./publications-patents.html"> 
<span class="menu-text">Publications &amp; Patents</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./courseworks.html"> 
<span class="menu-text">Coursework</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a>
  <ul class="collapse">
  <li><a href="#dataset-nasdaq-index" id="toc-dataset-nasdaq-index" class="nav-link" data-scroll-target="#dataset-nasdaq-index">Dataset: Nasdaq Index</a></li>
  <li><a href="#objectives" id="toc-objectives" class="nav-link" data-scroll-target="#objectives">Objectives</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">1. Data Preparation</a>
  <ul class="collapse">
  <li><a href="#exploratory-data-analysis-and-data-preprocessing" id="toc-exploratory-data-analysis-and-data-preprocessing" class="nav-link" data-scroll-target="#exploratory-data-analysis-and-data-preprocessing">1.1 Exploratory data analysis and data preprocessing</a></li>
  <li><a href="#data-standardization-and-train-test-split" id="toc-data-standardization-and-train-test-split" class="nav-link" data-scroll-target="#data-standardization-and-train-test-split">1.2 Data standardization and train test split</a></li>
  <li><a href="#exploratory-data-visualization" id="toc-exploratory-data-visualization" class="nav-link" data-scroll-target="#exploratory-data-visualization">1.3 Exploratory data visualization</a></li>
  <li><a href="#baseline-model" id="toc-baseline-model" class="nav-link" data-scroll-target="#baseline-model">1.4 Baseline model</a></li>
  </ul></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">2. Linear Regression</a>
  <ul class="collapse">
  <li><a href="#linear-regression-with-all-the-features" id="toc-linear-regression-with-all-the-features" class="nav-link" data-scroll-target="#linear-regression-with-all-the-features">2.1 Linear regression with all the features</a></li>
  <li><a href="#feature-selection-for-linear-regression-recursive-feature-elimination-rfe" id="toc-feature-selection-for-linear-regression-recursive-feature-elimination-rfe" class="nav-link" data-scroll-target="#feature-selection-for-linear-regression-recursive-feature-elimination-rfe">2.2 Feature selection for linear regression: Recursive Feature Elimination (RFE)</a></li>
  </ul></li>
  <li><a href="#tree-based-models-for-regression" id="toc-tree-based-models-for-regression" class="nav-link" data-scroll-target="#tree-based-models-for-regression">3. Tree Based Models for Regression</a>
  <ul class="collapse">
  <li><a href="#decision-tree" id="toc-decision-tree" class="nav-link" data-scroll-target="#decision-tree">3.1 Decision Tree</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">3.2 Random Forest</a></li>
  <li><a href="#xgboost" id="toc-xgboost" class="nav-link" data-scroll-target="#xgboost">3.3 XGBoost</a></li>
  </ul></li>
  <li><a href="#neural-network-models-for-regression" id="toc-neural-network-models-for-regression" class="nav-link" data-scroll-target="#neural-network-models-for-regression">4. Neural Network Models for Regression</a>
  <ul class="collapse">
  <li><a href="#multilayer-perceptron" id="toc-multilayer-perceptron" class="nav-link" data-scroll-target="#multilayer-perceptron">4.1 Multilayer Perceptron</a></li>
  <li><a href="#long-short-term-memory-lstm-network" id="toc-long-short-term-memory-lstm-network" class="nav-link" data-scroll-target="#long-short-term-memory-lstm-network">4.2 Long Short Term Memory (LSTM) Network</a></li>
  <li><a href="#convolutional-neural-network-cnn" id="toc-convolutional-neural-network-cnn" class="nav-link" data-scroll-target="#convolutional-neural-network-cnn">4.3 Convolutional Neural Network (CNN)</a></li>
  <li><a href="#cnn-lstm" id="toc-cnn-lstm" class="nav-link" data-scroll-target="#cnn-lstm">4.4 CNN LSTM</a></li>
  </ul></li>
  <li><a href="#model-comparison-and-discussion" id="toc-model-comparison-and-discussion" class="nav-link" data-scroll-target="#model-comparison-and-discussion">5. Model Comparison and Discussion</a>
  <ul class="collapse">
  <li><a href="#comparison-of-rmse" id="toc-comparison-of-rmse" class="nav-link" data-scroll-target="#comparison-of-rmse">5.1 Comparison of RMSE</a></li>
  <li><a href="#predicting-nasdaq-index" id="toc-predicting-nasdaq-index" class="nav-link" data-scroll-target="#predicting-nasdaq-index">5.2 Predicting Nasdaq Index</a></li>
  <li><a href="#investment-simulation" id="toc-investment-simulation" class="nav-link" data-scroll-target="#investment-simulation">5.3 Investment Simulation</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">6. Conclusion</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Nasdaq Index Prediction and Investment Simulation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Math 178 Final Project</strong></p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<section id="dataset-nasdaq-index" class="level3">
<h3 class="anchored" data-anchor-id="dataset-nasdaq-index">Dataset: Nasdaq Index</h3>
<p>The dataset is from UCI Machine Learning Repository.</p>
<p>Link: https://archive.ics.uci.edu/dataset/554/cnnpred+cnn+based+stock+market+prediction+using+a+diverse+set+of+variables</p>
<p>“This dataset contains several daily features of S&amp;P 500, NASDAQ Composite, Dow Jones Industrial Average, RUSSELL 2000, and NYSE Composite from 2010 to 2017……It covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates.” [1]</p>
<p>For this project, we only use the “NASDAQ Composite” part of this dataset.</p>
</section>
<section id="objectives" class="level3">
<h3 class="anchored" data-anchor-id="objectives">Objectives</h3>
<p>The goal of this project is predicting the future Nasdaq Composite Index (COMP) using historical data by machine learning and deep learning models.</p>
<p>Particularly, we have two goals:</p>
<ol type="1">
<li>Predicting the value of Nasdaq Index of the next day given historical data.</li>
<li>Conducting an investment simulation by predicting increasing or decreasing trend of Nasdaq Index of the next day.</li>
</ol>
</section>
<section id="methods" class="level3">
<h3 class="anchored" data-anchor-id="methods">Methods</h3>
<p>First, we cleaned the data, did basic feature engineering, implemented exploratory data visulization, and made baseline models in section 1. Then, we used a variety of regression models to try to predict the values of future Nasdaq Index, respectively. Also, we implemented some feature selection methods and optimized the hyperparameters to improve the performance of the models.</p>
<ul>
<li>Regression Models (Section 2-4): Linear Regression, Tree Based Models (Decision Tree, Random Forest, XGBoost), MLP, LSTM, 1DCNN, CNN LSTM</li>
<li>Feature Selection Method: Recursive Feature Elimination (RFE)</li>
</ul>
<p>Finally, in section 5, we evaluated and compared the accuracy of the different models on the testing data and implement an investment simulation to evaluate the profitabilities of these models.</p>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>Because the noise of data is high and the test data is deviated from the training data, it is very hard to make precise predictions. The performance of all the models we trained are not very good, which are even worse than the baseline model. This result is expected. It reflects the complexity of the financial market. More advanced model and more significant features might be helpful to improve the prediction.</p>
</section>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">1. Data Preparation</h2>
<section id="exploratory-data-analysis-and-data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-analysis-and-data-preprocessing">1.1 Exploratory data analysis and data preprocessing</h3>
<div id="cell-3" class="cell" data-cell_id="e3c8a67216ef492bb8bda39c6f0e6479" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="17051" data-execution_start="1741122054307" data-source_hash="b22946f6" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> altair <span class="im">as</span> alt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> umap.umap_ <span class="im">import</span> UMAP</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFECV</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> TimeSeriesSplit</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential, load_model</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense, Dropout, LSTM, Conv1D, Flatten, MaxPooling1D</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> ReduceLROnPlateau, EarlyStopping</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Firstly, we take a look at the columns and the dataset.</p>
<div id="cell-5" class="cell" data-cell_id="ae154660a0ed4eaf8d8c9a0c1b25310d" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="306" data-execution_start="1741122077184" data-source_hash="5b1e8577" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>FILE_PATH <span class="op">=</span> <span class="st">"Processed_NASDAQ.csv"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(FILE_PATH)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the data.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Date        Close    Volume       mom      mom1      mom2  \
0     2009-12-31  2269.149902       NaN       NaN       NaN       NaN   
1     2010-01-04  2308.419922  0.560308  0.017306       NaN       NaN   
2     2010-01-05  2308.709961  0.225994  0.000126  0.017306       NaN   
3     2010-01-06  2301.090088 -0.048364 -0.003300  0.000126  0.017306   
4     2010-01-07  2300.050049  0.007416 -0.000452 -0.003300  0.000126   
...          ...          ...       ...       ...       ...       ...   
1979  2017-11-09  6750.049805  0.058830 -0.005755  0.003153 -0.002750   
1980  2017-11-10  6750.939941 -0.116863  0.000132 -0.005755  0.003153   
1981  2017-11-13  6757.600098 -0.000091  0.000987  0.000132 -0.005755   
1982  2017-11-14  6737.870117  0.005087 -0.002920  0.000987  0.000132   
1983  2017-11-15  6706.209961 -0.037891 -0.004699 -0.002920  0.000987   

          mom3     ROC_5    ROC_10    ROC_15  ...   NZD  silver-F  RUSSELL-F  \
0          NaN       NaN       NaN       NaN  ...  0.03      0.26      -1.08   
1          NaN       NaN       NaN       NaN  ...  1.52      3.26       1.61   
2          NaN       NaN       NaN       NaN  ... -0.07      1.96      -0.20   
3          NaN       NaN       NaN       NaN  ...  0.56      2.15      -0.02   
4     0.017306       NaN       NaN       NaN  ... -0.72      0.94       0.50   
...        ...       ...       ...       ...  ...   ...       ...        ...   
1979  0.003252  0.522862  2.947790  2.194980  ... -0.24     -0.62      -0.34   
1980 -0.002750 -0.199573  0.741356  1.838727  ... -0.27     -0.58      -0.20   
1981  0.003153 -0.424963  0.875362  2.592598  ... -0.38      0.72      -0.04   
1982 -0.005755 -0.441942  0.151616  2.113229  ... -0.39      0.17      -0.21   
1983  0.000132 -1.221221 -0.153648  2.168224  ...  0.03     -0.60      -0.46   

      S&amp;P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  
0     -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  
1      1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  
2      0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  
3      0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  
4      0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  
...     ...   ...             ...           ...      ...   ...   ...  
1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  
1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  
1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  
1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  
1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  

[1984 rows x 84 columns]</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-cell_id="52d460cc5bf0410bbe8534fa92c6c33a" data-deepnote_cell_type="code" data-execution_context_id="71d0e18f-cbd6-474d-b598-cfb519076d90" data-execution_millis="1" data-execution_start="1740861966932" data-source_hash="de1e323c" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1984 entries, 0 to 1983
Data columns (total 84 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   Date            1984 non-null   object 
 1   Close           1984 non-null   float64
 2   Volume          1983 non-null   float64
 3   mom             1983 non-null   float64
 4   mom1            1982 non-null   float64
 5   mom2            1981 non-null   float64
 6   mom3            1980 non-null   float64
 7   ROC_5           1979 non-null   float64
 8   ROC_10          1974 non-null   float64
 9   ROC_15          1969 non-null   float64
 10  ROC_20          1964 non-null   float64
 11  EMA_10          1975 non-null   float64
 12  EMA_20          1965 non-null   float64
 13  EMA_50          1935 non-null   float64
 14  EMA_200         1785 non-null   float64
 15  DTB4WK          1984 non-null   float64
 16  DTB3            1984 non-null   float64
 17  DTB6            1984 non-null   float64
 18  DGS5            1984 non-null   float64
 19  DGS10           1984 non-null   float64
 20  Oil             1983 non-null   float64
 21  Gold            1983 non-null   float64
 22  DAAA            1984 non-null   float64
 23  DBAA            1984 non-null   float64
 24  GBP             1506 non-null   float64
 25  JPY             1505 non-null   float64
 26  CAD             1501 non-null   float64
 27  CNY             1505 non-null   float64
 28  AAPL            1983 non-null   float64
 29  AMZN            1983 non-null   float64
 30  GE              1983 non-null   float64
 31  JNJ             1983 non-null   float64
 32  JPM             1983 non-null   float64
 33  MSFT            1983 non-null   float64
 34  WFC             1983 non-null   float64
 35  XOM             1983 non-null   float64
 36  FCHI            1944 non-null   float64
 37  FTSE            1913 non-null   float64
 38  GDAXI           1917 non-null   float64
 39  GSPC            1983 non-null   float64
 40  HSI             1814 non-null   float64
 41  DJI             1983 non-null   float64
 42  SSEC            1746 non-null   float64
 43  RUT             1983 non-null   float64
 44  NYSE            1983 non-null   float64
 45  TE1             1984 non-null   float64
 46  TE2             1984 non-null   float64
 47  TE3             1984 non-null   float64
 48  TE5             1984 non-null   float64
 49  TE6             1984 non-null   float64
 50  DE1             1984 non-null   float64
 51  DE2             1984 non-null   float64
 52  DE4             1984 non-null   float64
 53  DE5             1984 non-null   float64
 54  DE6             1984 non-null   float64
 55  CTB3M           1983 non-null   float64
 56  CTB6M           1983 non-null   float64
 57  CTB1Y           1983 non-null   float64
 58  Name            1984 non-null   object 
 59  AUD             1984 non-null   float64
 60  Brent           1984 non-null   float64
 61  CAC-F           1980 non-null   float64
 62  copper-F        1984 non-null   float64
 63  WIT-oil         1984 non-null   float64
 64  DAX-F           1980 non-null   float64
 65  DJI-F           1984 non-null   float64
 66  EUR             1984 non-null   float64
 67  FTSE-F          1978 non-null   float64
 68  gold-F          1984 non-null   float64
 69  HSI-F           1909 non-null   float64
 70  KOSPI-F         1905 non-null   float64
 71  NASDAQ-F        1984 non-null   float64
 72  GAS-F           1984 non-null   float64
 73  Nikkei-F        1984 non-null   float64
 74  NZD             1984 non-null   float64
 75  silver-F        1984 non-null   float64
 76  RUSSELL-F       1984 non-null   float64
 77  S&amp;P-F           1984 non-null   float64
 78  CHF             1984 non-null   float64
 79  Dollar index-F  1984 non-null   float64
 80  Dollar index    1984 non-null   float64
 81  wheat-F         1982 non-null   float64
 82  XAG             1984 non-null   float64
 83  XAU             1984 non-null   float64
dtypes: float64(82), object(2)
memory usage: 1.3+ MB</code></pre>
</div>
</div>
<p>As we can see, there are a lot of null values. We try to fill these null values to keep as many samples as possible. Considering the nature of time-series prediction, it is important to avoid leaking the future data when we fill these values. So, we don’t choose the methods filling backward or filling the mean values which involves using future values to fill the past. Instead, we utilize approaches including filling forward and dropping null values.</p>
<div id="cell-8" class="cell" data-cell_id="7c071fed8b83479dabacd9a97622d50a" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="0" data-execution_start="1741122233157" data-source_hash="671eebdc" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># EMA_200 contains too many null values.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropping these rows would lose a lot of data, and filling them could distort the data pattern.</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Therefore, we choose to drop this feature.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">'EMA_200'</span>], errors<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill forward missing values and then drop any remaining null values.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.ffill()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Date        Close    Volume       mom      mom1      mom2  \
49    2010-03-15  2362.209961 -0.067949 -0.002302 -0.000338  0.004031   
50    2010-03-16  2378.010010  0.125608  0.006689 -0.002302 -0.000338   
51    2010-03-17  2389.090088  0.036119  0.004659  0.006689 -0.002302   
52    2010-03-18  2391.280029 -0.047493  0.000917  0.004659  0.006689   
53    2010-03-19  2374.409912  0.406201 -0.007055  0.000917  0.004659   
...          ...          ...       ...       ...       ...       ...   
1979  2017-11-09  6750.049805  0.058830 -0.005755  0.003153 -0.002750   
1980  2017-11-10  6750.939941 -0.116863  0.000132 -0.005755  0.003153   
1981  2017-11-13  6757.600098 -0.000091  0.000987  0.000132 -0.005755   
1982  2017-11-14  6737.870117  0.005087 -0.002920  0.000987  0.000132   
1983  2017-11-15  6706.209961 -0.037891 -0.004699 -0.002920  0.000987   

          mom3     ROC_5    ROC_10    ROC_15  ...   NZD  silver-F  RUSSELL-F  \
49    0.007805  1.286334  3.898710  5.360318  ...  0.14     -0.47      -0.27   
50    0.004031  1.594839  4.262557  7.435037  ...  1.35      1.46       0.77   
51   -0.000338  1.277693  4.753414  6.851388  ...  0.21      0.98       0.49   
52   -0.002302  0.963498  4.317477  7.029749  ...  0.18     -0.54      -0.28   
53    0.006689  0.285092  2.065889  6.082846  ... -0.95     -2.24       0.54   
...        ...       ...       ...       ...  ...   ...       ...        ...   
1979  0.003252  0.522862  2.947790  2.194980  ... -0.24     -0.62      -0.34   
1980 -0.002750 -0.199573  0.741356  1.838727  ... -0.27     -0.58      -0.20   
1981  0.003153 -0.424963  0.875362  2.592598  ... -0.38      0.72      -0.04   
1982 -0.005755 -0.441942  0.151616  2.113229  ... -0.39      0.17      -0.21   
1983  0.000132 -1.221221 -0.153648  2.168224  ...  0.03     -0.60      -0.46   

      S&amp;P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  
49    -0.09  0.39            0.41          0.53    -1.34 -0.06  0.60  
50     0.80 -0.62           -0.21         -0.62     1.83  2.11  1.50  
51     0.54 -0.09           -0.19         -0.14     1.74  0.40 -0.06  
52     0.02  0.30            0.74          0.73    -1.31 -0.69  0.08  
53     0.60  0.38            0.66          0.62    -0.95 -2.36 -1.63  
...     ...   ...             ...           ...      ...   ...   ...  
1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  
1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  
1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  
1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  
1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  

[1935 rows x 83 columns]</code></pre>
</div>
</div>
<p><strong>Feature Processing and Feature Expansion</strong></p>
<ol type="1">
<li>Create three date-related features.</li>
<li>The ‘Close’ (Nasdaq Composite Index) and ‘EMA’ (Exponential Moving Average) features are numerical values, but most of the other features are rate of change (ROC). It doesn’t make sense to use ROC to predict a numerical value, especially for linear models. (For example, from 5000 to 6000, the ROC is 0.2. From 1000 to 2000, the ROC is 1.) Therefore, we converge the ‘Close’ and ‘EMA’ features to rate of change.</li>
<li>We insert the <code>t-1</code> to <code>t-k</code> days data to every <code>t</code> day’s row by creating new columns.</li>
<li>Sqaure all the features to create new features to enhance non-linearity capture and highlight volatility effects.</li>
</ol>
<div id="cell-10" class="cell" data-cell_id="8a3b92015b494bf4a403a3e62b556caf" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="344" data-execution_start="1741122461969" data-source_hash="4ac88604" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the 'Date' column to datetime format.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Date'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'Date'</span>])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new features:</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Day of the week (Monday=0, Sunday=6)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'day_of_week'</span>] <span class="op">=</span> df[<span class="st">'Date'</span>].dt.dayofweek</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Day of the month</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'day_of_month'</span>] <span class="op">=</span> df[<span class="st">'Date'</span>].dt.day</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Month as a number</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'month'</span>] <span class="op">=</span> df[<span class="st">'Date'</span>].dt.month</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the rate of change (as a decimal, not a percentage)</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Close_ROC'</span>] <span class="op">=</span> df[<span class="st">'Close'</span>].pct_change()</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'EMA10_ROC'</span>] <span class="op">=</span> df[<span class="st">'EMA_10'</span>].pct_change()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'EMA20_ROC'</span>] <span class="op">=</span> df[<span class="st">'EMA_20'</span>].pct_change()</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'EMA50_ROC'</span>] <span class="op">=</span> df[<span class="st">'EMA_50'</span>].pct_change()</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Save a copy of the original DataFrame.</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>df_raw <span class="op">=</span> df.copy()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop non-numeric columns.</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">'Date'</span>, <span class="st">'Name'</span>], errors<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the first row because the rate of change doesn't apply to it.</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the list of original columns (before adding lag features).</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>original_cols <span class="op">=</span> df.columns.tolist()</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of previous days to include.</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co"># For each lag (from 1 to k), create new columns by shifting the original columns and</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="co"># create squared features for each lagged column.</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> original_cols:</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        lagged_col <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_day-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        squared_col <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>lagged_col<span class="sc">}</span><span class="ss">^2'</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        df[lagged_col] <span class="op">=</span> df[col].shift(i)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        df[squared_col] <span class="op">=</span> df[lagged_col] <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with NaN values resulting from shifting.</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>            Close    Volume       mom      mom1      mom2      mom3     ROC_5  \
55    2415.239990  0.006985  0.008283  0.008840 -0.007055  0.000917  1.565594   
56    2398.760010 -0.011816 -0.006823  0.008283  0.008840 -0.007055  0.404753   
57    2397.409912  0.110328 -0.000563 -0.006823  0.008283  0.008840  0.256343   
58    2395.129883 -0.124568 -0.000951 -0.000563 -0.006823  0.008283  0.872637   
59    2404.360107 -0.168958  0.003854 -0.000951 -0.000563 -0.006823  0.374059   
...           ...       ...       ...       ...       ...       ...       ...   
1979  6750.049805  0.058830 -0.005755  0.003153 -0.002750  0.003252  0.522862   
1980  6750.939941 -0.116863  0.000132 -0.005755  0.003153 -0.002750 -0.199573   
1981  6757.600098 -0.000091  0.000987  0.000132 -0.005755  0.003153 -0.424963   
1982  6737.870117  0.005087 -0.002920  0.000987  0.000132 -0.005755 -0.441942   
1983  6706.209961 -0.037891 -0.004699 -0.002920  0.000987  0.000132 -1.221221   

        ROC_10    ROC_15    ROC_20  ...  month_day-5  month_day-5^2  \
55    3.185402  5.894885  9.117033  ...          3.0            9.0   
56    1.687618  5.177407  7.283873  ...          3.0            9.0   
57    1.222311  4.584888  7.304113  ...          3.0            9.0   
58    1.160216  2.956553  7.008563  ...          3.0            9.0   
59    1.784352  3.093639  5.752628  ...          3.0            9.0   
...        ...       ...       ...  ...          ...            ...   
1979  2.947790  2.194980  2.405216  ...         11.0          121.0   
1980  0.741356  1.838727  2.197162  ...         11.0          121.0   
1981  0.875362  2.592598  2.016910  ...         11.0          121.0   
1982  0.151616  2.113229  1.724273  ...         11.0          121.0   
1983 -0.153648  2.168224  1.237727  ...         11.0          121.0   

      Close_ROC_day-5  Close_ROC_day-5^2  EMA10_ROC_day-5  EMA10_ROC_day-5^2  \
55           0.006689       4.473834e-05         0.003607           0.000013   
56           0.004659       2.170992e-05         0.003802           0.000014   
57           0.000917       8.402331e-07         0.003268           0.000011   
58          -0.007055       4.977088e-05         0.001364           0.000002   
59           0.008840       7.814714e-05         0.002731           0.000007   
...               ...                ...              ...                ...   
1979        -0.000237       5.602981e-08         0.001613           0.000003   
1980         0.007372       5.434081e-05         0.002668           0.000007   
1981         0.003252       1.057747e-05         0.002775           0.000008   
1982        -0.002750       7.560430e-06         0.001758           0.000003   
1983         0.003153       9.942828e-06         0.002014           0.000004   

      EMA20_ROC_day-5  EMA20_ROC_day-5^2  EMA50_ROC_day-5  EMA50_ROC_day-5^2  
55           0.003339           0.000011         0.002181           0.000005  
56           0.003468           0.000012         0.002284           0.000005  
57           0.003217           0.000010         0.002227           0.000005  
58           0.002209           0.000005         0.001843           0.000003  
59           0.002854           0.000008         0.002130           0.000005  
...               ...                ...              ...                ...  
1979         0.001461           0.000002         0.001191           0.000001  
1980         0.002031           0.000004         0.001441           0.000002  
1981         0.002150           0.000005         0.001514           0.000002  
1982         0.001674           0.000003         0.001341           0.000002  
1983         0.001817           0.000003         0.001414           0.000002  

[1929 rows x 968 columns]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[lagged_col] = df[col].shift(i)
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3073063491.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[squared_col] = df[lagged_col] ** 2</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-cell_id="805d13f2994d4671a3949cea91952b38" data-deepnote_cell_type="code" data-deepnote_table_loading="false" data-quarto-private-1="{&quot;key&quot;:&quot;deepnote_table_state&quot;,&quot;value&quot;:{&quot;cellFormattingRules&quot;:[],&quot;columnDisplayNames&quot;:[],&quot;columnOrder&quot;:[&quot;Close&quot;,&quot;Volume&quot;,&quot;mom&quot;,&quot;mom1&quot;,&quot;mom2&quot;,&quot;mom3&quot;,&quot;ROC_5&quot;,&quot;ROC_10&quot;,&quot;ROC_15&quot;,&quot;ROC_20&quot;,&quot;EMA_10&quot;,&quot;EMA_20&quot;,&quot;EMA_50&quot;,&quot;DTB4WK&quot;,&quot;DTB3&quot;,&quot;DTB6&quot;,&quot;DGS5&quot;,&quot;DGS10&quot;,&quot;Oil&quot;,&quot;Gold&quot;,&quot;DAAA&quot;,&quot;DBAA&quot;,&quot;GBP&quot;,&quot;JPY&quot;,&quot;CAD&quot;,&quot;CNY&quot;,&quot;AAPL&quot;,&quot;AMZN&quot;,&quot;GE&quot;,&quot;JNJ&quot;,&quot;JPM&quot;,&quot;MSFT&quot;,&quot;WFC&quot;,&quot;XOM&quot;,&quot;FCHI&quot;,&quot;FTSE&quot;,&quot;GDAXI&quot;,&quot;GSPC&quot;,&quot;HSI&quot;,&quot;DJI&quot;,&quot;SSEC&quot;,&quot;RUT&quot;,&quot;NYSE&quot;,&quot;TE1&quot;,&quot;TE2&quot;,&quot;TE3&quot;,&quot;TE5&quot;,&quot;TE6&quot;,&quot;DE1&quot;,&quot;DE2&quot;,&quot;DE4&quot;,&quot;DE5&quot;,&quot;DE6&quot;,&quot;CTB3M&quot;,&quot;CTB6M&quot;,&quot;CTB1Y&quot;,&quot;AUD&quot;,&quot;Brent&quot;,&quot;CAC-F&quot;,&quot;copper-F&quot;,&quot;WIT-oil&quot;,&quot;DAX-F&quot;,&quot;DJI-F&quot;,&quot;EUR&quot;,&quot;FTSE-F&quot;,&quot;gold-F&quot;,&quot;HSI-F&quot;,&quot;KOSPI-F&quot;,&quot;NASDAQ-F&quot;,&quot;GAS-F&quot;,&quot;Nikkei-F&quot;,&quot;NZD&quot;,&quot;silver-F&quot;,&quot;RUSSELL-F&quot;,&quot;S&amp;P-F&quot;,&quot;CHF&quot;,&quot;Dollar index-F&quot;,&quot;Dollar index&quot;,&quot;wheat-F&quot;,&quot;XAG&quot;,&quot;XAU&quot;,&quot;day_of_week&quot;,&quot;day_of_month&quot;,&quot;month&quot;,&quot;Close_ROC&quot;,&quot;EMA10_ROC&quot;,&quot;EMA20_ROC&quot;,&quot;EMA50_ROC&quot;],&quot;conditionalFilters&quot;:[],&quot;filters&quot;:[],&quot;hiddenColumnIds&quot;:[],&quot;pageIndex&quot;:0,&quot;pageSize&quot;:10,&quot;sortBy&quot;:[],&quot;wrappedTextColumnIds&quot;:[]}}" data-execution_context_id="71d0e18f-cbd6-474d-b598-cfb519076d90" data-execution_millis="217" data-execution_start="1740861967600" data-source_hash="f804c160" data-execution_count="6">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Close</th>
<th data-quarto-table-cell-role="th">Volume</th>
<th data-quarto-table-cell-role="th">mom</th>
<th data-quarto-table-cell-role="th">mom1</th>
<th data-quarto-table-cell-role="th">mom2</th>
<th data-quarto-table-cell-role="th">mom3</th>
<th data-quarto-table-cell-role="th">ROC_5</th>
<th data-quarto-table-cell-role="th">ROC_10</th>
<th data-quarto-table-cell-role="th">ROC_15</th>
<th data-quarto-table-cell-role="th">ROC_20</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">month_day-5</th>
<th data-quarto-table-cell-role="th">month_day-5^2</th>
<th data-quarto-table-cell-role="th">Close_ROC_day-5</th>
<th data-quarto-table-cell-role="th">Close_ROC_day-5^2</th>
<th data-quarto-table-cell-role="th">EMA10_ROC_day-5</th>
<th data-quarto-table-cell-role="th">EMA10_ROC_day-5^2</th>
<th data-quarto-table-cell-role="th">EMA20_ROC_day-5</th>
<th data-quarto-table-cell-role="th">EMA20_ROC_day-5^2</th>
<th data-quarto-table-cell-role="th">EMA50_ROC_day-5</th>
<th data-quarto-table-cell-role="th">EMA50_ROC_day-5^2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">55</td>
<td>2415.239990</td>
<td>0.006985</td>
<td>0.008283</td>
<td>0.008840</td>
<td>-0.007055</td>
<td>0.000917</td>
<td>1.565594</td>
<td>3.185402</td>
<td>5.894885</td>
<td>9.117033</td>
<td>...</td>
<td>3.0</td>
<td>9.0</td>
<td>0.006689</td>
<td>4.473834e-05</td>
<td>0.003607</td>
<td>0.000013</td>
<td>0.003339</td>
<td>0.000011</td>
<td>0.002181</td>
<td>0.000005</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">56</td>
<td>2398.760010</td>
<td>-0.011816</td>
<td>-0.006823</td>
<td>0.008283</td>
<td>0.008840</td>
<td>-0.007055</td>
<td>0.404753</td>
<td>1.687618</td>
<td>5.177407</td>
<td>7.283873</td>
<td>...</td>
<td>3.0</td>
<td>9.0</td>
<td>0.004659</td>
<td>2.170992e-05</td>
<td>0.003802</td>
<td>0.000014</td>
<td>0.003468</td>
<td>0.000012</td>
<td>0.002284</td>
<td>0.000005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">57</td>
<td>2397.409912</td>
<td>0.110328</td>
<td>-0.000563</td>
<td>-0.006823</td>
<td>0.008283</td>
<td>0.008840</td>
<td>0.256343</td>
<td>1.222311</td>
<td>4.584888</td>
<td>7.304113</td>
<td>...</td>
<td>3.0</td>
<td>9.0</td>
<td>0.000917</td>
<td>8.402331e-07</td>
<td>0.003268</td>
<td>0.000011</td>
<td>0.003217</td>
<td>0.000010</td>
<td>0.002227</td>
<td>0.000005</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">58</td>
<td>2395.129883</td>
<td>-0.124568</td>
<td>-0.000951</td>
<td>-0.000563</td>
<td>-0.006823</td>
<td>0.008283</td>
<td>0.872637</td>
<td>1.160216</td>
<td>2.956553</td>
<td>7.008563</td>
<td>...</td>
<td>3.0</td>
<td>9.0</td>
<td>-0.007055</td>
<td>4.977088e-05</td>
<td>0.001364</td>
<td>0.000002</td>
<td>0.002209</td>
<td>0.000005</td>
<td>0.001843</td>
<td>0.000003</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">59</td>
<td>2404.360107</td>
<td>-0.168958</td>
<td>0.003854</td>
<td>-0.000951</td>
<td>-0.000563</td>
<td>-0.006823</td>
<td>0.374059</td>
<td>1.784352</td>
<td>3.093639</td>
<td>5.752628</td>
<td>...</td>
<td>3.0</td>
<td>9.0</td>
<td>0.008840</td>
<td>7.814714e-05</td>
<td>0.002731</td>
<td>0.000007</td>
<td>0.002854</td>
<td>0.000008</td>
<td>0.002130</td>
<td>0.000005</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1979</td>
<td>6750.049805</td>
<td>0.058830</td>
<td>-0.005755</td>
<td>0.003153</td>
<td>-0.002750</td>
<td>0.003252</td>
<td>0.522862</td>
<td>2.947790</td>
<td>2.194980</td>
<td>2.405216</td>
<td>...</td>
<td>11.0</td>
<td>121.0</td>
<td>-0.000237</td>
<td>5.602981e-08</td>
<td>0.001613</td>
<td>0.000003</td>
<td>0.001461</td>
<td>0.000002</td>
<td>0.001191</td>
<td>0.000001</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1980</td>
<td>6750.939941</td>
<td>-0.116863</td>
<td>0.000132</td>
<td>-0.005755</td>
<td>0.003153</td>
<td>-0.002750</td>
<td>-0.199573</td>
<td>0.741356</td>
<td>1.838727</td>
<td>2.197162</td>
<td>...</td>
<td>11.0</td>
<td>121.0</td>
<td>0.007372</td>
<td>5.434081e-05</td>
<td>0.002668</td>
<td>0.000007</td>
<td>0.002031</td>
<td>0.000004</td>
<td>0.001441</td>
<td>0.000002</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1981</td>
<td>6757.600098</td>
<td>-0.000091</td>
<td>0.000987</td>
<td>0.000132</td>
<td>-0.005755</td>
<td>0.003153</td>
<td>-0.424963</td>
<td>0.875362</td>
<td>2.592598</td>
<td>2.016910</td>
<td>...</td>
<td>11.0</td>
<td>121.0</td>
<td>0.003252</td>
<td>1.057747e-05</td>
<td>0.002775</td>
<td>0.000008</td>
<td>0.002150</td>
<td>0.000005</td>
<td>0.001514</td>
<td>0.000002</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1982</td>
<td>6737.870117</td>
<td>0.005087</td>
<td>-0.002920</td>
<td>0.000987</td>
<td>0.000132</td>
<td>-0.005755</td>
<td>-0.441942</td>
<td>0.151616</td>
<td>2.113229</td>
<td>1.724273</td>
<td>...</td>
<td>11.0</td>
<td>121.0</td>
<td>-0.002750</td>
<td>7.560430e-06</td>
<td>0.001758</td>
<td>0.000003</td>
<td>0.001674</td>
<td>0.000003</td>
<td>0.001341</td>
<td>0.000002</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1983</td>
<td>6706.209961</td>
<td>-0.037891</td>
<td>-0.004699</td>
<td>-0.002920</td>
<td>0.000987</td>
<td>0.000132</td>
<td>-1.221221</td>
<td>-0.153648</td>
<td>2.168224</td>
<td>1.237727</td>
<td>...</td>
<td>11.0</td>
<td>121.0</td>
<td>0.003153</td>
<td>9.942828e-06</td>
<td>0.002014</td>
<td>0.000004</td>
<td>0.001817</td>
<td>0.000003</td>
<td>0.001414</td>
<td>0.000002</td>
</tr>
</tbody>
</table>

<p>1929 rows × 968 columns</p>
</div>
</div>
</div>
</section>
<section id="data-standardization-and-train-test-split" class="level3">
<h3 class="anchored" data-anchor-id="data-standardization-and-train-test-split">1.2 Data standardization and train test split</h3>
<p>We use the historical data to predict the future data. Therefore, we use <code>lagged_cols</code> as predictors.</p>
<div id="cell-13" class="cell" data-cell_id="3428c917f83046efb13984c3b9263271" data-deepnote_app_block_visible="false" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="16" data-execution_start="1741122632357" data-source_hash="284e4008" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out the lagged columns.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lagged_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'_day-'</span> <span class="kw">in</span> col]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input Features:"</span>, lagged_cols)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define target variable.</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>target_var_regression <span class="op">=</span> <span class="st">'Close_ROC'</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define X and y.</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[lagged_cols]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[target_var_regression]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize features using StandardScaler.</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> pd.DataFrame(</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    scaler.fit_transform(X),</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>X.columns,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>X.index</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets.</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>split_ratio <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># 80% train data, 20% test data</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>split_index <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(df) <span class="op">*</span> split_ratio)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_scaled.iloc[:split_index]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_scaled.iloc[split_index:]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y.iloc[:split_index]</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y.iloc[split_index:]</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input Features: ['Close_day-1', 'Close_day-1^2', 'Volume_day-1', 'Volume_day-1^2', 'mom_day-1', 'mom_day-1^2', 'mom1_day-1', 'mom1_day-1^2', 'mom2_day-1', 'mom2_day-1^2', 'mom3_day-1', 'mom3_day-1^2', 'ROC_5_day-1', 'ROC_5_day-1^2', 'ROC_10_day-1', 'ROC_10_day-1^2', 'ROC_15_day-1', 'ROC_15_day-1^2', 'ROC_20_day-1', 'ROC_20_day-1^2', 'EMA_10_day-1', 'EMA_10_day-1^2', 'EMA_20_day-1', 'EMA_20_day-1^2', 'EMA_50_day-1', 'EMA_50_day-1^2', 'DTB4WK_day-1', 'DTB4WK_day-1^2', 'DTB3_day-1', 'DTB3_day-1^2', 'DTB6_day-1', 'DTB6_day-1^2', 'DGS5_day-1', 'DGS5_day-1^2', 'DGS10_day-1', 'DGS10_day-1^2', 'Oil_day-1', 'Oil_day-1^2', 'Gold_day-1', 'Gold_day-1^2', 'DAAA_day-1', 'DAAA_day-1^2', 'DBAA_day-1', 'DBAA_day-1^2', 'GBP_day-1', 'GBP_day-1^2', 'JPY_day-1', 'JPY_day-1^2', 'CAD_day-1', 'CAD_day-1^2', 'CNY_day-1', 'CNY_day-1^2', 'AAPL_day-1', 'AAPL_day-1^2', 'AMZN_day-1', 'AMZN_day-1^2', 'GE_day-1', 'GE_day-1^2', 'JNJ_day-1', 'JNJ_day-1^2', 'JPM_day-1', 'JPM_day-1^2', 'MSFT_day-1', 'MSFT_day-1^2', 'WFC_day-1', 'WFC_day-1^2', 'XOM_day-1', 'XOM_day-1^2', 'FCHI_day-1', 'FCHI_day-1^2', 'FTSE_day-1', 'FTSE_day-1^2', 'GDAXI_day-1', 'GDAXI_day-1^2', 'GSPC_day-1', 'GSPC_day-1^2', 'HSI_day-1', 'HSI_day-1^2', 'DJI_day-1', 'DJI_day-1^2', 'SSEC_day-1', 'SSEC_day-1^2', 'RUT_day-1', 'RUT_day-1^2', 'NYSE_day-1', 'NYSE_day-1^2', 'TE1_day-1', 'TE1_day-1^2', 'TE2_day-1', 'TE2_day-1^2', 'TE3_day-1', 'TE3_day-1^2', 'TE5_day-1', 'TE5_day-1^2', 'TE6_day-1', 'TE6_day-1^2', 'DE1_day-1', 'DE1_day-1^2', 'DE2_day-1', 'DE2_day-1^2', 'DE4_day-1', 'DE4_day-1^2', 'DE5_day-1', 'DE5_day-1^2', 'DE6_day-1', 'DE6_day-1^2', 'CTB3M_day-1', 'CTB3M_day-1^2', 'CTB6M_day-1', 'CTB6M_day-1^2', 'CTB1Y_day-1', 'CTB1Y_day-1^2', 'AUD_day-1', 'AUD_day-1^2', 'Brent_day-1', 'Brent_day-1^2', 'CAC-F_day-1', 'CAC-F_day-1^2', 'copper-F_day-1', 'copper-F_day-1^2', 'WIT-oil_day-1', 'WIT-oil_day-1^2', 'DAX-F_day-1', 'DAX-F_day-1^2', 'DJI-F_day-1', 'DJI-F_day-1^2', 'EUR_day-1', 'EUR_day-1^2', 'FTSE-F_day-1', 'FTSE-F_day-1^2', 'gold-F_day-1', 'gold-F_day-1^2', 'HSI-F_day-1', 'HSI-F_day-1^2', 'KOSPI-F_day-1', 'KOSPI-F_day-1^2', 'NASDAQ-F_day-1', 'NASDAQ-F_day-1^2', 'GAS-F_day-1', 'GAS-F_day-1^2', 'Nikkei-F_day-1', 'Nikkei-F_day-1^2', 'NZD_day-1', 'NZD_day-1^2', 'silver-F_day-1', 'silver-F_day-1^2', 'RUSSELL-F_day-1', 'RUSSELL-F_day-1^2', 'S&amp;P-F_day-1', 'S&amp;P-F_day-1^2', 'CHF_day-1', 'CHF_day-1^2', 'Dollar index-F_day-1', 'Dollar index-F_day-1^2', 'Dollar index_day-1', 'Dollar index_day-1^2', 'wheat-F_day-1', 'wheat-F_day-1^2', 'XAG_day-1', 'XAG_day-1^2', 'XAU_day-1', 'XAU_day-1^2', 'day_of_week_day-1', 'day_of_week_day-1^2', 'day_of_month_day-1', 'day_of_month_day-1^2', 'month_day-1', 'month_day-1^2', 'Close_ROC_day-1', 'Close_ROC_day-1^2', 'EMA10_ROC_day-1', 'EMA10_ROC_day-1^2', 'EMA20_ROC_day-1', 'EMA20_ROC_day-1^2', 'EMA50_ROC_day-1', 'EMA50_ROC_day-1^2', 'Close_day-2', 'Close_day-2^2', 'Volume_day-2', 'Volume_day-2^2', 'mom_day-2', 'mom_day-2^2', 'mom1_day-2', 'mom1_day-2^2', 'mom2_day-2', 'mom2_day-2^2', 'mom3_day-2', 'mom3_day-2^2', 'ROC_5_day-2', 'ROC_5_day-2^2', 'ROC_10_day-2', 'ROC_10_day-2^2', 'ROC_15_day-2', 'ROC_15_day-2^2', 'ROC_20_day-2', 'ROC_20_day-2^2', 'EMA_10_day-2', 'EMA_10_day-2^2', 'EMA_20_day-2', 'EMA_20_day-2^2', 'EMA_50_day-2', 'EMA_50_day-2^2', 'DTB4WK_day-2', 'DTB4WK_day-2^2', 'DTB3_day-2', 'DTB3_day-2^2', 'DTB6_day-2', 'DTB6_day-2^2', 'DGS5_day-2', 'DGS5_day-2^2', 'DGS10_day-2', 'DGS10_day-2^2', 'Oil_day-2', 'Oil_day-2^2', 'Gold_day-2', 'Gold_day-2^2', 'DAAA_day-2', 'DAAA_day-2^2', 'DBAA_day-2', 'DBAA_day-2^2', 'GBP_day-2', 'GBP_day-2^2', 'JPY_day-2', 'JPY_day-2^2', 'CAD_day-2', 'CAD_day-2^2', 'CNY_day-2', 'CNY_day-2^2', 'AAPL_day-2', 'AAPL_day-2^2', 'AMZN_day-2', 'AMZN_day-2^2', 'GE_day-2', 'GE_day-2^2', 'JNJ_day-2', 'JNJ_day-2^2', 'JPM_day-2', 'JPM_day-2^2', 'MSFT_day-2', 'MSFT_day-2^2', 'WFC_day-2', 'WFC_day-2^2', 'XOM_day-2', 'XOM_day-2^2', 'FCHI_day-2', 'FCHI_day-2^2', 'FTSE_day-2', 'FTSE_day-2^2', 'GDAXI_day-2', 'GDAXI_day-2^2', 'GSPC_day-2', 'GSPC_day-2^2', 'HSI_day-2', 'HSI_day-2^2', 'DJI_day-2', 'DJI_day-2^2', 'SSEC_day-2', 'SSEC_day-2^2', 'RUT_day-2', 'RUT_day-2^2', 'NYSE_day-2', 'NYSE_day-2^2', 'TE1_day-2', 'TE1_day-2^2', 'TE2_day-2', 'TE2_day-2^2', 'TE3_day-2', 'TE3_day-2^2', 'TE5_day-2', 'TE5_day-2^2', 'TE6_day-2', 'TE6_day-2^2', 'DE1_day-2', 'DE1_day-2^2', 'DE2_day-2', 'DE2_day-2^2', 'DE4_day-2', 'DE4_day-2^2', 'DE5_day-2', 'DE5_day-2^2', 'DE6_day-2', 'DE6_day-2^2', 'CTB3M_day-2', 'CTB3M_day-2^2', 'CTB6M_day-2', 'CTB6M_day-2^2', 'CTB1Y_day-2', 'CTB1Y_day-2^2', 'AUD_day-2', 'AUD_day-2^2', 'Brent_day-2', 'Brent_day-2^2', 'CAC-F_day-2', 'CAC-F_day-2^2', 'copper-F_day-2', 'copper-F_day-2^2', 'WIT-oil_day-2', 'WIT-oil_day-2^2', 'DAX-F_day-2', 'DAX-F_day-2^2', 'DJI-F_day-2', 'DJI-F_day-2^2', 'EUR_day-2', 'EUR_day-2^2', 'FTSE-F_day-2', 'FTSE-F_day-2^2', 'gold-F_day-2', 'gold-F_day-2^2', 'HSI-F_day-2', 'HSI-F_day-2^2', 'KOSPI-F_day-2', 'KOSPI-F_day-2^2', 'NASDAQ-F_day-2', 'NASDAQ-F_day-2^2', 'GAS-F_day-2', 'GAS-F_day-2^2', 'Nikkei-F_day-2', 'Nikkei-F_day-2^2', 'NZD_day-2', 'NZD_day-2^2', 'silver-F_day-2', 'silver-F_day-2^2', 'RUSSELL-F_day-2', 'RUSSELL-F_day-2^2', 'S&amp;P-F_day-2', 'S&amp;P-F_day-2^2', 'CHF_day-2', 'CHF_day-2^2', 'Dollar index-F_day-2', 'Dollar index-F_day-2^2', 'Dollar index_day-2', 'Dollar index_day-2^2', 'wheat-F_day-2', 'wheat-F_day-2^2', 'XAG_day-2', 'XAG_day-2^2', 'XAU_day-2', 'XAU_day-2^2', 'day_of_week_day-2', 'day_of_week_day-2^2', 'day_of_month_day-2', 'day_of_month_day-2^2', 'month_day-2', 'month_day-2^2', 'Close_ROC_day-2', 'Close_ROC_day-2^2', 'EMA10_ROC_day-2', 'EMA10_ROC_day-2^2', 'EMA20_ROC_day-2', 'EMA20_ROC_day-2^2', 'EMA50_ROC_day-2', 'EMA50_ROC_day-2^2', 'Close_day-3', 'Close_day-3^2', 'Volume_day-3', 'Volume_day-3^2', 'mom_day-3', 'mom_day-3^2', 'mom1_day-3', 'mom1_day-3^2', 'mom2_day-3', 'mom2_day-3^2', 'mom3_day-3', 'mom3_day-3^2', 'ROC_5_day-3', 'ROC_5_day-3^2', 'ROC_10_day-3', 'ROC_10_day-3^2', 'ROC_15_day-3', 'ROC_15_day-3^2', 'ROC_20_day-3', 'ROC_20_day-3^2', 'EMA_10_day-3', 'EMA_10_day-3^2', 'EMA_20_day-3', 'EMA_20_day-3^2', 'EMA_50_day-3', 'EMA_50_day-3^2', 'DTB4WK_day-3', 'DTB4WK_day-3^2', 'DTB3_day-3', 'DTB3_day-3^2', 'DTB6_day-3', 'DTB6_day-3^2', 'DGS5_day-3', 'DGS5_day-3^2', 'DGS10_day-3', 'DGS10_day-3^2', 'Oil_day-3', 'Oil_day-3^2', 'Gold_day-3', 'Gold_day-3^2', 'DAAA_day-3', 'DAAA_day-3^2', 'DBAA_day-3', 'DBAA_day-3^2', 'GBP_day-3', 'GBP_day-3^2', 'JPY_day-3', 'JPY_day-3^2', 'CAD_day-3', 'CAD_day-3^2', 'CNY_day-3', 'CNY_day-3^2', 'AAPL_day-3', 'AAPL_day-3^2', 'AMZN_day-3', 'AMZN_day-3^2', 'GE_day-3', 'GE_day-3^2', 'JNJ_day-3', 'JNJ_day-3^2', 'JPM_day-3', 'JPM_day-3^2', 'MSFT_day-3', 'MSFT_day-3^2', 'WFC_day-3', 'WFC_day-3^2', 'XOM_day-3', 'XOM_day-3^2', 'FCHI_day-3', 'FCHI_day-3^2', 'FTSE_day-3', 'FTSE_day-3^2', 'GDAXI_day-3', 'GDAXI_day-3^2', 'GSPC_day-3', 'GSPC_day-3^2', 'HSI_day-3', 'HSI_day-3^2', 'DJI_day-3', 'DJI_day-3^2', 'SSEC_day-3', 'SSEC_day-3^2', 'RUT_day-3', 'RUT_day-3^2', 'NYSE_day-3', 'NYSE_day-3^2', 'TE1_day-3', 'TE1_day-3^2', 'TE2_day-3', 'TE2_day-3^2', 'TE3_day-3', 'TE3_day-3^2', 'TE5_day-3', 'TE5_day-3^2', 'TE6_day-3', 'TE6_day-3^2', 'DE1_day-3', 'DE1_day-3^2', 'DE2_day-3', 'DE2_day-3^2', 'DE4_day-3', 'DE4_day-3^2', 'DE5_day-3', 'DE5_day-3^2', 'DE6_day-3', 'DE6_day-3^2', 'CTB3M_day-3', 'CTB3M_day-3^2', 'CTB6M_day-3', 'CTB6M_day-3^2', 'CTB1Y_day-3', 'CTB1Y_day-3^2', 'AUD_day-3', 'AUD_day-3^2', 'Brent_day-3', 'Brent_day-3^2', 'CAC-F_day-3', 'CAC-F_day-3^2', 'copper-F_day-3', 'copper-F_day-3^2', 'WIT-oil_day-3', 'WIT-oil_day-3^2', 'DAX-F_day-3', 'DAX-F_day-3^2', 'DJI-F_day-3', 'DJI-F_day-3^2', 'EUR_day-3', 'EUR_day-3^2', 'FTSE-F_day-3', 'FTSE-F_day-3^2', 'gold-F_day-3', 'gold-F_day-3^2', 'HSI-F_day-3', 'HSI-F_day-3^2', 'KOSPI-F_day-3', 'KOSPI-F_day-3^2', 'NASDAQ-F_day-3', 'NASDAQ-F_day-3^2', 'GAS-F_day-3', 'GAS-F_day-3^2', 'Nikkei-F_day-3', 'Nikkei-F_day-3^2', 'NZD_day-3', 'NZD_day-3^2', 'silver-F_day-3', 'silver-F_day-3^2', 'RUSSELL-F_day-3', 'RUSSELL-F_day-3^2', 'S&amp;P-F_day-3', 'S&amp;P-F_day-3^2', 'CHF_day-3', 'CHF_day-3^2', 'Dollar index-F_day-3', 'Dollar index-F_day-3^2', 'Dollar index_day-3', 'Dollar index_day-3^2', 'wheat-F_day-3', 'wheat-F_day-3^2', 'XAG_day-3', 'XAG_day-3^2', 'XAU_day-3', 'XAU_day-3^2', 'day_of_week_day-3', 'day_of_week_day-3^2', 'day_of_month_day-3', 'day_of_month_day-3^2', 'month_day-3', 'month_day-3^2', 'Close_ROC_day-3', 'Close_ROC_day-3^2', 'EMA10_ROC_day-3', 'EMA10_ROC_day-3^2', 'EMA20_ROC_day-3', 'EMA20_ROC_day-3^2', 'EMA50_ROC_day-3', 'EMA50_ROC_day-3^2', 'Close_day-4', 'Close_day-4^2', 'Volume_day-4', 'Volume_day-4^2', 'mom_day-4', 'mom_day-4^2', 'mom1_day-4', 'mom1_day-4^2', 'mom2_day-4', 'mom2_day-4^2', 'mom3_day-4', 'mom3_day-4^2', 'ROC_5_day-4', 'ROC_5_day-4^2', 'ROC_10_day-4', 'ROC_10_day-4^2', 'ROC_15_day-4', 'ROC_15_day-4^2', 'ROC_20_day-4', 'ROC_20_day-4^2', 'EMA_10_day-4', 'EMA_10_day-4^2', 'EMA_20_day-4', 'EMA_20_day-4^2', 'EMA_50_day-4', 'EMA_50_day-4^2', 'DTB4WK_day-4', 'DTB4WK_day-4^2', 'DTB3_day-4', 'DTB3_day-4^2', 'DTB6_day-4', 'DTB6_day-4^2', 'DGS5_day-4', 'DGS5_day-4^2', 'DGS10_day-4', 'DGS10_day-4^2', 'Oil_day-4', 'Oil_day-4^2', 'Gold_day-4', 'Gold_day-4^2', 'DAAA_day-4', 'DAAA_day-4^2', 'DBAA_day-4', 'DBAA_day-4^2', 'GBP_day-4', 'GBP_day-4^2', 'JPY_day-4', 'JPY_day-4^2', 'CAD_day-4', 'CAD_day-4^2', 'CNY_day-4', 'CNY_day-4^2', 'AAPL_day-4', 'AAPL_day-4^2', 'AMZN_day-4', 'AMZN_day-4^2', 'GE_day-4', 'GE_day-4^2', 'JNJ_day-4', 'JNJ_day-4^2', 'JPM_day-4', 'JPM_day-4^2', 'MSFT_day-4', 'MSFT_day-4^2', 'WFC_day-4', 'WFC_day-4^2', 'XOM_day-4', 'XOM_day-4^2', 'FCHI_day-4', 'FCHI_day-4^2', 'FTSE_day-4', 'FTSE_day-4^2', 'GDAXI_day-4', 'GDAXI_day-4^2', 'GSPC_day-4', 'GSPC_day-4^2', 'HSI_day-4', 'HSI_day-4^2', 'DJI_day-4', 'DJI_day-4^2', 'SSEC_day-4', 'SSEC_day-4^2', 'RUT_day-4', 'RUT_day-4^2', 'NYSE_day-4', 'NYSE_day-4^2', 'TE1_day-4', 'TE1_day-4^2', 'TE2_day-4', 'TE2_day-4^2', 'TE3_day-4', 'TE3_day-4^2', 'TE5_day-4', 'TE5_day-4^2', 'TE6_day-4', 'TE6_day-4^2', 'DE1_day-4', 'DE1_day-4^2', 'DE2_day-4', 'DE2_day-4^2', 'DE4_day-4', 'DE4_day-4^2', 'DE5_day-4', 'DE5_day-4^2', 'DE6_day-4', 'DE6_day-4^2', 'CTB3M_day-4', 'CTB3M_day-4^2', 'CTB6M_day-4', 'CTB6M_day-4^2', 'CTB1Y_day-4', 'CTB1Y_day-4^2', 'AUD_day-4', 'AUD_day-4^2', 'Brent_day-4', 'Brent_day-4^2', 'CAC-F_day-4', 'CAC-F_day-4^2', 'copper-F_day-4', 'copper-F_day-4^2', 'WIT-oil_day-4', 'WIT-oil_day-4^2', 'DAX-F_day-4', 'DAX-F_day-4^2', 'DJI-F_day-4', 'DJI-F_day-4^2', 'EUR_day-4', 'EUR_day-4^2', 'FTSE-F_day-4', 'FTSE-F_day-4^2', 'gold-F_day-4', 'gold-F_day-4^2', 'HSI-F_day-4', 'HSI-F_day-4^2', 'KOSPI-F_day-4', 'KOSPI-F_day-4^2', 'NASDAQ-F_day-4', 'NASDAQ-F_day-4^2', 'GAS-F_day-4', 'GAS-F_day-4^2', 'Nikkei-F_day-4', 'Nikkei-F_day-4^2', 'NZD_day-4', 'NZD_day-4^2', 'silver-F_day-4', 'silver-F_day-4^2', 'RUSSELL-F_day-4', 'RUSSELL-F_day-4^2', 'S&amp;P-F_day-4', 'S&amp;P-F_day-4^2', 'CHF_day-4', 'CHF_day-4^2', 'Dollar index-F_day-4', 'Dollar index-F_day-4^2', 'Dollar index_day-4', 'Dollar index_day-4^2', 'wheat-F_day-4', 'wheat-F_day-4^2', 'XAG_day-4', 'XAG_day-4^2', 'XAU_day-4', 'XAU_day-4^2', 'day_of_week_day-4', 'day_of_week_day-4^2', 'day_of_month_day-4', 'day_of_month_day-4^2', 'month_day-4', 'month_day-4^2', 'Close_ROC_day-4', 'Close_ROC_day-4^2', 'EMA10_ROC_day-4', 'EMA10_ROC_day-4^2', 'EMA20_ROC_day-4', 'EMA20_ROC_day-4^2', 'EMA50_ROC_day-4', 'EMA50_ROC_day-4^2', 'Close_day-5', 'Close_day-5^2', 'Volume_day-5', 'Volume_day-5^2', 'mom_day-5', 'mom_day-5^2', 'mom1_day-5', 'mom1_day-5^2', 'mom2_day-5', 'mom2_day-5^2', 'mom3_day-5', 'mom3_day-5^2', 'ROC_5_day-5', 'ROC_5_day-5^2', 'ROC_10_day-5', 'ROC_10_day-5^2', 'ROC_15_day-5', 'ROC_15_day-5^2', 'ROC_20_day-5', 'ROC_20_day-5^2', 'EMA_10_day-5', 'EMA_10_day-5^2', 'EMA_20_day-5', 'EMA_20_day-5^2', 'EMA_50_day-5', 'EMA_50_day-5^2', 'DTB4WK_day-5', 'DTB4WK_day-5^2', 'DTB3_day-5', 'DTB3_day-5^2', 'DTB6_day-5', 'DTB6_day-5^2', 'DGS5_day-5', 'DGS5_day-5^2', 'DGS10_day-5', 'DGS10_day-5^2', 'Oil_day-5', 'Oil_day-5^2', 'Gold_day-5', 'Gold_day-5^2', 'DAAA_day-5', 'DAAA_day-5^2', 'DBAA_day-5', 'DBAA_day-5^2', 'GBP_day-5', 'GBP_day-5^2', 'JPY_day-5', 'JPY_day-5^2', 'CAD_day-5', 'CAD_day-5^2', 'CNY_day-5', 'CNY_day-5^2', 'AAPL_day-5', 'AAPL_day-5^2', 'AMZN_day-5', 'AMZN_day-5^2', 'GE_day-5', 'GE_day-5^2', 'JNJ_day-5', 'JNJ_day-5^2', 'JPM_day-5', 'JPM_day-5^2', 'MSFT_day-5', 'MSFT_day-5^2', 'WFC_day-5', 'WFC_day-5^2', 'XOM_day-5', 'XOM_day-5^2', 'FCHI_day-5', 'FCHI_day-5^2', 'FTSE_day-5', 'FTSE_day-5^2', 'GDAXI_day-5', 'GDAXI_day-5^2', 'GSPC_day-5', 'GSPC_day-5^2', 'HSI_day-5', 'HSI_day-5^2', 'DJI_day-5', 'DJI_day-5^2', 'SSEC_day-5', 'SSEC_day-5^2', 'RUT_day-5', 'RUT_day-5^2', 'NYSE_day-5', 'NYSE_day-5^2', 'TE1_day-5', 'TE1_day-5^2', 'TE2_day-5', 'TE2_day-5^2', 'TE3_day-5', 'TE3_day-5^2', 'TE5_day-5', 'TE5_day-5^2', 'TE6_day-5', 'TE6_day-5^2', 'DE1_day-5', 'DE1_day-5^2', 'DE2_day-5', 'DE2_day-5^2', 'DE4_day-5', 'DE4_day-5^2', 'DE5_day-5', 'DE5_day-5^2', 'DE6_day-5', 'DE6_day-5^2', 'CTB3M_day-5', 'CTB3M_day-5^2', 'CTB6M_day-5', 'CTB6M_day-5^2', 'CTB1Y_day-5', 'CTB1Y_day-5^2', 'AUD_day-5', 'AUD_day-5^2', 'Brent_day-5', 'Brent_day-5^2', 'CAC-F_day-5', 'CAC-F_day-5^2', 'copper-F_day-5', 'copper-F_day-5^2', 'WIT-oil_day-5', 'WIT-oil_day-5^2', 'DAX-F_day-5', 'DAX-F_day-5^2', 'DJI-F_day-5', 'DJI-F_day-5^2', 'EUR_day-5', 'EUR_day-5^2', 'FTSE-F_day-5', 'FTSE-F_day-5^2', 'gold-F_day-5', 'gold-F_day-5^2', 'HSI-F_day-5', 'HSI-F_day-5^2', 'KOSPI-F_day-5', 'KOSPI-F_day-5^2', 'NASDAQ-F_day-5', 'NASDAQ-F_day-5^2', 'GAS-F_day-5', 'GAS-F_day-5^2', 'Nikkei-F_day-5', 'Nikkei-F_day-5^2', 'NZD_day-5', 'NZD_day-5^2', 'silver-F_day-5', 'silver-F_day-5^2', 'RUSSELL-F_day-5', 'RUSSELL-F_day-5^2', 'S&amp;P-F_day-5', 'S&amp;P-F_day-5^2', 'CHF_day-5', 'CHF_day-5^2', 'Dollar index-F_day-5', 'Dollar index-F_day-5^2', 'Dollar index_day-5', 'Dollar index_day-5^2', 'wheat-F_day-5', 'wheat-F_day-5^2', 'XAG_day-5', 'XAG_day-5^2', 'XAU_day-5', 'XAU_day-5^2', 'day_of_week_day-5', 'day_of_week_day-5^2', 'day_of_month_day-5', 'day_of_month_day-5^2', 'month_day-5', 'month_day-5^2', 'Close_ROC_day-5', 'Close_ROC_day-5^2', 'EMA10_ROC_day-5', 'EMA10_ROC_day-5^2', 'EMA20_ROC_day-5', 'EMA20_ROC_day-5^2', 'EMA50_ROC_day-5', 'EMA50_ROC_day-5^2']
      Close_day-1  Close_day-1^2  Volume_day-1  Volume_day-1^2  mom_day-1  \
55      -1.316708      -1.145535     -0.878617       -0.022459   0.779534   
56      -1.300546      -1.136318     -0.048001       -0.062368   0.726812   
57      -1.313971      -1.143979     -0.123872       -0.062276  -0.701704   
58      -1.315071      -1.144605      0.369029       -0.050116  -0.109669   
59      -1.316928      -1.145660     -0.578869       -0.046736  -0.146381   
...           ...            ...           ...             ...        ...   
1593     0.622093       0.502464      0.325335       -0.052412  -0.645461   
1594     0.656495       0.541583     -0.918432       -0.018396   0.779839   
1595     0.612203       0.491282      0.146735       -0.059333  -1.123691   
1596     0.581589       0.456847     -0.144396       -0.062128  -0.802541   
1597     0.574624       0.449051     -0.205371       -0.061382  -0.227548   

      mom_day-1^2  mom1_day-1  mom1_day-1^2  mom2_day-1  mom2_day-1^2  ...  \
55      -0.130427   -0.723318     -0.239273    0.030443     -0.426815  ...   
56      -0.167015    0.779653     -0.130513   -0.723315     -0.239273  ...   
57      -0.251500    0.726937     -0.167101    0.779657     -0.130512  ...   
58      -0.428728   -0.701426     -0.251588    0.726941     -0.167101  ...   
59      -0.426476   -0.109455     -0.428822   -0.701423     -0.251588  ...   
...           ...         ...           ...         ...           ...  ...   
1593    -0.281251   -1.181046      0.112329   -0.542522     -0.328662  ...   
1594    -0.130209   -0.645190     -0.281341   -1.181042      0.112330  ...   
1595     0.058214    0.779958     -0.130294   -0.645186     -0.281340  ...   
1596    -0.191370   -1.123368      0.058133    0.779961     -0.130294  ...   
1597    -0.417395   -0.802253     -0.191457   -1.123365      0.058134  ...   

      month_day-5  month_day-5^2  Close_ROC_day-5  Close_ROC_day-5^2  \
55      -1.069114      -1.012034         0.575377          -0.258634   
56      -1.069114      -1.012034         0.383501          -0.346900   
57      -1.069114      -1.012034         0.029610          -0.426891   
58      -1.069114      -1.012034        -0.724125          -0.239345   
59      -1.069114      -1.012034         0.778801          -0.130582   
...           ...            ...              ...                ...   
1593    -0.770184      -0.856133        -0.258263          -0.412757   
1594    -0.770184      -0.856133        -0.202110          -0.421092   
1595    -0.770184      -0.856133        -0.543337          -0.328736   
1596    -0.770184      -0.856133        -1.181839           0.112266   
1597    -0.770184      -0.856133        -0.645999          -0.281413   

      EMA10_ROC_day-5  EMA10_ROC_day-5^2  EMA20_ROC_day-5  EMA20_ROC_day-5^2  \
55           0.993332           0.155614         1.348414           0.789463   
56           1.056562           0.224377         1.411268           0.895477   
57           0.882989           0.044257         1.289674           0.694041   
58           0.263398          -0.376990         0.801326           0.037388   
59           0.708346          -0.109536         1.113639           0.429215   
...               ...                ...              ...                ...   
1593        -0.478602          -0.425671        -0.168581          -0.543339   
1594        -0.514958          -0.415289        -0.249065          -0.548287   
1595        -0.757198          -0.315673        -0.488311          -0.523867   
1596        -1.351179           0.152671        -1.013943          -0.264569   
1597        -1.504202           0.324898        -1.227174          -0.078794   

      EMA50_ROC_day-5  EMA50_ROC_day-5^2  
55           1.368601           1.338713  
56           1.454741           1.540091  
57           1.407105           1.427586  
58           1.084101           0.739215  
59           1.325418           1.241235  
...               ...                ...  
1593         0.174005          -0.502153  
1594         0.096821          -0.560023  
1595        -0.098169          -0.673204  
1596        -0.509592          -0.756795  
1597        -0.713234          -0.720245  

[1543 rows x 880 columns]</code></pre>
</div>
</div>
</section>
<section id="exploratory-data-visualization" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-visualization">1.3 Exploratory data visualization</h3>
<section id="dimension-reduction-method-and-probability-density-curves" class="level4">
<h4 class="anchored" data-anchor-id="dimension-reduction-method-and-probability-density-curves">1.3.1 Dimension reduction method and probability density curves</h4>
<p>Here, we used <code>UMAP</code> and Probability Density Curves to visualize the train and test data.</p>
<p><code>UMAP</code> is a dimension reduction method by which we can visualize the high dimensional data in a 2-D graph.</p>
<p>We can see that the distribution of train and test data are different. Therefore, we assume that training a good model tends to be hard for this dataset.</p>
<div id="cell-16" class="cell" data-cell_id="43dad4fe71384da3be4aaff4bb1b557f" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="13705" data-execution_start="1741122883185" data-source_hash="19815667" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the UMAP model (using a fixed random state for reproducibility).</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>umap_model <span class="op">=</span> UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit UMAP on the training data and transform both training and test data.</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>X_train_umap <span class="op">=</span> umap_model.fit_transform(X_train)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>X_test_umap <span class="op">=</span> umap_model.transform(X_test)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the UMAP projection.</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    X_train_umap[:, <span class="dv">0</span>],</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    X_train_umap[:, <span class="dv">1</span>],</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="st">'blue'</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">'Train'</span>,</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    X_test_umap[:, <span class="dv">0</span>],</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    X_test_umap[:, <span class="dv">1</span>],</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">'Test'</span>,</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'UMAP Dimension 1'</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'UMAP Dimension 2'</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'UMAP Projection of Train and Test Data'</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Roaming\Python\Python311\site-packages\umap\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-17" class="cell" data-cell_id="47995a75bd86491b82d2e3cd1a4feea0" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="209" data-execution_start="1741122983288" data-source_hash="6f6f5865" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(y_train, label<span class="op">=</span><span class="st">"Train"</span>, color<span class="op">=</span><span class="st">"blue"</span>, shade<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(y_test, label<span class="op">=</span><span class="st">"Test"</span>, color<span class="op">=</span><span class="st">"red"</span>, shade<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Close_ROC"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Probability Density Curve for Close Rate of Change (Train vs Test)"</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3388851737.py:3: FutureWarning: 

`shade` is now deprecated in favor of `fill`; setting `fill=True`.
This will become an error in seaborn v0.14.0; please update your code.

  sns.kdeplot(y_train, label="Train", color="blue", shade=True)
c:\ProgramData\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\zln92\AppData\Local\Temp\ipykernel_9188\3388851737.py:4: FutureWarning: 

`shade` is now deprecated in favor of `fill`; setting `fill=True`.
This will become an error in seaborn v0.14.0; please update your code.

  sns.kdeplot(y_test, label="Test", color="red", shade=True)
c:\ProgramData\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualization-of-close_roc" class="level4">
<h4 class="anchored" data-anchor-id="visualization-of-close_roc">1.3.2 Visualization of “Close_ROC”</h4>
<p>We can also see it in another perspective. The daily rate of change of Nasdaq Composite Index looks like noise, which could be hard to predict.</p>
<div id="cell-19" class="cell" data-cell_id="779cdc0883ad49b79cd0313ad96fe99d" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="179" data-execution_start="1741123059009" data-source_hash="b158fcd5" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    df_raw[<span class="st">"Date"</span>],</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    df_raw[<span class="st">"Close_ROC"</span>],</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"True Close_ROC"</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    linewidth<span class="op">=</span><span class="dv">1</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Rate of Change of Nasdaq Index"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Close_ROC"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="baseline-model" class="level3">
<h3 class="anchored" data-anchor-id="baseline-model">1.4 Baseline model</h3>
<p>The baseline model is just predict every Nasdaq index value on t day to be the value on t-1 day. It reflects the Martingale in probability theory. [3]</p>
<p><span class="math display">\[{\displaystyle \mathbf {E} (\vert X_{n}\vert )&lt;\infty }\]</span> <span class="math display">\[\mathbf {E} (X_{n+1}\mid X_{1},\ldots ,X_{n})=X_{n}.\]</span></p>
<p>Since we are predicting the value doesn’t change from t-1 day to t day, the predicted rate of change (‘Close_ROC’) of our baseline model is just 0.</p>
<p>In this project, we will use <strong>Root Mean Sqare Error (RMSE)</strong> to evaluate how the predicted value deviate from the true value in a more intuitive way.</p>
<div id="cell-22" class="cell" data-cell_id="16184f024390473aabd159f86b959964" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="0" data-execution_start="1741123255481" data-source_hash="55cff794" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create baseline predictions (all zeros) for training and test sets.</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>baseline_train_predict <span class="op">=</span> pd.Series(<span class="dv">0</span>, index<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(X_train)))</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>baseline_test_predict <span class="op">=</span> pd.Series(<span class="dv">0</span>, index<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(X_test)))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Mean Squared Error for training and test sets.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>baseline_mse_train_final <span class="op">=</span> mean_squared_error(baseline_train_predict, y_train)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>baseline_mse_test_final <span class="op">=</span> mean_squared_error(baseline_test_predict, y_test)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Root Mean Squared Error.</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>baseline_rmse_train_final <span class="op">=</span> np.sqrt(baseline_mse_train_final)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>baseline_rmse_test_final <span class="op">=</span> np.sqrt(baseline_mse_test_final)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results.</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Baseline Regression Model Train MSE:"</span>, baseline_mse_train_final)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Baseline Regression Model Test MSE:"</span>, baseline_mse_test_final)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Baseline Regression Model Train RMSE:"</span>, baseline_rmse_train_final)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Baseline Regression Model Test RMSE:"</span>, baseline_rmse_test_final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Baseline Regression Model Train MSE: 0.0001273147808231767
Baseline Regression Model Test MSE: 5.152090781204474e-05
Baseline Regression Model Train RMSE: 0.01128338516683609
Baseline Regression Model Test RMSE: 0.00717780661567618</code></pre>
</div>
</div>
</section>
</section>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">2. Linear Regression</h2>
<section id="linear-regression-with-all-the-features" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-with-all-the-features">2.1 Linear regression with all the features</h3>
<p>Since we have squared features, this is actually polynomial regression. We just treat them as independent features, so we call “linear regression” here.</p>
<div id="cell-24" class="cell" data-cell_id="290e6a57671240dcacd387a11bc8c710" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="2712" data-execution_start="1741123558672" data-source_hash="ecc817ac" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and fit the linear regression model.</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>lin_reg.fit(X_train, y_train)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print training and testing MSE.</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>train_mse <span class="op">=</span> mean_squared_error(lin_reg.predict(X_train), y_train)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>test_mse <span class="op">=</span> mean_squared_error(lin_reg.predict(X_test), y_test)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training mean_squared_error:"</span>, train_mse)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing mean_squared_error:"</span>, test_mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training mean_squared_error: 4.631345421141122e-05
Testing mean_squared_error: 0.02637718530839419</code></pre>
</div>
</div>
<div id="cell-25" class="cell" data-cell_id="309337fcc21349019fcc05221111ae8c" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="1" data-execution_start="1741123682128" data-source_hash="95927893" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Output model parameters and score.</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model coefficients:"</span>, lin_reg.coef_)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model intercept:"</span>, lin_reg.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model coefficients: [ 2.62057358e+07 -2.96578327e+00 -4.13480215e-04 -2.72071920e-05
 -1.44017558e+05  1.08345848e+05 -3.83533363e+08 -1.91476076e+08
  3.32686386e+08 -3.68066553e+07  1.42758759e+08 -1.02127726e+09
  1.47720671e+00 -3.62809338e-02  2.81973891e-02  1.54078007e-04
  1.03226490e-02  8.03850591e-04  3.86587158e-03 -7.49139115e-04
 -7.99382745e+07  3.06860472e+01 -9.05965750e+07 -4.54048501e+01
 -7.21136865e+07  3.83088106e+01 -4.45410220e+08  6.79812764e-02
  8.55298351e+08 -9.78320895e-02 -5.46330650e+07  7.55610084e-02
 -2.95291096e-03  1.14880130e-02  3.76559227e+08  2.28153099e-01
  2.10749358e-03  1.81356270e-03 -6.28710911e-03 -7.41733983e-03
 -7.72151243e+08 -6.47891592e-02 -3.65905154e+08 -7.72383958e-01
 -2.26842239e-04 -3.40396538e-04  7.73995183e-04 -2.21330673e-04
 -4.31597233e-04 -4.28985804e-04 -6.03104010e-04  4.24114987e-04
 -2.10508704e-04 -4.15660441e-04 -8.28200020e-04 -2.55010091e-04
 -1.24806305e-04 -2.48365104e-05 -8.67899507e-06 -9.38728452e-04
 -3.14038247e-04  6.48051500e-05 -1.09843910e-04  2.30763108e-04
 -6.44342974e-04  6.96144998e-05 -1.93879940e-04 -3.78049910e-04
 -9.99668613e-04  1.44044682e-03  2.07625888e-03 -7.32921064e-04
  1.91627815e-03  1.51914731e-03  1.26789380e-02 -4.05019894e-03
 -4.49717045e-04  2.38418579e-06 -9.28389281e-03  2.46874616e-03
  6.05840236e-04  3.62843275e-04  2.59920582e-03  8.25606287e-03
 -2.50494666e-03  6.75324351e-04 -4.96984245e+08  2.57943077e-02
  1.41094127e+09  1.48746443e-01 -7.44916334e+08 -4.16972808e-01
 -3.66827576e+07 -4.96551488e-03 -1.09662973e+08  5.89082390e-03
 -3.31723571e+08 -2.32027322e-02  4.38763215e+08  1.42487660e-02
 -2.97501488e+08  1.08641377e+00  1.09941140e+08  1.35141216e-01
  8.55251198e+08 -1.56077679e-01 -1.46519393e-03  3.91995534e-04
 -1.66719779e-03  6.44147396e-04  4.99680638e-04 -2.11019143e-02
 -5.02852723e-04 -4.82596457e-04  1.74440444e-04 -1.90682709e-04
  1.51686370e-03  1.65253878e-04  7.86118209e-04  2.66898423e-05
 -2.18183175e-03 -3.47549096e-04 -2.00533681e-03 -2.17219070e-03
  5.87402191e-03 -2.01880932e-03 -2.13204045e-03 -1.44053251e-04
 -1.85622275e-03 -3.54498625e-04  5.21294773e-04 -3.44254076e-04
  9.89805907e-04  2.32450664e-04 -4.98671085e-04 -6.44958578e-04
  9.33244824e-04  2.64229253e-03 -6.23449683e-04 -2.65441835e-04
  1.49097294e-04 -1.91995874e-04 -3.00809741e-04  1.73951499e-04
 -6.03713095e-04 -9.93054360e-04 -3.84919345e-04 -4.43592109e-03
 -3.98005359e-03  4.17549163e-03  4.94679436e-04  3.55049036e-04
 -4.55371011e-03  1.43658184e-03  1.13740435e-03 -5.35298139e-04
  9.96394083e-05  2.48168595e-04 -3.32668424e-05  1.34536251e-03
  8.28970224e-04  3.19182873e-04 -9.38921375e-03  4.71939472e-03
 -2.79052742e-03  3.52782756e-03  4.36417758e-04  4.33474779e-05
  1.44019503e+05 -1.08345880e+05 -7.07136685e+00  1.44147981e-01
  4.69361988e+00 -1.11443514e-01 -1.50132220e+00  3.95782832e-02
 -7.58076859e+07  2.99460695e+00  3.75622651e-04 -7.50156119e-04
  3.83694284e+08  1.91701349e+08 -9.18530914e+08  2.73091193e+08
  3.62502912e+08  1.52348178e+08  4.77554878e+08 -5.35554408e+08
 -5.57312391e-01  1.40600875e-02  2.43721716e-02 -2.65014917e-03
  8.74751061e-03 -1.63334794e-03  7.03550130e-03 -1.67050026e-03
  3.48745010e+08 -4.99560316e+01  2.54087698e+08  4.90134665e+01
  2.55770365e+08  2.61316518e+00 -1.38367010e+09 -5.43577196e-02
  1.59453852e+09  1.00604813e-01 -2.50301275e+08 -1.56793077e-01
  9.19553638e-03 -8.35467130e-03  5.02880167e+08 -1.80158225e-01
 -1.99749693e-03  1.80619014e-02 -1.44760683e-02 -1.41290780e-02
 -8.34795431e+08 -1.26528181e-02  7.60811340e+08  4.72815812e-01
 -4.32543457e-04 -5.92859462e-04  4.31454740e-04 -2.16336921e-04
 -3.31010669e-04 -2.15161592e-04 -1.63820572e-04 -8.88444483e-05
  6.64774328e-04  2.55461782e-04  5.99021092e-04  1.55138317e-04
  3.57009470e-04 -4.54422552e-04 -5.69013879e-04 -6.82063401e-04
 -5.65672293e-04  6.94105867e-04  2.25262716e-04 -3.78400087e-04
 -6.79403543e-04  1.48117542e-04  4.70077619e-04  1.46068633e-05
 -1.78450719e-03 -3.00404429e-03  2.74198316e-03 -4.17916104e-04
 -1.30403787e-04 -1.17681921e-04  1.23955309e-03  2.33393908e-03
  7.98057765e-04 -5.55988401e-04 -1.97822601e-03  2.24778429e-03
  9.08467919e-05 -7.57798553e-05  1.81981549e-03 -2.00450979e-03
 -3.15406546e-03 -5.66351973e-03  2.90933191e+08 -5.28128953e-02
 -3.21072634e+08 -6.03636376e-01 -1.83294853e+08  8.44726223e-01
 -3.47278482e+08  5.04561327e-03  5.61183222e+07 -3.77563387e-03
 -3.58220901e+08  5.76590672e-02  2.65414736e+08 -2.56042853e-02
  9.34688393e+07 -1.64581435e+00 -8.73863724e+08  9.95124876e-01
  5.14246105e+08  2.69232988e-02 -3.54088843e-05 -6.97188079e-06
 -1.39476359e-03  1.21091641e-02  3.05553898e-03  1.01378933e-03
 -1.20602548e-03 -1.00100413e-04  1.29752606e-03 -6.65001571e-05
  2.63286754e-03  4.09756228e-03  1.64574012e-04  9.65138897e-05
  2.85319984e-05 -2.78608873e-04 -7.69028440e-04  2.88311392e-04
  3.05164233e-03  1.62302330e-03 -4.99407062e-04 -5.28521836e-04
 -1.76540762e-03  2.82354653e-04  1.61041319e-03 -5.39831817e-04
 -6.63969666e-04  1.62825733e-03 -3.86139378e-04 -4.97372821e-05
 -2.13696435e-03 -1.11355074e-03  2.92520970e-04 -2.71508470e-04
  8.33291560e-05  5.51383011e-04  5.49517572e-05 -8.99387524e-04
 -1.92902237e-03  2.63471156e-04 -2.01787427e-03  5.47098927e-04
  4.75287437e-03  2.32979655e-05 -1.61765143e-04 -4.68520448e-04
  5.63289970e-04  2.76416540e-04 -1.97694823e-03  1.69471279e-03
 -9.46354121e-05 -1.76105648e-04  1.78915262e-03  3.23541462e-05
  2.33054161e-05  4.70131636e-05  1.34656974e-02 -1.64756649e-02
  6.21721335e-03 -6.68330677e-03  4.00772691e-03 -5.05124032e-03
 -1.60918669e+05 -2.25273099e+05  2.75560892e+00 -7.24322610e-02
 -8.04407997e+00  1.80345116e-01  4.39857134e+00 -1.05086368e-01
 -7.72841657e+07  2.67670179e+00 -7.79964030e-05  1.41989440e-04
  5.85908410e+08 -2.36372760e+08 -3.88973875e+08  2.46040601e+08
 -7.93894042e+07 -7.93844439e+07  6.76234786e+07  5.30399462e+08
 -4.60655686e-01  9.54800285e-03  1.89510286e-02 -1.07686967e-04
  8.87437165e-03  5.30703925e-04  1.09641999e-03 -9.89402644e-04
  1.31660047e+08  2.26757506e+01 -1.74827360e+08 -9.69528705e+01
  1.38989136e+08  9.37246974e+01  2.28047130e+08 -1.24002793e-02
 -4.51970514e+08  1.77573613e-02  3.50800373e+08 -1.09964549e-02
  3.88414450e-02 -4.14879173e-02 -2.83458982e+08  2.31746739e-01
  4.37937677e-04  8.14113766e-04 -1.63009055e-02 -1.71709713e-02
  4.31237909e+07  7.53031261e-02 -1.14368990e+07 -2.09814034e-01
 -5.79290092e-04  3.48025933e-04  5.13102859e-05  4.69595194e-04
  5.82609326e-04 -3.34728509e-04 -1.08151697e-04 -4.05339524e-04
  1.70242041e-04  1.01359561e-04 -8.63801688e-05 -7.65771139e-04
 -2.49238685e-04  1.25181861e-04 -1.96946785e-04 -7.63842836e-05
 -5.63133508e-05  4.28296626e-05  2.79400498e-04 -5.25413547e-04
 -2.33098865e-04  4.47896309e-04 -7.36817718e-04  4.76781279e-04
 -2.96624377e-04 -3.17106256e-03  9.71117988e-04  1.11708045e-03
 -9.83774662e-04 -3.30153853e-04  9.23985988e-03 -9.73826088e-03
  3.13572586e-04  2.31659040e-04 -9.07392614e-03  1.40720280e-03
  7.20554963e-05 -3.00392509e-04 -6.78368099e-03 -4.56801988e-03
  2.91998312e-03  2.39097327e-03 -1.79739846e+08  5.55952955e-02
  3.44793685e+08  4.96307658e-02 -3.67253684e+08 -2.77184501e-01
  8.50978318e+07  7.90213235e-04 -1.15281738e+08 -8.10250640e-05
  1.84825199e+07 -1.49305463e-02 -3.72744568e+08 -4.25354913e-02
  2.88947553e+08  5.66416565e-01 -3.05309134e+08 -2.32664082e-01
  6.17188763e+08 -1.55461222e-01 -2.58110464e-04 -3.28820199e-04
  7.00112432e-04 -4.75697964e-03  5.36713749e-04  8.85460898e-03
  9.03290696e-04  5.27605414e-04 -5.18757850e-04 -1.93601474e-04
 -7.99672678e-04  3.87602113e-03  4.72556800e-04 -1.03782024e-03
  5.65238297e-04  5.32228500e-04  2.93800607e-03 -1.04389060e-03
  7.75189605e-03  2.39936728e-03 -2.26914417e-03 -2.87275761e-04
 -2.09900178e-03  2.94567086e-04  1.32662058e-03 -1.70119107e-04
 -1.07938051e-03 -6.77078962e-04  4.40768898e-04 -9.54102725e-05
 -5.07216901e-04 -1.51727814e-03 -1.06364489e-04  6.56111166e-04
  1.06427819e-04  6.80577010e-04 -3.14056873e-04  1.09440088e-03
 -2.06752494e-03 -8.77022743e-04  5.26526477e-03  2.11834721e-03
 -6.47418015e-03 -2.78832531e-03 -7.00398348e-04 -7.10269436e-04
 -1.61170959e-04  3.08081508e-06 -1.86529569e-03 -4.44613397e-05
  1.09211542e-05 -3.22155654e-04  2.04877555e-03  3.93580645e-04
 -5.79819083e-04  6.46566972e-04 -4.12782910e-03  5.99174108e-03
 -7.61861354e-03  8.57660547e-03  3.62806022e-03 -2.84300745e-03
 -6.38821926e+04  8.82228909e+04  3.58049703e+00 -7.42405821e-02
  2.38376074e-01 -2.03442611e-02 -3.09107400e-01  1.90193057e-02
 -2.82500000e+07 -2.94284438e+00  3.62894498e-04  2.31517479e-04
 -1.16249588e+08  6.22557977e+08 -1.94127721e+08  2.88146497e+08
 -4.39620157e+08  1.91783593e+08  2.82384558e+08  9.13197426e+07
  5.64981140e-01 -1.64119219e-02  1.45581588e-02 -5.27389348e-05
  6.34174049e-03 -3.42745613e-03 -3.35102901e-03  2.65965424e-03
 -2.69673272e+08  1.03001527e+01  1.98795357e+08  1.47285370e+02
 -1.60457035e+08 -2.63196674e+02 -3.92276175e+08  1.61441274e-02
  1.01167300e+08 -3.73200870e-02  1.95694508e+07  3.18813231e-02
 -1.92042366e-02  2.18161196e-02  2.84032488e+08 -4.77510955e-01
 -3.11491638e-03  5.95536875e-03 -4.04088572e-03 -3.38539761e-03
  3.05973037e+08 -5.74705862e-02  1.11678769e+08  9.90952712e-01
 -1.71770342e-04 -1.89806800e-04 -1.15001574e-04  4.02650796e-04
 -2.91088596e-04 -3.32625583e-04 -8.56917351e-04  8.16769898e-04
  4.90834936e-04 -4.63884324e-04  3.31852585e-04 -4.96966764e-04
  4.19577584e-04 -5.63133508e-04  4.18039039e-04  1.61416829e-05
  1.28729828e-03 -4.40614298e-04  3.68189067e-05 -4.76390123e-05
 -1.54921599e-03  1.14366226e-03  1.04306638e-03 -9.47734341e-04
 -3.92310321e-05 -4.48897481e-05  3.08420137e-03  1.43991783e-05
 -1.68686360e-03  4.35502268e-04 -6.24807179e-03 -1.90264732e-03
 -3.23191285e-04  2.48501077e-04  4.34424914e-03  6.84682280e-04
  7.95170199e-04 -1.95369124e-04  6.84965402e-04  5.83965331e-04
 -2.46786140e-03  4.89757676e-03 -5.18328132e+08  1.09250715e-01
  1.79601207e+08  3.62519262e-01 -2.47009983e+08  5.18836062e-02
 -4.17491373e+07 -1.04687922e-03 -1.85779460e+07  1.77665055e-03
  1.32070162e+08 -9.09690559e-03 -1.88515923e+08 -1.85406953e-03
  1.76285228e+08 -2.36321699e-01 -6.42166704e+08 -3.49990245e-01
  2.64162880e+08 -5.42632584e-01 -4.04356048e-04  6.41353428e-04
  1.96583569e-03 -1.00218542e-02 -2.08716840e-04  1.75200775e-02
 -5.42805530e-04  2.86944211e-04  1.42880157e-03  7.07386993e-04
  4.97370958e-04 -4.43143770e-04  1.90753490e-04  4.05076891e-04
  9.93933529e-04 -7.85485841e-04  1.24306977e-03  1.09615736e-04
 -3.57621815e-03 -2.88124196e-04  2.58515589e-04 -6.89113513e-04
 -3.31896730e-03  7.39726238e-05 -5.70341945e-04 -2.81434506e-04
 -2.98067927e-04  2.85591930e-04  5.95809892e-04 -1.63037702e-03
 -2.09925510e-03  7.47689977e-04  7.10783526e-04 -3.43123917e-04
  4.92839143e-04  3.08316201e-04  4.26768325e-04  9.22667794e-04
 -1.99169666e-03  8.67165625e-04  8.94172117e-04 -1.17521919e-03
  5.26542775e-03 -9.59337689e-04  3.52272764e-04  6.52877148e-04
 -1.61186606e-03  6.81344420e-04  1.19907595e-03 -4.04238701e-04
 -1.18480530e-04 -3.55364755e-04  1.24654919e-03 -3.22546810e-04
  1.86721236e-03 -1.33803114e-04  8.55335407e-03 -7.87111837e-03
  9.74478573e-03 -9.66214202e-03  9.27622616e-03 -1.15540028e-02
 -3.82076496e+04  3.30500619e+05 -3.28309132e+00  6.62801974e-02
  6.12602887e+00 -1.25332962e-01 -3.85327780e+00  8.61278027e-02
  4.61519744e-01 -3.01127065e-01  5.62573317e-04 -7.04266131e-05
 -2.03795091e+08  3.26931383e+08  3.71996679e+08 -7.22183056e+08
 -2.82384558e+08 -9.13197426e+07 -2.45863415e-01  1.87298656e-03
  2.38748454e-02  1.12282299e-03  1.04319006e-02 -9.96150076e-04
  3.41490656e-03  3.06953676e-03  8.38867947e-03 -1.46391243e-03
 -2.23602878e+07 -1.02882111e+01 -1.64056761e+08 -5.94880942e+01
 -1.39140906e+08  1.31203108e+02  2.38022836e+07  3.93006045e-02
  7.42691152e+06  1.08493103e-02  4.51356170e+07  4.94066365e-02
 -1.08511001e-02  4.10205126e-03  1.91970002e+08  2.61775924e-01
 -9.43323597e-04  4.52343503e-03 -6.39970042e-03 -5.78400958e-03
  1.56734623e+08  6.66545406e-02 -5.91931049e+08 -4.00250580e-01
  4.24778089e-04  5.40474430e-05  5.28376549e-05 -2.54702754e-04
  3.04075889e-04 -2.34063715e-04  1.62553042e-04 -3.27820890e-04
 -4.48601320e-04 -7.36340880e-05  3.05745751e-04  1.33980066e-05
  5.41724265e-04 -5.16455621e-04  3.23073938e-04  1.71296299e-04
  4.40615695e-04 -4.31347638e-04 -7.10375607e-05 -3.19455750e-04
 -7.05904327e-04  9.96874645e-04 -6.13827258e-04  6.39501959e-04
 -5.39699569e-04 -7.07104802e-04  1.06311589e-03 -2.10737064e-03
 -1.19427405e-03  2.00462714e-03 -2.93325726e-03 -4.14118171e-04
  4.14527953e-04  3.97972763e-04  8.52432102e-04 -4.15523723e-03
 -2.51466408e-04 -3.91051173e-04 -4.37567011e-04 -7.49830157e-04
  3.72385047e-03  3.94434109e-03  3.64448997e+08 -7.99464416e-02
  3.65861028e+08 -1.88737954e-01 -4.74268066e+08 -7.55061917e-02
  6.78573043e+07  5.66391274e-04 -1.01895521e+08 -3.61134112e-03
  6.75782746e+07 -4.83452156e-03  3.44988389e+08  3.84124294e-02
 -2.87154506e+08 -3.26753877e-01  7.33240256e+08  2.38411695e-01
 -5.63321193e+08  4.87546895e-01  1.74969807e-03 -3.10759991e-03
 -1.04537606e-03 -9.20918584e-03  1.68327987e-03  1.21587068e-02
 -1.62206125e-03 -3.72342765e-04  6.29968941e-04  9.71682370e-04
 -2.03307718e-04  7.73645937e-04 -9.34612006e-04  6.97683543e-04
 -4.10247594e-04 -1.08621828e-03  1.33115798e-04 -1.05115399e-03
 -3.01816501e-04  2.09816732e-03  7.41318567e-04  6.00114465e-04
 -6.33637421e-04  8.00475478e-04 -1.63827091e-04  2.63186172e-04
 -3.70287336e-04 -5.57564199e-04  4.41348180e-04  8.45052302e-04
 -1.06884167e-03  1.00009702e-03 -2.66503543e-04  7.26211816e-04
  3.33329663e-04  3.85753810e-05  1.17437262e-03 -1.31708384e-03
  8.22907314e-04 -8.98305327e-04 -1.73187815e-03  1.55647844e-03
  2.12565064e-04 -6.05812296e-03  9.02129337e-04  2.30273232e-04
  3.51615250e-04  3.25694680e-04 -3.63683328e-04 -9.16386954e-04
 -3.43021937e-04  5.69811091e-05  3.79960984e-04  4.59177420e-04
 -1.17439777e-04  2.15541571e-04 -1.45976511e-02  1.60959316e-02
 -4.56042588e-03  4.34851833e-03 -1.94329768e-02  2.18475163e-02
 -2.42662771e+05 -1.39027815e+05  1.69911600e+00 -4.18187603e-02
 -2.89512119e+00  6.85486030e-02  1.36955070e+00 -3.46447825e-02]
Model intercept: 0.004142097607368128</code></pre>
</div>
</div>
</section>
<section id="feature-selection-for-linear-regression-recursive-feature-elimination-rfe" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-for-linear-regression-recursive-feature-elimination-rfe">2.2 Feature selection for linear regression: Recursive Feature Elimination (RFE)</h3>
<p>From above, we can notice that the overfitting occurs. To solve the overfitting, we will try to use fewer features.</p>
<p>Since there are there are too many features, it is too computational expensive to try all the combinations of features. Therefore, we need to select the most important features. We implement Recursive Feature Elimination (RFE) to select the most important features.</p>
<section id="visualization-of-feature-importance" class="level4">
<h4 class="anchored" data-anchor-id="visualization-of-feature-importance">2.2.1 Visualization of feature importance</h4>
<div id="cell-28" class="cell" data-cell_id="faee2b6356114c7baff7992d09b98746" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="1796" data-execution_start="1741124042696" data-source_hash="43e5470a" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming `X.columns` contains the feature names and `lin_reg.coef_` contains the coefficients.</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>df_coef <span class="op">=</span> pd.DataFrame({</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: X.columns,</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'coef'</span>: np.<span class="bu">abs</span>(lin_reg.coef_)  <span class="co"># Taking absolute values of coefficients</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>df_coef[<span class="st">'day'</span>] <span class="op">=</span> df_coef[<span class="st">'feature'</span>].<span class="bu">str</span>.extract(<span class="vs">r'day-</span><span class="kw">(</span><span class="dv">\d</span><span class="op">+</span><span class="kw">)</span><span class="vs">'</span>).astype(<span class="bu">float</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>df_coef[<span class="st">'base_feature'</span>] <span class="op">=</span> df_coef[<span class="st">'feature'</span>].<span class="bu">str</span>.replace(</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r'_day-</span><span class="dv">\d</span><span class="op">+</span><span class="ex">(</span><span class="fu">?=</span><span class="ch">\^</span><span class="vs">2</span><span class="ex">)</span><span class="cf">|</span><span class="vs">_day-</span><span class="dv">\d</span><span class="op">+</span><span class="vs">'</span>,</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">''</span>,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    regex<span class="op">=</span><span class="va">True</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by 'base_feature' and 'day' to ensure uniqueness.</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>df_coef_grouped <span class="op">=</span> df_coef.groupby([<span class="st">'base_feature'</span>, <span class="st">'day'</span>])[<span class="st">'coef'</span>].mean().unstack()</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot heatmap.</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">30</span>))</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>plt.imshow(</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    df_coef_grouped,</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>    aspect<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">'viridis'</span>,</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>    interpolation<span class="op">=</span><span class="st">'nearest'</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">"Absolute Coefficient Value"</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>plt.xticks(</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>    ticks<span class="op">=</span>np.arange(<span class="bu">len</span>(df_coef_grouped.columns)),</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>df_coef_grouped.columns,</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    rotation<span class="op">=</span><span class="dv">45</span></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>plt.yticks(</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>    ticks<span class="op">=</span>np.arange(<span class="bu">len</span>(df_coef_grouped.index)),</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>df_coef_grouped.index</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Day Lag"</span>)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Feature"</span>)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature Importance Heatmap (Absolute Coefficients)"</span>)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-29" class="cell" data-cell_id="121d4d1b0d334cdcbf22e018ea342a28" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="0" data-execution_start="1741124148233" data-source_hash="3f232eb2" data-execution_count="15">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since the 'LinearRegression' object doesn't have 'feature_names_in_', manually retrieve column names.</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>df_coef <span class="op">=</span> pd.DataFrame({</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: X.columns,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'coef'</span>: lin_reg.coef_</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>chart <span class="op">=</span> alt.Chart(df_coef).mark_bar().encode(</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'coef'</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'feature'</span>, sort<span class="op">=</span><span class="st">'-x'</span>)  <span class="co"># Sort by descending coefficient values.</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>).properties(</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Coefficients'</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>chart</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\ProgramData\anaconda3\Lib\site-packages\altair\utils\core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.
  col = df[col_name].apply(to_list_if_array, convert_dtype=False)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">

<style>
  #altair-viz-dc96eb0bcd7946bda914957d49c9c6d9.vega-embed {
    width: 100%;
    display: flex;
  }

  #altair-viz-dc96eb0bcd7946bda914957d49c9c6d9.vega-embed details,
  #altair-viz-dc96eb0bcd7946bda914957d49c9c6d9.vega-embed details summary {
    position: relative;
  }
</style>
<div id="altair-viz-dc96eb0bcd7946bda914957d49c9c6d9"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-dc96eb0bcd7946bda914957d49c9c6d9") {
      outputDiv = document.getElementById("altair-viz-dc96eb0bcd7946bda914957d49c9c6d9");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm/vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm/vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm/vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "5.8.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "data": {"name": "data-95b567012a09cc0d11e21af0d1696c22"}, "mark": {"type": "bar"}, "encoding": {"x": {"field": "coef", "type": "quantitative"}, "y": {"field": "feature", "sort": "-x", "type": "nominal"}}, "title": "Coefficients", "$schema": "https://vega.github.io/schema/vega-lite/v5.8.0.json", "datasets": {"data-95b567012a09cc0d11e21af0d1696c22": [{"feature": "Close_day-1", "coef": 26205735.75946959}, {"feature": "Close_day-1^2", "coef": -2.965783271472901}, {"feature": "Volume_day-1", "coef": -0.0004134802147746086}, {"feature": "Volume_day-1^2", "coef": -2.720719203352928e-05}, {"feature": "mom_day-1", "coef": -144017.55845696386}, {"feature": "mom_day-1^2", "coef": 108345.84777292679}, {"feature": "mom1_day-1", "coef": -383533363.2388537}, {"feature": "mom1_day-1^2", "coef": -191476075.79552805}, {"feature": "mom2_day-1", "coef": 332686386.1284594}, {"feature": "mom2_day-1^2", "coef": -36806655.3156527}, {"feature": "mom3_day-1", "coef": 142758758.93163133}, {"feature": "mom3_day-1^2", "coef": -1021277256.5242652}, {"feature": "ROC_5_day-1", "coef": 1.477206714451313}, {"feature": "ROC_5_day-1^2", "coef": -0.036280933767557144}, {"feature": "ROC_10_day-1", "coef": 0.028197389096021652}, {"feature": "ROC_10_day-1^2", "coef": 0.00015407800674438477}, {"feature": "ROC_15_day-1", "coef": 0.010322649031877518}, {"feature": "ROC_15_day-1^2", "coef": 0.0008038505911827087}, {"feature": "ROC_20_day-1", "coef": 0.0038658715784549713}, {"feature": "ROC_20_day-1^2", "coef": -0.0007491391152143478}, {"feature": "EMA_10_day-1", "coef": -79938274.49451977}, {"feature": "EMA_10_day-1^2", "coef": 30.68604715913534}, {"feature": "EMA_20_day-1", "coef": -90596574.98769353}, {"feature": "EMA_20_day-1^2", "coef": -45.404850050807}, {"feature": "EMA_50_day-1", "coef": -72113686.48885575}, {"feature": "EMA_50_day-1^2", "coef": 38.30881055817008}, {"feature": "DTB4WK_day-1", "coef": -445410219.88508767}, {"feature": "DTB4WK_day-1^2", "coef": 0.06798127642832696}, {"feature": "DTB3_day-1", "coef": 855298350.672444}, {"feature": "DTB3_day-1^2", "coef": -0.0978320895228535}, {"feature": "DTB6_day-1", "coef": -54633065.03484189}, {"feature": "DTB6_day-1^2", "coef": 0.07556100841611624}, {"feature": "DGS5_day-1", "coef": -0.0029529109597206116}, {"feature": "DGS5_day-1^2", "coef": 0.011488012969493866}, {"feature": "DGS10_day-1", "coef": 376559227.3066178}, {"feature": "DGS10_day-1^2", "coef": 0.22815309930592775}, {"feature": "Oil_day-1", "coef": 0.002107493579387665}, {"feature": "Oil_day-1^2", "coef": 0.001813562703318894}, {"feature": "Gold_day-1", "coef": -0.0062871091067790985}, {"feature": "Gold_day-1^2", "coef": -0.0074173398315906525}, {"feature": "DAAA_day-1", "coef": -772151243.1704643}, {"feature": "DAAA_day-1^2", "coef": -0.06478915922343731}, {"feature": "DBAA_day-1", "coef": -365905154.34681404}, {"feature": "DBAA_day-1^2", "coef": -0.7723839581012726}, {"feature": "GBP_day-1", "coef": -0.0002268422394990921}, {"feature": "GBP_day-1^2", "coef": -0.00034039653837680817}, {"feature": "JPY_day-1", "coef": 0.0007739951834082603}, {"feature": "JPY_day-1^2", "coef": -0.0002213306725025177}, {"feature": "CAD_day-1", "coef": -0.0004315972328186035}, {"feature": "CAD_day-1^2", "coef": -0.0004289858043193817}, {"feature": "CNY_day-1", "coef": -0.0006031040102243423}, {"feature": "CNY_day-1^2", "coef": 0.00042411498725414276}, {"feature": "AAPL_day-1", "coef": -0.00021050870418548584}, {"feature": "AAPL_day-1^2", "coef": -0.00041566044092178345}, {"feature": "AMZN_day-1", "coef": -0.0008282000198960304}, {"feature": "AMZN_day-1^2", "coef": -0.00025501009076833725}, {"feature": "GE_day-1", "coef": -0.00012480630539357662}, {"feature": "GE_day-1^2", "coef": -2.483651041984558e-05}, {"feature": "JNJ_day-1", "coef": -8.678995072841644e-06}, {"feature": "JNJ_day-1^2", "coef": -0.0009387284517288208}, {"feature": "JPM_day-1", "coef": -0.0003140382468700409}, {"feature": "JPM_day-1^2", "coef": 6.480515003204346e-05}, {"feature": "MSFT_day-1", "coef": -0.000109843909740448}, {"feature": "MSFT_day-1^2", "coef": 0.00023076310753822327}, {"feature": "WFC_day-1", "coef": -0.0006443429738283157}, {"feature": "WFC_day-1^2", "coef": 6.961449980735779e-05}, {"feature": "XOM_day-1", "coef": -0.00019387993961572647}, {"feature": "XOM_day-1^2", "coef": -0.00037804991006851196}, {"feature": "FCHI_day-1", "coef": -0.00099966861307621}, {"feature": "FCHI_day-1^2", "coef": 0.001440446823835373}, {"feature": "FTSE_day-1", "coef": 0.002076258882880211}, {"feature": "FTSE_day-1^2", "coef": -0.0007329210638999939}, {"feature": "GDAXI_day-1", "coef": 0.0019162781536579132}, {"feature": "GDAXI_day-1^2", "coef": 0.0015191473066806793}, {"feature": "GSPC_day-1", "coef": 0.01267893798649311}, {"feature": "GSPC_day-1^2", "coef": -0.004050198942422867}, {"feature": "HSI_day-1", "coef": -0.00044971704483032227}, {"feature": "HSI_day-1^2", "coef": 2.384185791015625e-06}, {"feature": "DJI_day-1", "coef": -0.009283892810344696}, {"feature": "DJI_day-1^2", "coef": 0.002468746155500412}, {"feature": "SSEC_day-1", "coef": 0.0006058402359485626}, {"feature": "SSEC_day-1^2", "coef": 0.00036284327507019043}, {"feature": "RUT_day-1", "coef": 0.0025992058217525482}, {"feature": "RUT_day-1^2", "coef": 0.008256062865257263}, {"feature": "NYSE_day-1", "coef": -0.0025049466639757156}, {"feature": "NYSE_day-1^2", "coef": 0.0006753243505954742}, {"feature": "TE1_day-1", "coef": -496984245.317563}, {"feature": "TE1_day-1^2", "coef": 0.025794307701289654}, {"feature": "TE2_day-1", "coef": 1410941268.3828108}, {"feature": "TE2_day-1^2", "coef": 0.1487464434467256}, {"feature": "TE3_day-1", "coef": -744916333.6922346}, {"feature": "TE3_day-1^2", "coef": -0.4169728076085448}, {"feature": "TE5_day-1", "coef": -36682757.580243334}, {"feature": "TE5_day-1^2", "coef": -0.004965514875948429}, {"feature": "TE6_day-1", "coef": -109662973.43731126}, {"feature": "TE6_day-1^2", "coef": 0.0058908239006996155}, {"feature": "DE1_day-1", "coef": -331723570.8774233}, {"feature": "DE1_day-1^2", "coef": -0.02320273220539093}, {"feature": "DE2_day-1", "coef": 438763215.0788819}, {"feature": "DE2_day-1^2", "coef": 0.014248766005039215}, {"feature": "DE4_day-1", "coef": -297501488.1273809}, {"feature": "DE4_day-1^2", "coef": 1.0864137671887875}, {"feature": "DE5_day-1", "coef": 109941139.56799543}, {"feature": "DE5_day-1^2", "coef": 0.13514121621847153}, {"feature": "DE6_day-1", "coef": 855251197.7279971}, {"feature": "DE6_day-1^2", "coef": -0.15607767924666405}, {"feature": "CTB3M_day-1", "coef": -0.0014651939272880554}, {"feature": "CTB3M_day-1^2", "coef": 0.00039199553430080414}, {"feature": "CTB6M_day-1", "coef": -0.0016671977937221527}, {"feature": "CTB6M_day-1^2", "coef": 0.0006441473960876465}, {"feature": "CTB1Y_day-1", "coef": 0.0004996806383132935}, {"feature": "CTB1Y_day-1^2", "coef": -0.02110191434621811}, {"feature": "AUD_day-1", "coef": -0.0005028527230024338}, {"feature": "AUD_day-1^2", "coef": -0.0004825964570045471}, {"feature": "Brent_day-1", "coef": 0.0001744404435157776}, {"feature": "Brent_day-1^2", "coef": -0.00019068270921707153}, {"feature": "CAC-F_day-1", "coef": 0.0015168637037277222}, {"feature": "CAC-F_day-1^2", "coef": 0.0001652538776397705}, {"feature": "copper-F_day-1", "coef": 0.00078611820936203}, {"feature": "copper-F_day-1^2", "coef": 2.6689842343330383e-05}, {"feature": "WIT-oil_day-1", "coef": -0.0021818317472934723}, {"feature": "WIT-oil_day-1^2", "coef": -0.00034754909574985504}, {"feature": "DAX-F_day-1", "coef": -0.002005336806178093}, {"feature": "DAX-F_day-1^2", "coef": -0.002172190696001053}, {"feature": "DJI-F_day-1", "coef": 0.005874021910130978}, {"feature": "DJI-F_day-1^2", "coef": -0.0020188093185424805}, {"feature": "EUR_day-1", "coef": -0.002132040448486805}, {"feature": "EUR_day-1^2", "coef": -0.00014405325055122375}, {"feature": "FTSE-F_day-1", "coef": -0.0018562227487564087}, {"feature": "FTSE-F_day-1^2", "coef": -0.00035449862480163574}, {"feature": "gold-F_day-1", "coef": 0.0005212947726249695}, {"feature": "gold-F_day-1^2", "coef": -0.0003442540764808655}, {"feature": "HSI-F_day-1", "coef": 0.000989805907011032}, {"feature": "HSI-F_day-1^2", "coef": 0.00023245066404342651}, {"feature": "KOSPI-F_day-1", "coef": -0.0004986710846424103}, {"feature": "KOSPI-F_day-1^2", "coef": -0.0006449585780501366}, {"feature": "NASDAQ-F_day-1", "coef": 0.0009332448244094849}, {"feature": "NASDAQ-F_day-1^2", "coef": 0.0026422925293445587}, {"feature": "GAS-F_day-1", "coef": -0.0006234496831893921}, {"feature": "GAS-F_day-1^2", "coef": -0.0002654418349266052}, {"feature": "Nikkei-F_day-1", "coef": 0.0001490972936153412}, {"feature": "Nikkei-F_day-1^2", "coef": -0.00019199587404727936}, {"feature": "NZD_day-1", "coef": -0.00030080974102020264}, {"feature": "NZD_day-1^2", "coef": 0.00017395149916410446}, {"feature": "silver-F_day-1", "coef": -0.0006037130951881409}, {"feature": "silver-F_day-1^2", "coef": -0.000993054360151291}, {"feature": "RUSSELL-F_day-1", "coef": -0.00038491934537887573}, {"feature": "RUSSELL-F_day-1^2", "coef": -0.004435921087861061}, {"feature": "S&P-F_day-1", "coef": -0.003980053588747978}, {"feature": "S&P-F_day-1^2", "coef": 0.004175491631031036}, {"feature": "CHF_day-1", "coef": 0.0004946794360876083}, {"feature": "CHF_day-1^2", "coef": 0.0003550490364432335}, {"feature": "Dollar index-F_day-1", "coef": -0.004553710110485554}, {"feature": "Dollar index-F_day-1^2", "coef": 0.0014365818351507187}, {"feature": "Dollar index_day-1", "coef": 0.0011374043533578515}, {"feature": "Dollar index_day-1^2", "coef": -0.0005352981388568878}, {"feature": "wheat-F_day-1", "coef": 9.96394082903862e-05}, {"feature": "wheat-F_day-1^2", "coef": 0.00024816859513521194}, {"feature": "XAG_day-1", "coef": -3.326684236526489e-05}, {"feature": "XAG_day-1^2", "coef": 0.001345362514257431}, {"feature": "XAU_day-1", "coef": 0.0008289702236652374}, {"feature": "XAU_day-1^2", "coef": 0.0003191828727722168}, {"feature": "day_of_week_day-1", "coef": -0.009389213752001524}, {"feature": "day_of_week_day-1^2", "coef": 0.004719394724816084}, {"feature": "day_of_month_day-1", "coef": -0.002790527418255806}, {"feature": "day_of_month_day-1^2", "coef": 0.003527827560901642}, {"feature": "month_day-1", "coef": 0.00043641775846481323}, {"feature": "month_day-1^2", "coef": 4.334747791290283e-05}, {"feature": "Close_ROC_day-1", "coef": 144019.50302228332}, {"feature": "Close_ROC_day-1^2", "coef": -108345.87951386534}, {"feature": "EMA10_ROC_day-1", "coef": -7.0713668540120125}, {"feature": "EMA10_ROC_day-1^2", "coef": 0.14414798095822334}, {"feature": "EMA20_ROC_day-1", "coef": 4.693619875703007}, {"feature": "EMA20_ROC_day-1^2", "coef": -0.11144351400434971}, {"feature": "EMA50_ROC_day-1", "coef": -1.501322197727859}, {"feature": "EMA50_ROC_day-1^2", "coef": 0.039578283205628395}, {"feature": "Close_day-2", "coef": -75807685.88746005}, {"feature": "Close_day-2^2", "coef": 2.9946069456636906}, {"feature": "Volume_day-2", "coef": 0.00037562265060842037}, {"feature": "Volume_day-2^2", "coef": -0.0007501561194658279}, {"feature": "mom_day-2", "coef": 383694284.18611807}, {"feature": "mom_day-2^2", "coef": 191701348.86625674}, {"feature": "mom1_day-2", "coef": -918530914.4835702}, {"feature": "mom1_day-2^2", "coef": 273091192.76835537}, {"feature": "mom2_day-2", "coef": 362502911.7447524}, {"feature": "mom2_day-2^2", "coef": 152348178.03080076}, {"feature": "mom3_day-2", "coef": 477554878.45976317}, {"feature": "mom3_day-2^2", "coef": -535554407.9841377}, {"feature": "ROC_5_day-2", "coef": -0.5573123907670379}, {"feature": "ROC_5_day-2^2", "coef": 0.014060087502002716}, {"feature": "ROC_10_day-2", "coef": 0.024372171610593796}, {"feature": "ROC_10_day-2^2", "coef": -0.002650149166584015}, {"feature": "ROC_15_day-2", "coef": 0.008747510612010956}, {"feature": "ROC_15_day-2^2", "coef": -0.0016333479434251785}, {"feature": "ROC_20_day-2", "coef": 0.007035501301288605}, {"feature": "ROC_20_day-2^2", "coef": -0.0016705002635717392}, {"feature": "EMA_10_day-2", "coef": 348745009.7341055}, {"feature": "EMA_10_day-2^2", "coef": -49.9560316093266}, {"feature": "EMA_20_day-2", "coef": 254087698.23748362}, {"feature": "EMA_20_day-2^2", "coef": 49.013466507196426}, {"feature": "EMA_50_day-2", "coef": 255770365.37650758}, {"feature": "EMA_50_day-2^2", "coef": 2.6131651774048805}, {"feature": "DTB4WK_day-2", "coef": -1383670101.4629586}, {"feature": "DTB4WK_day-2^2", "coef": -0.054357719607651234}, {"feature": "DTB3_day-2", "coef": 1594538515.9516468}, {"feature": "DTB3_day-2^2", "coef": 0.10060481284745038}, {"feature": "DTB6_day-2", "coef": -250301274.65912238}, {"feature": "DTB6_day-2^2", "coef": -0.15679307747632265}, {"feature": "DGS5_day-2", "coef": 0.009195536375045776}, {"feature": "DGS5_day-2^2", "coef": -0.00835467129945755}, {"feature": "DGS10_day-2", "coef": 502880166.5751054}, {"feature": "DGS10_day-2^2", "coef": -0.18015822488814592}, {"feature": "Oil_day-2", "coef": -0.00199749693274498}, {"feature": "Oil_day-2^2", "coef": 0.018061901442706585}, {"feature": "Gold_day-2", "coef": -0.014476068317890167}, {"feature": "Gold_day-2^2", "coef": -0.014129078015685081}, {"feature": "DAAA_day-2", "coef": -834795430.6628444}, {"feature": "DAAA_day-2^2", "coef": -0.012652818113565445}, {"feature": "DBAA_day-2", "coef": 760811340.2471797}, {"feature": "DBAA_day-2^2", "coef": 0.4728158116340637}, {"feature": "GBP_day-2", "coef": -0.00043254345655441284}, {"feature": "GBP_day-2^2", "coef": -0.0005928594619035721}, {"feature": "JPY_day-2", "coef": 0.00043145474046468735}, {"feature": "JPY_day-2^2", "coef": -0.0002163369208574295}, {"feature": "CAD_day-2", "coef": -0.0003310106694698334}, {"feature": "CAD_day-2^2", "coef": -0.00021516159176826477}, {"feature": "CNY_day-2", "coef": -0.00016382057219743729}, {"feature": "CNY_day-2^2", "coef": -8.884444832801819e-05}, {"feature": "AAPL_day-2", "coef": 0.0006647743284702301}, {"feature": "AAPL_day-2^2", "coef": 0.00025546178221702576}, {"feature": "AMZN_day-2", "coef": 0.0005990210920572281}, {"feature": "AMZN_day-2^2", "coef": 0.0001551383174955845}, {"feature": "GE_day-2", "coef": 0.00035700947046279907}, {"feature": "GE_day-2^2", "coef": -0.00045442255213856697}, {"feature": "JNJ_day-2", "coef": -0.0005690138787031174}, {"feature": "JNJ_day-2^2", "coef": -0.0006820634007453918}, {"feature": "JPM_day-2", "coef": -0.000565672293305397}, {"feature": "JPM_day-2^2", "coef": 0.0006941058672964573}, {"feature": "MSFT_day-2", "coef": 0.00022526271641254425}, {"feature": "MSFT_day-2^2", "coef": -0.0003784000873565674}, {"feature": "WFC_day-2", "coef": -0.00067940354347229}, {"feature": "WFC_day-2^2", "coef": 0.0001481175422668457}, {"feature": "XOM_day-2", "coef": 0.00047007761895656586}, {"feature": "XOM_day-2^2", "coef": 1.4606863260269165e-05}, {"feature": "FCHI_day-2", "coef": -0.0017845071852207184}, {"feature": "FCHI_day-2^2", "coef": -0.0030040442943573}, {"feature": "FTSE_day-2", "coef": 0.0027419831603765488}, {"feature": "FTSE_day-2^2", "coef": -0.00041791610419750214}, {"feature": "GDAXI_day-2", "coef": -0.0001304037868976593}, {"feature": "GDAXI_day-2^2", "coef": -0.00011768192052841187}, {"feature": "GSPC_day-2", "coef": 0.0012395530939102173}, {"feature": "GSPC_day-2^2", "coef": 0.0023339390754699707}, {"feature": "HSI_day-2", "coef": 0.0007980577647686005}, {"feature": "HSI_day-2^2", "coef": -0.0005559884011745453}, {"feature": "DJI_day-2", "coef": -0.0019782260060310364}, {"feature": "DJI_day-2^2", "coef": 0.002247784286737442}, {"feature": "SSEC_day-2", "coef": 9.084679186344147e-05}, {"feature": "SSEC_day-2^2", "coef": -7.577985525131226e-05}, {"feature": "RUT_day-2", "coef": 0.0018198154866695404}, {"feature": "RUT_day-2^2", "coef": -0.0020045097917318344}, {"feature": "NYSE_day-2", "coef": -0.0031540654599666595}, {"feature": "NYSE_day-2^2", "coef": -0.005663519725203514}, {"feature": "TE1_day-2", "coef": 290933191.09541875}, {"feature": "TE1_day-2^2", "coef": -0.05281289527192712}, {"feature": "TE2_day-2", "coef": -321072633.5914944}, {"feature": "TE2_day-2^2", "coef": -0.6036363756284118}, {"feature": "TE3_day-2", "coef": -183294853.2206438}, {"feature": "TE3_day-2^2", "coef": 0.8447262234985828}, {"feature": "TE5_day-2", "coef": -347278481.5243455}, {"feature": "TE5_day-2^2", "coef": 0.005045613273978233}, {"feature": "TE6_day-2", "coef": 56118322.23852399}, {"feature": "TE6_day-2^2", "coef": -0.0037756338715553284}, {"feature": "DE1_day-2", "coef": -358220901.241258}, {"feature": "DE1_day-2^2", "coef": 0.05765906721353531}, {"feature": "DE2_day-2", "coef": 265414735.59095064}, {"feature": "DE2_day-2^2", "coef": -0.025604285299777985}, {"feature": "DE4_day-2", "coef": 93468839.34378095}, {"feature": "DE4_day-2^2", "coef": -1.6458143517374992}, {"feature": "DE5_day-2", "coef": -873863724.432195}, {"feature": "DE5_day-2^2", "coef": 0.995124876499176}, {"feature": "DE6_day-2", "coef": 514246104.7647128}, {"feature": "DE6_day-2^2", "coef": 0.026923298835754395}, {"feature": "CTB3M_day-2", "coef": -3.540888428688049e-05}, {"feature": "CTB3M_day-2^2", "coef": -6.971880793571472e-06}, {"feature": "CTB6M_day-2", "coef": -0.0013947635889053345}, {"feature": "CTB6M_day-2^2", "coef": 0.012109164148569107}, {"feature": "CTB1Y_day-2", "coef": 0.003055538982152939}, {"feature": "CTB1Y_day-2^2", "coef": 0.00101378932595253}, {"feature": "AUD_day-2", "coef": -0.00120602548122406}, {"feature": "AUD_day-2^2", "coef": -0.00010010041296482086}, {"feature": "Brent_day-2", "coef": 0.0012975260615348816}, {"feature": "Brent_day-2^2", "coef": -6.650015711784363e-05}, {"feature": "CAC-F_day-2", "coef": 0.00263286754488945}, {"feature": "CAC-F_day-2^2", "coef": 0.004097562283277512}, {"feature": "copper-F_day-2", "coef": 0.0001645740121603012}, {"feature": "copper-F_day-2^2", "coef": 9.651388972997665e-05}, {"feature": "WIT-oil_day-2", "coef": 2.85319983959198e-05}, {"feature": "WIT-oil_day-2^2", "coef": -0.00027860887348651886}, {"feature": "DAX-F_day-2", "coef": -0.000769028440117836}, {"feature": "DAX-F_day-2^2", "coef": 0.0002883113920688629}, {"feature": "DJI-F_day-2", "coef": 0.0030516423285007477}, {"feature": "DJI-F_day-2^2", "coef": 0.0016230233013629913}, {"feature": "EUR_day-2", "coef": -0.0004994070623070002}, {"feature": "EUR_day-2^2", "coef": -0.0005285218358039856}, {"feature": "FTSE-F_day-2", "coef": -0.0017654076218605042}, {"feature": "FTSE-F_day-2^2", "coef": 0.0002823546528816223}, {"feature": "gold-F_day-2", "coef": 0.0016104131937026978}, {"feature": "gold-F_day-2^2", "coef": -0.000539831817150116}, {"feature": "HSI-F_day-2", "coef": -0.0006639696657657623}, {"feature": "HSI-F_day-2^2", "coef": 0.0016282573342323303}, {"feature": "KOSPI-F_day-2", "coef": -0.000386139377951622}, {"feature": "KOSPI-F_day-2^2", "coef": -4.973728209733963e-05}, {"feature": "NASDAQ-F_day-2", "coef": -0.002136964350938797}, {"feature": "NASDAQ-F_day-2^2", "coef": -0.0011135507375001907}, {"feature": "GAS-F_day-2", "coef": 0.0002925209701061249}, {"feature": "GAS-F_day-2^2", "coef": -0.00027150847017765045}, {"feature": "Nikkei-F_day-2", "coef": 8.332915604114532e-05}, {"feature": "Nikkei-F_day-2^2", "coef": 0.0005513830110430717}, {"feature": "NZD_day-2", "coef": 5.4951757192611694e-05}, {"feature": "NZD_day-2^2", "coef": -0.0008993875235319138}, {"feature": "silver-F_day-2", "coef": -0.0019290223717689514}, {"feature": "silver-F_day-2^2", "coef": 0.00026347115635871887}, {"feature": "RUSSELL-F_day-2", "coef": -0.0020178742706775665}, {"feature": "RUSSELL-F_day-2^2", "coef": 0.0005470989271998405}, {"feature": "S&P-F_day-2", "coef": 0.0047528743743896484}, {"feature": "S&P-F_day-2^2", "coef": 2.329796552658081e-05}, {"feature": "CHF_day-2", "coef": -0.00016176514327526093}, {"feature": "CHF_day-2^2", "coef": -0.0004685204476118088}, {"feature": "Dollar index-F_day-2", "coef": 0.0005632899701595306}, {"feature": "Dollar index-F_day-2^2", "coef": 0.000276416540145874}, {"feature": "Dollar index_day-2", "coef": -0.001976948231458664}, {"feature": "Dollar index_day-2^2", "coef": 0.0016947127878665924}, {"feature": "wheat-F_day-2", "coef": -9.463541209697723e-05}, {"feature": "wheat-F_day-2^2", "coef": -0.00017610564827919006}, {"feature": "XAG_day-2", "coef": 0.0017891526222229004}, {"feature": "XAG_day-2^2", "coef": 3.2354146242141724e-05}, {"feature": "XAU_day-2", "coef": 2.3305416107177734e-05}, {"feature": "XAU_day-2^2", "coef": 4.7013163566589355e-05}, {"feature": "day_of_week_day-2", "coef": 0.013465697411447763}, {"feature": "day_of_week_day-2^2", "coef": -0.016475664917379618}, {"feature": "day_of_month_day-2", "coef": 0.006217213347554207}, {"feature": "day_of_month_day-2^2", "coef": -0.006683306768536568}, {"feature": "month_day-2", "coef": 0.0040077269077301025}, {"feature": "month_day-2^2", "coef": -0.00505124032497406}, {"feature": "Close_ROC_day-2", "coef": -160918.66881597787}, {"feature": "Close_ROC_day-2^2", "coef": -225273.0993350763}, {"feature": "EMA10_ROC_day-2", "coef": 2.755608918145299}, {"feature": "EMA10_ROC_day-2^2", "coef": -0.0724322609603405}, {"feature": "EMA20_ROC_day-2", "coef": -8.044079966843128}, {"feature": "EMA20_ROC_day-2^2", "coef": 0.18034511618316174}, {"feature": "EMA50_ROC_day-2", "coef": 4.398571344092488}, {"feature": "EMA50_ROC_day-2^2", "coef": -0.10508636757731438}, {"feature": "Close_day-3", "coef": -77284165.70033914}, {"feature": "Close_day-3^2", "coef": 2.6767017878592014}, {"feature": "Volume_day-3", "coef": -7.79964029788971e-05}, {"feature": "Volume_day-3^2", "coef": 0.00014198943972587585}, {"feature": "mom_day-3", "coef": 585908410.2929596}, {"feature": "mom_day-3^2", "coef": -236372760.33272845}, {"feature": "mom1_day-3", "coef": -388973874.7960297}, {"feature": "mom1_day-3^2", "coef": 246040600.51407182}, {"feature": "mom2_day-3", "coef": -79389404.21277836}, {"feature": "mom2_day-3^2", "coef": -79384443.94570775}, {"feature": "mom3_day-3", "coef": 67623478.61197807}, {"feature": "mom3_day-3^2", "coef": 530399462.439049}, {"feature": "ROC_5_day-3", "coef": -0.46065568551421165}, {"feature": "ROC_5_day-3^2", "coef": 0.009548002853989601}, {"feature": "ROC_10_day-3", "coef": 0.01895102858543396}, {"feature": "ROC_10_day-3^2", "coef": -0.00010768696665763855}, {"feature": "ROC_15_day-3", "coef": 0.008874371647834778}, {"feature": "ROC_15_day-3^2", "coef": 0.0005307039245963097}, {"feature": "ROC_20_day-3", "coef": 0.0010964199900627136}, {"feature": "ROC_20_day-3^2", "coef": -0.0009894026443362236}, {"feature": "EMA_10_day-3", "coef": 131660047.30468334}, {"feature": "EMA_10_day-3^2", "coef": 22.675750575959682}, {"feature": "EMA_20_day-3", "coef": -174827359.57263446}, {"feature": "EMA_20_day-3^2", "coef": -96.95287048816681}, {"feature": "EMA_50_day-3", "coef": 138989135.82506678}, {"feature": "EMA_50_day-3^2", "coef": 93.7246974259615}, {"feature": "DTB4WK_day-3", "coef": 228047130.4648697}, {"feature": "DTB4WK_day-3^2", "coef": -0.01240027928724885}, {"feature": "DTB3_day-3", "coef": -451970514.0019256}, {"feature": "DTB3_day-3^2", "coef": 0.01775736128911376}, {"feature": "DTB6_day-3", "coef": 350800372.7187015}, {"feature": "DTB6_day-3^2", "coef": -0.010996454861015081}, {"feature": "DGS5_day-3", "coef": 0.03884144499897957}, {"feature": "DGS5_day-3^2", "coef": -0.041487917304039}, {"feature": "DGS10_day-3", "coef": -283458981.6168839}, {"feature": "DGS10_day-3^2", "coef": 0.2317467387765646}, {"feature": "Oil_day-3", "coef": 0.0004379376769065857}, {"feature": "Oil_day-3^2", "coef": 0.0008141137659549713}, {"feature": "Gold_day-3", "coef": -0.016300905495882034}, {"feature": "Gold_day-3^2", "coef": -0.017170971259474754}, {"feature": "DAAA_day-3", "coef": 43123790.92772673}, {"feature": "DAAA_day-3^2", "coef": 0.07530312612652779}, {"feature": "DBAA_day-3", "coef": -11436898.97126093}, {"feature": "DBAA_day-3^2", "coef": -0.20981403440237045}, {"feature": "GBP_day-3", "coef": -0.0005792900919914246}, {"feature": "GBP_day-3^2", "coef": 0.00034802593290805817}, {"feature": "JPY_day-3", "coef": 5.131028592586517e-05}, {"feature": "JPY_day-3^2", "coef": 0.00046959519386291504}, {"feature": "CAD_day-3", "coef": 0.0005826093256473541}, {"feature": "CAD_day-3^2", "coef": -0.00033472850918769836}, {"feature": "CNY_day-3", "coef": -0.00010815169662237167}, {"feature": "CNY_day-3^2", "coef": -0.0004053395241498947}, {"feature": "AAPL_day-3", "coef": 0.000170242041349411}, {"feature": "AAPL_day-3^2", "coef": 0.00010135956108570099}, {"feature": "AMZN_day-3", "coef": -8.638016879558563e-05}, {"feature": "AMZN_day-3^2", "coef": -0.0007657711394131184}, {"feature": "GE_day-3", "coef": -0.00024923868477344513}, {"feature": "GE_day-3^2", "coef": 0.00012518186122179031}, {"feature": "JNJ_day-3", "coef": -0.00019694678485393524}, {"feature": "JNJ_day-3^2", "coef": -7.63842836022377e-05}, {"feature": "JPM_day-3", "coef": -5.6313350796699524e-05}, {"feature": "JPM_day-3^2", "coef": 4.2829662561416626e-05}, {"feature": "MSFT_day-3", "coef": 0.000279400497674942}, {"feature": "MSFT_day-3^2", "coef": -0.0005254135467112064}, {"feature": "WFC_day-3", "coef": -0.0002330988645553589}, {"feature": "WFC_day-3^2", "coef": 0.000447896309196949}, {"feature": "XOM_day-3", "coef": -0.0007368177175521851}, {"feature": "XOM_day-3^2", "coef": 0.00047678127884864807}, {"feature": "FCHI_day-3", "coef": -0.0002966243773698807}, {"feature": "FCHI_day-3^2", "coef": -0.0031710625626146793}, {"feature": "FTSE_day-3", "coef": 0.0009711179882287979}, {"feature": "FTSE_day-3^2", "coef": 0.0011170804500579834}, {"feature": "GDAXI_day-3", "coef": -0.0009837746620178223}, {"feature": "GDAXI_day-3^2", "coef": -0.00033015385270118713}, {"feature": "GSPC_day-3", "coef": 0.009239859879016876}, {"feature": "GSPC_day-3^2", "coef": -0.009738260880112648}, {"feature": "HSI_day-3", "coef": 0.00031357258558273315}, {"feature": "HSI_day-3^2", "coef": 0.00023165903985500336}, {"feature": "DJI_day-3", "coef": -0.009073926135897636}, {"feature": "DJI_day-3^2", "coef": 0.0014072027988731861}, {"feature": "SSEC_day-3", "coef": 7.205549627542496e-05}, {"feature": "SSEC_day-3^2", "coef": -0.0003003925085067749}, {"feature": "RUT_day-3", "coef": -0.0067836809903383255}, {"feature": "RUT_day-3^2", "coef": -0.004568019881844521}, {"feature": "NYSE_day-3", "coef": 0.0029199831187725067}, {"feature": "NYSE_day-3^2", "coef": 0.0023909732699394226}, {"feature": "TE1_day-3", "coef": -179739846.19311273}, {"feature": "TE1_day-3^2", "coef": 0.05559529550373554}, {"feature": "TE2_day-3", "coef": 344793684.9053042}, {"feature": "TE2_day-3^2", "coef": 0.04963076580315828}, {"feature": "TE3_day-3", "coef": -367253684.2894886}, {"feature": "TE3_day-3^2", "coef": -0.27718450129032135}, {"feature": "TE5_day-3", "coef": 85097831.79756296}, {"feature": "TE5_day-3^2", "coef": 0.0007902132347226143}, {"feature": "TE6_day-3", "coef": -115281738.01959485}, {"feature": "TE6_day-3^2", "coef": -8.102506399154663e-05}, {"feature": "DE1_day-3", "coef": 18482519.882233858}, {"feature": "DE1_day-3^2", "coef": -0.014930546283721924}, {"feature": "DE2_day-3", "coef": -372744568.399951}, {"feature": "DE2_day-3^2", "coef": -0.04253549128770828}, {"feature": "DE4_day-3", "coef": 288947553.305619}, {"feature": "DE4_day-3^2", "coef": 0.5664165653288364}, {"feature": "DE5_day-3", "coef": -305309134.1321261}, {"feature": "DE5_day-3^2", "coef": -0.2326640821993351}, {"feature": "DE6_day-3", "coef": 617188762.6242898}, {"feature": "DE6_day-3^2", "coef": -0.15546122193336487}, {"feature": "CTB3M_day-3", "coef": -0.0002581104636192322}, {"feature": "CTB3M_day-3^2", "coef": -0.00032882019877433777}, {"feature": "CTB6M_day-3", "coef": 0.0007001124322414398}, {"feature": "CTB6M_day-3^2", "coef": -0.0047569796442985535}, {"feature": "CTB1Y_day-3", "coef": 0.0005367137491703033}, {"feature": "CTB1Y_day-3^2", "coef": 0.008854608982801437}, {"feature": "AUD_day-3", "coef": 0.0009032906964421272}, {"feature": "AUD_day-3^2", "coef": 0.000527605414390564}, {"feature": "Brent_day-3", "coef": -0.0005187578499317169}, {"feature": "Brent_day-3^2", "coef": -0.00019360147416591644}, {"feature": "CAC-F_day-3", "coef": -0.0007996726781129837}, {"feature": "CAC-F_day-3^2", "coef": 0.003876021131873131}, {"feature": "copper-F_day-3", "coef": 0.00047255679965019226}, {"feature": "copper-F_day-3^2", "coef": -0.001037820242345333}, {"feature": "WIT-oil_day-3", "coef": 0.0005652382969856262}, {"feature": "WIT-oil_day-3^2", "coef": 0.0005322284996509552}, {"feature": "DAX-F_day-3", "coef": 0.0029380060732364655}, {"feature": "DAX-F_day-3^2", "coef": -0.0010438906028866768}, {"feature": "DJI-F_day-3", "coef": 0.007751896046102047}, {"feature": "DJI-F_day-3^2", "coef": 0.002399367280304432}, {"feature": "EUR_day-3", "coef": -0.002269144169986248}, {"feature": "EUR_day-3^2", "coef": -0.0002872757613658905}, {"feature": "FTSE-F_day-3", "coef": -0.002099001780152321}, {"feature": "FTSE-F_day-3^2", "coef": 0.0002945670858025551}, {"feature": "gold-F_day-3", "coef": 0.0013266205787658691}, {"feature": "gold-F_day-3^2", "coef": -0.00017011910676956177}, {"feature": "HSI-F_day-3", "coef": -0.0010793805122375488}, {"feature": "HSI-F_day-3^2", "coef": -0.0006770789623260498}, {"feature": "KOSPI-F_day-3", "coef": 0.00044076889753341675}, {"feature": "KOSPI-F_day-3^2", "coef": -9.541027247905731e-05}, {"feature": "NASDAQ-F_day-3", "coef": -0.0005072169005870819}, {"feature": "NASDAQ-F_day-3^2", "coef": -0.001517278142273426}, {"feature": "GAS-F_day-3", "coef": -0.00010636448860168457}, {"feature": "GAS-F_day-3^2", "coef": 0.0006561111658811569}, {"feature": "Nikkei-F_day-3", "coef": 0.00010642781853675842}, {"feature": "Nikkei-F_day-3^2", "coef": 0.0006805770099163055}, {"feature": "NZD_day-3", "coef": -0.0003140568733215332}, {"feature": "NZD_day-3^2", "coef": 0.0010944008827209473}, {"feature": "silver-F_day-3", "coef": -0.002067524939775467}, {"feature": "silver-F_day-3^2", "coef": -0.0008770227432250977}, {"feature": "RUSSELL-F_day-3", "coef": 0.005265264771878719}, {"feature": "RUSSELL-F_day-3^2", "coef": 0.0021183472126722336}, {"feature": "S&P-F_day-3", "coef": -0.006474180147051811}, {"feature": "S&P-F_day-3^2", "coef": -0.0027883253060281277}, {"feature": "CHF_day-3", "coef": -0.0007003983482718468}, {"feature": "CHF_day-3^2", "coef": -0.0007102694362401962}, {"feature": "Dollar index-F_day-3", "coef": -0.00016117095947265625}, {"feature": "Dollar index-F_day-3^2", "coef": 3.080815076828003e-06}, {"feature": "Dollar index_day-3", "coef": -0.0018652956932783127}, {"feature": "Dollar index_day-3^2", "coef": -4.4461339712142944e-05}, {"feature": "wheat-F_day-3", "coef": 1.0921154171228409e-05}, {"feature": "wheat-F_day-3^2", "coef": -0.0003221556544303894}, {"feature": "XAG_day-3", "coef": 0.002048775553703308}, {"feature": "XAG_day-3^2", "coef": 0.0003935806453227997}, {"feature": "XAU_day-3", "coef": -0.0005798190832138062}, {"feature": "XAU_day-3^2", "coef": 0.0006465669721364975}, {"feature": "day_of_week_day-3", "coef": -0.004127829102799296}, {"feature": "day_of_week_day-3^2", "coef": 0.005991741083562374}, {"feature": "day_of_month_day-3", "coef": -0.007618613541126251}, {"feature": "day_of_month_day-3^2", "coef": 0.008576605468988419}, {"feature": "month_day-3", "coef": 0.003628060221672058}, {"feature": "month_day-3^2", "coef": -0.002843007445335388}, {"feature": "Close_ROC_day-3", "coef": -63882.19261011481}, {"feature": "Close_ROC_day-3^2", "coef": 88222.89086028002}, {"feature": "EMA10_ROC_day-3", "coef": 3.580497033894062}, {"feature": "EMA10_ROC_day-3^2", "coef": -0.07424058206379414}, {"feature": "EMA20_ROC_day-3", "coef": 0.23837607353925705}, {"feature": "EMA20_ROC_day-3^2", "coef": -0.020344261080026627}, {"feature": "EMA50_ROC_day-3", "coef": -0.3091074004769325}, {"feature": "EMA50_ROC_day-3^2", "coef": 0.01901930570602417}, {"feature": "Close_day-4", "coef": -28250000.005939066}, {"feature": "Close_day-4^2", "coef": -2.94284438341856}, {"feature": "Volume_day-4", "coef": 0.0003628944978117943}, {"feature": "Volume_day-4^2", "coef": 0.0002315174788236618}, {"feature": "mom_day-4", "coef": -116249588.32532002}, {"feature": "mom_day-4^2", "coef": 622557977.3596575}, {"feature": "mom1_day-4", "coef": -194127720.58095887}, {"feature": "mom1_day-4^2", "coef": 288146496.6496242}, {"feature": "mom2_day-4", "coef": -439620156.95835835}, {"feature": "mom2_day-4^2", "coef": 191783593.2892548}, {"feature": "mom3_day-4", "coef": 282384558.144998}, {"feature": "mom3_day-4^2", "coef": 91319742.6018894}, {"feature": "ROC_5_day-4", "coef": 0.5649811401963234}, {"feature": "ROC_5_day-4^2", "coef": -0.016411921940743923}, {"feature": "ROC_10_day-4", "coef": 0.014558158814907074}, {"feature": "ROC_10_day-4^2", "coef": -5.273893475532532e-05}, {"feature": "ROC_15_day-4", "coef": 0.0063417404890060425}, {"feature": "ROC_15_day-4^2", "coef": -0.003427456133067608}, {"feature": "ROC_20_day-4", "coef": -0.003351029008626938}, {"feature": "ROC_20_day-4^2", "coef": 0.0026596542447805405}, {"feature": "EMA_10_day-4", "coef": -269673271.9318088}, {"feature": "EMA_10_day-4^2", "coef": 10.30015273205936}, {"feature": "EMA_20_day-4", "coef": 198795356.71185938}, {"feature": "EMA_20_day-4^2", "coef": 147.28537033125758}, {"feature": "EMA_50_day-4", "coef": -160457034.9276193}, {"feature": "EMA_50_day-4^2", "coef": -263.19667434319854}, {"feature": "DTB4WK_day-4", "coef": -392276175.4481637}, {"feature": "DTB4WK_day-4^2", "coef": 0.016144127352163196}, {"feature": "DTB3_day-4", "coef": 101167300.2728505}, {"feature": "DTB3_day-4^2", "coef": -0.037320086965337396}, {"feature": "DTB6_day-4", "coef": 19569450.840826374}, {"feature": "DTB6_day-4^2", "coef": 0.03188132308423519}, {"feature": "DGS5_day-4", "coef": -0.019204236567020416}, {"feature": "DGS5_day-4^2", "coef": 0.02181611955165863}, {"feature": "DGS10_day-4", "coef": 284032487.51400024}, {"feature": "DGS10_day-4^2", "coef": -0.4775109551846981}, {"feature": "Oil_day-4", "coef": -0.0031149163842201233}, {"feature": "Oil_day-4^2", "coef": 0.005955368746072054}, {"feature": "Gold_day-4", "coef": -0.004040885716676712}, {"feature": "Gold_day-4^2", "coef": -0.003385397605597973}, {"feature": "DAAA_day-4", "coef": 305973036.5392222}, {"feature": "DAAA_day-4^2", "coef": -0.05747058615088463}, {"feature": "DBAA_day-4", "coef": 111678769.24616368}, {"feature": "DBAA_day-4^2", "coef": 0.9909527115523815}, {"feature": "GBP_day-4", "coef": -0.000171770341694355}, {"feature": "GBP_day-4^2", "coef": -0.00018980680033564568}, {"feature": "JPY_day-4", "coef": -0.00011500157415866852}, {"feature": "JPY_day-4^2", "coef": 0.00040265079587697983}, {"feature": "CAD_day-4", "coef": -0.00029108859598636627}, {"feature": "CAD_day-4^2", "coef": -0.0003326255828142166}, {"feature": "CNY_day-4", "coef": -0.0008569173514842987}, {"feature": "CNY_day-4^2", "coef": 0.0008167698979377747}, {"feature": "AAPL_day-4", "coef": 0.0004908349364995956}, {"feature": "AAPL_day-4^2", "coef": -0.0004638843238353729}, {"feature": "AMZN_day-4", "coef": 0.00033185258507728577}, {"feature": "AMZN_day-4^2", "coef": -0.000496966764330864}, {"feature": "GE_day-4", "coef": 0.00041957758367061615}, {"feature": "GE_day-4^2", "coef": -0.0005631335079669952}, {"feature": "JNJ_day-4", "coef": 0.0004180390387773514}, {"feature": "JNJ_day-4^2", "coef": 1.6141682863235474e-05}, {"feature": "JPM_day-4", "coef": 0.0012872982770204544}, {"feature": "JPM_day-4^2", "coef": -0.0004406142979860306}, {"feature": "MSFT_day-4", "coef": 3.681890666484833e-05}, {"feature": "MSFT_day-4^2", "coef": -4.763901233673096e-05}, {"feature": "WFC_day-4", "coef": -0.0015492159873247147}, {"feature": "WFC_day-4^2", "coef": 0.0011436622589826584}, {"feature": "XOM_day-4", "coef": 0.001043066382408142}, {"feature": "XOM_day-4^2", "coef": -0.0009477343410253525}, {"feature": "FCHI_day-4", "coef": -3.923103213310242e-05}, {"feature": "FCHI_day-4^2", "coef": -4.4889748096466064e-05}, {"feature": "FTSE_day-4", "coef": 0.003084201365709305}, {"feature": "FTSE_day-4^2", "coef": 1.4399178326129913e-05}, {"feature": "GDAXI_day-4", "coef": -0.0016868636012077332}, {"feature": "GDAXI_day-4^2", "coef": 0.0004355022683739662}, {"feature": "GSPC_day-4", "coef": -0.006248071789741516}, {"feature": "GSPC_day-4^2", "coef": -0.001902647316455841}, {"feature": "HSI_day-4", "coef": -0.0003231912851333618}, {"feature": "HSI_day-4^2", "coef": 0.00024850107729434967}, {"feature": "DJI_day-4", "coef": 0.00434424914419651}, {"feature": "DJI_day-4^2", "coef": 0.0006846822798252106}, {"feature": "SSEC_day-4", "coef": 0.0007951701991260052}, {"feature": "SSEC_day-4^2", "coef": -0.00019536912441253662}, {"feature": "RUT_day-4", "coef": 0.0006849654018878937}, {"feature": "RUT_day-4^2", "coef": 0.0005839653313159943}, {"feature": "NYSE_day-4", "coef": -0.0024678613990545273}, {"feature": "NYSE_day-4^2", "coef": 0.004897576756775379}, {"feature": "TE1_day-4", "coef": -518328132.10355836}, {"feature": "TE1_day-4^2", "coef": 0.10925071500241756}, {"feature": "TE2_day-4", "coef": 179601207.4972118}, {"feature": "TE2_day-4^2", "coef": 0.36251926235854626}, {"feature": "TE3_day-4", "coef": -247009982.63159424}, {"feature": "TE3_day-4^2", "coef": 0.05188360624015331}, {"feature": "TE5_day-4", "coef": -41749137.34122702}, {"feature": "TE5_day-4^2", "coef": -0.0010468792170286179}, {"feature": "TE6_day-4", "coef": -18577946.00741197}, {"feature": "TE6_day-4^2", "coef": 0.0017766505479812622}, {"feature": "DE1_day-4", "coef": 132070162.21463996}, {"feature": "DE1_day-4^2", "coef": -0.009096905589103699}, {"feature": "DE2_day-4", "coef": -188515923.20893636}, {"feature": "DE2_day-4^2", "coef": -0.0018540695309638977}, {"feature": "DE4_day-4", "coef": 176285228.17820236}, {"feature": "DE4_day-4^2", "coef": -0.23632169887423515}, {"feature": "DE5_day-4", "coef": -642166703.6083906}, {"feature": "DE5_day-4^2", "coef": -0.34999024495482445}, {"feature": "DE6_day-4", "coef": 264162879.8463114}, {"feature": "DE6_day-4^2", "coef": -0.5426325835287571}, {"feature": "CTB3M_day-4", "coef": -0.00040435604751110077}, {"feature": "CTB3M_day-4^2", "coef": 0.0006413534283638}, {"feature": "CTB6M_day-4", "coef": 0.001965835690498352}, {"feature": "CTB6M_day-4^2", "coef": -0.010021854192018509}, {"feature": "CTB1Y_day-4", "coef": -0.00020871683955192566}, {"feature": "CTB1Y_day-4^2", "coef": 0.017520077526569366}, {"feature": "AUD_day-4", "coef": -0.0005428055301308632}, {"feature": "AUD_day-4^2", "coef": 0.0002869442105293274}, {"feature": "Brent_day-4", "coef": 0.001428801566362381}, {"feature": "Brent_day-4^2", "coef": 0.0007073869928717613}, {"feature": "CAC-F_day-4", "coef": 0.0004973709583282471}, {"feature": "CAC-F_day-4^2", "coef": -0.0004431437700986862}, {"feature": "copper-F_day-4", "coef": 0.0001907534897327423}, {"feature": "copper-F_day-4^2", "coef": 0.00040507689118385315}, {"feature": "WIT-oil_day-4", "coef": 0.000993933528661728}, {"feature": "WIT-oil_day-4^2", "coef": -0.0007854858413338661}, {"feature": "DAX-F_day-4", "coef": 0.0012430697679519653}, {"feature": "DAX-F_day-4^2", "coef": 0.0001096157357096672}, {"feature": "DJI-F_day-4", "coef": -0.0035762181505560875}, {"feature": "DJI-F_day-4^2", "coef": -0.0002881241962313652}, {"feature": "EUR_day-4", "coef": 0.0002585155889391899}, {"feature": "EUR_day-4^2", "coef": -0.000689113512635231}, {"feature": "FTSE-F_day-4", "coef": -0.0033189672976732254}, {"feature": "FTSE-F_day-4^2", "coef": 7.397262379527092e-05}, {"feature": "gold-F_day-4", "coef": -0.000570341944694519}, {"feature": "gold-F_day-4^2", "coef": -0.0002814345061779022}, {"feature": "HSI-F_day-4", "coef": -0.00029806792736053467}, {"feature": "HSI-F_day-4^2", "coef": 0.0002855919301509857}, {"feature": "KOSPI-F_day-4", "coef": 0.0005958098918199539}, {"feature": "KOSPI-F_day-4^2", "coef": -0.0016303770244121552}, {"feature": "NASDAQ-F_day-4", "coef": -0.0020992550998926163}, {"feature": "NASDAQ-F_day-4^2", "coef": 0.0007476899772882462}, {"feature": "GAS-F_day-4", "coef": 0.000710783526301384}, {"feature": "GAS-F_day-4^2", "coef": -0.0003431239165365696}, {"feature": "Nikkei-F_day-4", "coef": 0.0004928391426801682}, {"feature": "Nikkei-F_day-4^2", "coef": 0.0003083162009716034}, {"feature": "NZD_day-4", "coef": 0.00042676832526922226}, {"feature": "NZD_day-4^2", "coef": 0.0009226677939295769}, {"feature": "silver-F_day-4", "coef": -0.0019916966557502747}, {"feature": "silver-F_day-4^2", "coef": 0.0008671656250953674}, {"feature": "RUSSELL-F_day-4", "coef": 0.0008941721171140671}, {"feature": "RUSSELL-F_day-4^2", "coef": -0.0011752191931009293}, {"feature": "S&P-F_day-4", "coef": 0.005265427753329277}, {"feature": "S&P-F_day-4^2", "coef": -0.0009593376889824867}, {"feature": "CHF_day-4", "coef": 0.00035227276384830475}, {"feature": "CHF_day-4^2", "coef": 0.0006528771482408047}, {"feature": "Dollar index-F_day-4", "coef": -0.001611866056919098}, {"feature": "Dollar index-F_day-4^2", "coef": 0.0006813444197177887}, {"feature": "Dollar index_day-4", "coef": 0.0011990759521722794}, {"feature": "Dollar index_day-4^2", "coef": -0.0004042387008666992}, {"feature": "wheat-F_day-4", "coef": -0.00011848052963614464}, {"feature": "wheat-F_day-4^2", "coef": -0.00035536475479602814}, {"feature": "XAG_day-4", "coef": 0.0012465491890907288}, {"feature": "XAG_day-4^2", "coef": -0.0003225468099117279}, {"feature": "XAU_day-4", "coef": 0.0018672123551368713}, {"feature": "XAU_day-4^2", "coef": -0.0001338031142950058}, {"feature": "day_of_week_day-4", "coef": 0.008553354069590569}, {"feature": "day_of_week_day-4^2", "coef": -0.007871118374168873}, {"feature": "day_of_month_day-4", "coef": 0.009744785726070404}, {"feature": "day_of_month_day-4^2", "coef": -0.009662142023444176}, {"feature": "month_day-4", "coef": 0.009276226162910461}, {"feature": "month_day-4^2", "coef": -0.01155400276184082}, {"feature": "Close_ROC_day-4", "coef": -38207.64955095202}, {"feature": "Close_ROC_day-4^2", "coef": 330500.6189621985}, {"feature": "EMA10_ROC_day-4", "coef": -3.2830913178622723}, {"feature": "EMA10_ROC_day-4^2", "coef": 0.06628019735217094}, {"feature": "EMA20_ROC_day-4", "coef": 6.12602886557579}, {"feature": "EMA20_ROC_day-4^2", "coef": -0.12533296225592494}, {"feature": "EMA50_ROC_day-4", "coef": -3.8532777950167656}, {"feature": "EMA50_ROC_day-4^2", "coef": 0.08612780272960663}, {"feature": "Close_day-5", "coef": 0.4615197442471981}, {"feature": "Close_day-5^2", "coef": -0.3011270649731159}, {"feature": "Volume_day-5", "coef": 0.000562573317438364}, {"feature": "Volume_day-5^2", "coef": -7.042661309242249e-05}, {"feature": "mom_day-5", "coef": -203795091.2827102}, {"feature": "mom_day-5^2", "coef": 326931383.1028312}, {"feature": "mom1_day-5", "coef": 371996678.6143616}, {"feature": "mom1_day-5^2", "coef": -722183055.7318076}, {"feature": "mom2_day-5", "coef": -282384558.158273}, {"feature": "mom2_day-5^2", "coef": -91319742.60119602}, {"feature": "mom3_day-5", "coef": -0.2458634153008461}, {"feature": "mom3_day-5^2", "coef": 0.0018729865550994873}, {"feature": "ROC_5_day-5", "coef": 0.02387484535574913}, {"feature": "ROC_5_day-5^2", "coef": 0.0011228229850530624}, {"feature": "ROC_10_day-5", "coef": 0.01043190062046051}, {"feature": "ROC_10_day-5^2", "coef": -0.0009961500763893127}, {"feature": "ROC_15_day-5", "coef": 0.0034149065613746643}, {"feature": "ROC_15_day-5^2", "coef": 0.0030695367604494095}, {"feature": "ROC_20_day-5", "coef": 0.008388679474592209}, {"feature": "ROC_20_day-5^2", "coef": -0.0014639124274253845}, {"feature": "EMA_10_day-5", "coef": -22360287.835592244}, {"feature": "EMA_10_day-5^2", "coef": -10.2882111184299}, {"feature": "EMA_20_day-5", "coef": -164056760.73050773}, {"feature": "EMA_20_day-5^2", "coef": -59.48809423670173}, {"feature": "EMA_50_day-5", "coef": -139140906.00711995}, {"feature": "EMA_50_day-5^2", "coef": 131.20310753397644}, {"feature": "DTB4WK_day-5", "coef": 23802283.597566918}, {"feature": "DTB4WK_day-5^2", "coef": 0.03930060449056327}, {"feature": "DTB3_day-5", "coef": 7426911.517998575}, {"feature": "DTB3_day-5^2", "coef": 0.010849310318008065}, {"feature": "DTB6_day-5", "coef": 45135616.99935518}, {"feature": "DTB6_day-5^2", "coef": 0.049406636506319046}, {"feature": "DGS5_day-5", "coef": -0.010851100087165833}, {"feature": "DGS5_day-5^2", "coef": 0.004102051258087158}, {"feature": "DGS10_day-5", "coef": 191970001.55013552}, {"feature": "DGS10_day-5^2", "coef": 0.26177592389285564}, {"feature": "Oil_day-5", "coef": -0.0009433235973119736}, {"feature": "Oil_day-5^2", "coef": 0.004523435025475919}, {"feature": "Gold_day-5", "coef": -0.006399700418114662}, {"feature": "Gold_day-5^2", "coef": -0.005784009583294392}, {"feature": "DAAA_day-5", "coef": 156734623.29407462}, {"feature": "DAAA_day-5^2", "coef": 0.06665454059839249}, {"feature": "DBAA_day-5", "coef": -591931048.6983426}, {"feature": "DBAA_day-5^2", "coef": -0.4002505801618099}, {"feature": "GBP_day-5", "coef": 0.000424778088927269}, {"feature": "GBP_day-5^2", "coef": 5.4047442972660065e-05}, {"feature": "JPY_day-5", "coef": 5.283765494823456e-05}, {"feature": "JPY_day-5^2", "coef": -0.00025470275431871414}, {"feature": "CAD_day-5", "coef": 0.0003040758892893791}, {"feature": "CAD_day-5^2", "coef": -0.00023406371474266052}, {"feature": "CNY_day-5", "coef": 0.00016255304217338562}, {"feature": "CNY_day-5^2", "coef": -0.00032782088965177536}, {"feature": "AAPL_day-5", "coef": -0.0004486013203859329}, {"feature": "AAPL_day-5^2", "coef": -7.36340880393982e-05}, {"feature": "AMZN_day-5", "coef": 0.00030574575066566467}, {"feature": "AMZN_day-5^2", "coef": 1.3398006558418274e-05}, {"feature": "GE_day-5", "coef": 0.0005417242646217346}, {"feature": "GE_day-5^2", "coef": -0.0005164556205272675}, {"feature": "JNJ_day-5", "coef": 0.00032307393848896027}, {"feature": "JNJ_day-5^2", "coef": 0.00017129629850387573}, {"feature": "JPM_day-5", "coef": 0.0004406156949698925}, {"feature": "JPM_day-5^2", "coef": -0.00043134763836860657}, {"feature": "MSFT_day-5", "coef": -7.103756070137024e-05}, {"feature": "MSFT_day-5^2", "coef": -0.00031945575028657913}, {"feature": "WFC_day-5", "coef": -0.0007059043273329735}, {"feature": "WFC_day-5^2", "coef": 0.0009968746453523636}, {"feature": "XOM_day-5", "coef": -0.000613827258348465}, {"feature": "XOM_day-5^2", "coef": 0.0006395019590854645}, {"feature": "FCHI_day-5", "coef": -0.0005396995693445206}, {"feature": "FCHI_day-5^2", "coef": -0.0007071048021316528}, {"feature": "FTSE_day-5", "coef": 0.0010631158947944641}, {"feature": "FTSE_day-5^2", "coef": -0.0021073706448078156}, {"feature": "GDAXI_day-5", "coef": -0.001194274052977562}, {"feature": "GDAXI_day-5^2", "coef": 0.002004627138376236}, {"feature": "GSPC_day-5", "coef": -0.002933257259428501}, {"feature": "GSPC_day-5^2", "coef": -0.0004141181707382202}, {"feature": "HSI_day-5", "coef": 0.000414527952671051}, {"feature": "HSI_day-5^2", "coef": 0.0003979727625846863}, {"feature": "DJI_day-5", "coef": 0.0008524321019649506}, {"feature": "DJI_day-5^2", "coef": -0.004155237227678299}, {"feature": "SSEC_day-5", "coef": -0.00025146640837192535}, {"feature": "SSEC_day-5^2", "coef": -0.00039105117321014404}, {"feature": "RUT_day-5", "coef": -0.00043756701052188873}, {"feature": "RUT_day-5^2", "coef": -0.0007498301565647125}, {"feature": "NYSE_day-5", "coef": 0.0037238504737615585}, {"feature": "NYSE_day-5^2", "coef": 0.003944341093301773}, {"feature": "TE1_day-5", "coef": 364448997.3929024}, {"feature": "TE1_day-5^2", "coef": -0.07994644157588482}, {"feature": "TE2_day-5", "coef": 365861027.9235109}, {"feature": "TE2_day-5^2", "coef": -0.1887379540130496}, {"feature": "TE3_day-5", "coef": -474268065.90854174}, {"feature": "TE3_day-5^2", "coef": -0.07550619170069695}, {"feature": "TE5_day-5", "coef": 67857304.28192386}, {"feature": "TE5_day-5^2", "coef": 0.0005663912743330002}, {"feature": "TE6_day-5", "coef": -101895521.34713876}, {"feature": "TE6_day-5^2", "coef": -0.003611341118812561}, {"feature": "DE1_day-5", "coef": 67578274.61700791}, {"feature": "DE1_day-5^2", "coef": -0.004834521561861038}, {"feature": "DE2_day-5", "coef": 344988389.2892372}, {"feature": "DE2_day-5^2", "coef": 0.0384124293923378}, {"feature": "DE4_day-5", "coef": -287154505.67148376}, {"feature": "DE4_day-5^2", "coef": -0.3267538771033287}, {"feature": "DE5_day-5", "coef": 733240255.8882034}, {"feature": "DE5_day-5^2", "coef": 0.23841169476509094}, {"feature": "DE6_day-5", "coef": -563321192.6541308}, {"feature": "DE6_day-5^2", "coef": 0.4875468946993351}, {"feature": "CTB3M_day-5", "coef": 0.0017496980726718903}, {"feature": "CTB3M_day-5^2", "coef": -0.003107599914073944}, {"feature": "CTB6M_day-5", "coef": -0.0010453760623931885}, {"feature": "CTB6M_day-5^2", "coef": -0.00920918583869934}, {"feature": "CTB1Y_day-5", "coef": 0.0016832798719406128}, {"feature": "CTB1Y_day-5^2", "coef": 0.012158706784248352}, {"feature": "AUD_day-5", "coef": -0.0016220612451434135}, {"feature": "AUD_day-5^2", "coef": -0.0003723427653312683}, {"feature": "Brent_day-5", "coef": 0.0006299689412117004}, {"feature": "Brent_day-5^2", "coef": 0.0009716823697090149}, {"feature": "CAC-F_day-5", "coef": -0.00020330771803855896}, {"feature": "CAC-F_day-5^2", "coef": 0.0007736459374427795}, {"feature": "copper-F_day-5", "coef": -0.0009346120059490204}, {"feature": "copper-F_day-5^2", "coef": 0.0006976835429668427}, {"feature": "WIT-oil_day-5", "coef": -0.0004102475941181183}, {"feature": "WIT-oil_day-5^2", "coef": -0.0010862182825803757}, {"feature": "DAX-F_day-5", "coef": 0.00013311579823493958}, {"feature": "DAX-F_day-5^2", "coef": -0.001051153987646103}, {"feature": "DJI-F_day-5", "coef": -0.00030181650072336197}, {"feature": "DJI-F_day-5^2", "coef": 0.0020981673151254654}, {"feature": "EUR_day-5", "coef": 0.000741318566724658}, {"feature": "EUR_day-5^2", "coef": 0.0006001144647598267}, {"feature": "FTSE-F_day-5", "coef": -0.0006336374208331108}, {"feature": "FTSE-F_day-5^2", "coef": 0.0008004754781723022}, {"feature": "gold-F_day-5", "coef": -0.0001638270914554596}, {"feature": "gold-F_day-5^2", "coef": 0.00026318617165088654}, {"feature": "HSI-F_day-5", "coef": -0.00037028733640909195}, {"feature": "HSI-F_day-5^2", "coef": -0.0005575641989707947}, {"feature": "KOSPI-F_day-5", "coef": 0.0004413481801748276}, {"feature": "KOSPI-F_day-5^2", "coef": 0.0008450523018836975}, {"feature": "NASDAQ-F_day-5", "coef": -0.0010688416659832}, {"feature": "NASDAQ-F_day-5^2", "coef": 0.0010000970214605331}, {"feature": "GAS-F_day-5", "coef": -0.00026650354266166687}, {"feature": "GAS-F_day-5^2", "coef": 0.000726211816072464}, {"feature": "Nikkei-F_day-5", "coef": 0.0003333296626806259}, {"feature": "Nikkei-F_day-5^2", "coef": 3.857538104057312e-05}, {"feature": "NZD_day-5", "coef": 0.0011743726208806038}, {"feature": "NZD_day-5^2", "coef": -0.0013170838356018066}, {"feature": "silver-F_day-5", "coef": 0.0008229073137044907}, {"feature": "silver-F_day-5^2", "coef": -0.0008983053267002106}, {"feature": "RUSSELL-F_day-5", "coef": -0.0017318781465291977}, {"feature": "RUSSELL-F_day-5^2", "coef": 0.0015564784407615662}, {"feature": "S&P-F_day-5", "coef": 0.00021256506443023682}, {"feature": "S&P-F_day-5^2", "coef": -0.006058122962713242}, {"feature": "CHF_day-5", "coef": 0.0009021293371915817}, {"feature": "CHF_day-5^2", "coef": 0.00023027323186397552}, {"feature": "Dollar index-F_day-5", "coef": 0.0003516152501106262}, {"feature": "Dollar index-F_day-5^2", "coef": 0.0003256946802139282}, {"feature": "Dollar index_day-5", "coef": -0.0003636833280324936}, {"feature": "Dollar index_day-5^2", "coef": -0.0009163869544863701}, {"feature": "wheat-F_day-5", "coef": -0.0003430219367146492}, {"feature": "wheat-F_day-5^2", "coef": 5.698110908269882e-05}, {"feature": "XAG_day-5", "coef": 0.0003799609839916229}, {"feature": "XAG_day-5^2", "coef": 0.0004591774195432663}, {"feature": "XAU_day-5", "coef": -0.00011743977665901184}, {"feature": "XAU_day-5^2", "coef": 0.00021554157137870789}, {"feature": "day_of_week_day-5", "coef": -0.014597651083022356}, {"feature": "day_of_week_day-5^2", "coef": 0.016095931641757488}, {"feature": "day_of_month_day-5", "coef": -0.004560425877571106}, {"feature": "day_of_month_day-5^2", "coef": 0.004348518326878548}, {"feature": "month_day-5", "coef": -0.019432976841926575}, {"feature": "month_day-5^2", "coef": 0.021847516298294067}, {"feature": "Close_ROC_day-5", "coef": -242662.77062105015}, {"feature": "Close_ROC_day-5^2", "coef": -139027.8146728687}, {"feature": "EMA10_ROC_day-5", "coef": 1.6991159990429878}, {"feature": "EMA10_ROC_day-5^2", "coef": -0.041818760335445404}, {"feature": "EMA20_ROC_day-5", "coef": -2.8951211869716644}, {"feature": "EMA20_ROC_day-5^2", "coef": 0.06854860298335552}, {"feature": "EMA50_ROC_day-5", "coef": 1.3695507012307644}, {"feature": "EMA50_ROC_day-5^2", "coef": -0.03464478254318237}]}}, {"mode": "vega-lite"});
</script>
</div>
</div>
</section>
<section id="rfe-cross-validation" class="level4">
<h4 class="anchored" data-anchor-id="rfe-cross-validation">2.2.2 RFE Cross-Validation</h4>
<p>We used <code>RFECV</code> to do RFE and Cross-Validation (CV) at the same time.</p>
<div id="cell-32" class="cell" data-cell_id="6b39297d68c84b869fd773b50e24eb6d" data-deepnote_cell_type="code" data-execution_context_id="6a764889-2e6f-4fd0-94c9-4785402e8b36" data-execution_millis="1107028" data-execution_start="1741124329264" data-source_hash="be84936a" data-execution_count="16">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Series Cross-Validation.</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>tscv <span class="op">=</span> TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>step_lin_reg <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RFECV.</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>selector_lin_reg_cv <span class="op">=</span> RFECV(</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>LinearRegression(n_jobs<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    step<span class="op">=</span>step_lin_reg,  <span class="co"># Remove 1 feature at each step.</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,          <span class="co"># Time series split.</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>  <span class="co"># Negative MSE for scoring.</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and automatically select the optimal number of features.</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>selector_lin_reg_cv.fit(X_train, y_train)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the best model for later use.</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>best_model_reg_final <span class="op">=</span> selector_lin_reg_cv.estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting estimator with 880 features.
Fitting estimator with 879 features.
Fitting estimator with 878 features.
Fitting estimator with 877 features.
Fitting estimator with 876 features.
Fitting estimator with 875 features.
Fitting estimator with 874 features.
Fitting estimator with 873 features.
Fitting estimator with 872 features.
Fitting estimator with 871 features.
Fitting estimator with 870 features.
Fitting estimator with 869 features.
Fitting estimator with 868 features.
Fitting estimator with 867 features.
Fitting estimator with 866 features.
Fitting estimator with 865 features.
Fitting estimator with 864 features.
Fitting estimator with 863 features.
Fitting estimator with 862 features.
Fitting estimator with 861 features.
Fitting estimator with 860 features.
Fitting estimator with 859 features.
Fitting estimator with 858 features.
Fitting estimator with 857 features.
Fitting estimator with 856 features.
Fitting estimator with 855 features.
Fitting estimator with 854 features.
Fitting estimator with 853 features.
Fitting estimator with 852 features.
Fitting estimator with 851 features.
Fitting estimator with 850 features.
Fitting estimator with 849 features.
Fitting estimator with 848 features.
Fitting estimator with 847 features.
Fitting estimator with 846 features.
Fitting estimator with 845 features.
Fitting estimator with 844 features.
Fitting estimator with 843 features.
Fitting estimator with 842 features.
Fitting estimator with 841 features.
Fitting estimator with 840 features.
Fitting estimator with 839 features.
Fitting estimator with 838 features.
Fitting estimator with 837 features.
Fitting estimator with 836 features.
Fitting estimator with 835 features.
Fitting estimator with 834 features.
Fitting estimator with 833 features.
Fitting estimator with 832 features.
Fitting estimator with 831 features.
Fitting estimator with 830 features.
Fitting estimator with 829 features.
Fitting estimator with 828 features.
Fitting estimator with 827 features.
Fitting estimator with 826 features.
Fitting estimator with 825 features.
Fitting estimator with 824 features.
Fitting estimator with 823 features.
Fitting estimator with 822 features.
Fitting estimator with 821 features.
Fitting estimator with 820 features.
Fitting estimator with 819 features.
Fitting estimator with 818 features.
Fitting estimator with 817 features.
Fitting estimator with 816 features.
Fitting estimator with 815 features.
Fitting estimator with 814 features.
Fitting estimator with 813 features.
Fitting estimator with 812 features.
Fitting estimator with 811 features.
Fitting estimator with 810 features.
Fitting estimator with 809 features.
Fitting estimator with 808 features.
Fitting estimator with 807 features.
Fitting estimator with 806 features.
Fitting estimator with 805 features.
Fitting estimator with 804 features.
Fitting estimator with 803 features.
Fitting estimator with 802 features.
Fitting estimator with 801 features.
Fitting estimator with 800 features.
Fitting estimator with 799 features.
Fitting estimator with 798 features.
Fitting estimator with 797 features.
Fitting estimator with 796 features.
Fitting estimator with 795 features.
Fitting estimator with 794 features.
Fitting estimator with 793 features.
Fitting estimator with 792 features.
Fitting estimator with 791 features.
Fitting estimator with 790 features.
Fitting estimator with 789 features.
Fitting estimator with 788 features.
Fitting estimator with 787 features.
Fitting estimator with 786 features.
Fitting estimator with 785 features.
Fitting estimator with 784 features.
Fitting estimator with 783 features.
Fitting estimator with 782 features.
Fitting estimator with 781 features.
Fitting estimator with 780 features.
Fitting estimator with 779 features.
Fitting estimator with 778 features.
Fitting estimator with 777 features.
Fitting estimator with 776 features.
Fitting estimator with 775 features.
Fitting estimator with 774 features.
Fitting estimator with 773 features.
Fitting estimator with 772 features.
Fitting estimator with 771 features.
Fitting estimator with 770 features.
Fitting estimator with 769 features.
Fitting estimator with 768 features.
Fitting estimator with 767 features.
Fitting estimator with 766 features.
Fitting estimator with 765 features.
Fitting estimator with 764 features.
Fitting estimator with 763 features.
Fitting estimator with 762 features.
Fitting estimator with 761 features.
Fitting estimator with 760 features.
Fitting estimator with 759 features.
Fitting estimator with 758 features.
Fitting estimator with 757 features.
Fitting estimator with 756 features.
Fitting estimator with 755 features.
Fitting estimator with 754 features.
Fitting estimator with 753 features.
Fitting estimator with 752 features.
Fitting estimator with 751 features.
Fitting estimator with 750 features.
Fitting estimator with 749 features.
Fitting estimator with 748 features.
Fitting estimator with 747 features.
Fitting estimator with 746 features.
Fitting estimator with 745 features.
Fitting estimator with 744 features.
Fitting estimator with 743 features.
Fitting estimator with 742 features.
Fitting estimator with 741 features.
Fitting estimator with 740 features.
Fitting estimator with 739 features.
Fitting estimator with 738 features.
Fitting estimator with 737 features.
Fitting estimator with 736 features.
Fitting estimator with 735 features.
Fitting estimator with 734 features.
Fitting estimator with 733 features.
Fitting estimator with 732 features.
Fitting estimator with 731 features.
Fitting estimator with 730 features.
Fitting estimator with 729 features.
Fitting estimator with 728 features.
Fitting estimator with 727 features.
Fitting estimator with 726 features.
Fitting estimator with 725 features.
Fitting estimator with 724 features.
Fitting estimator with 723 features.
Fitting estimator with 722 features.
Fitting estimator with 721 features.
Fitting estimator with 720 features.
Fitting estimator with 719 features.
Fitting estimator with 718 features.
Fitting estimator with 717 features.
Fitting estimator with 716 features.
Fitting estimator with 715 features.
Fitting estimator with 714 features.
Fitting estimator with 713 features.
Fitting estimator with 712 features.
Fitting estimator with 711 features.
Fitting estimator with 710 features.
Fitting estimator with 709 features.
Fitting estimator with 708 features.
Fitting estimator with 707 features.
Fitting estimator with 706 features.
Fitting estimator with 705 features.
Fitting estimator with 704 features.
Fitting estimator with 703 features.
Fitting estimator with 702 features.
Fitting estimator with 701 features.
Fitting estimator with 700 features.
Fitting estimator with 699 features.
Fitting estimator with 698 features.
Fitting estimator with 697 features.
Fitting estimator with 696 features.
Fitting estimator with 695 features.
Fitting estimator with 694 features.
Fitting estimator with 693 features.
Fitting estimator with 692 features.
Fitting estimator with 691 features.
Fitting estimator with 690 features.
Fitting estimator with 689 features.
Fitting estimator with 688 features.
Fitting estimator with 687 features.
Fitting estimator with 686 features.
Fitting estimator with 685 features.
Fitting estimator with 684 features.
Fitting estimator with 683 features.
Fitting estimator with 682 features.
Fitting estimator with 681 features.
Fitting estimator with 680 features.
Fitting estimator with 679 features.
Fitting estimator with 678 features.
Fitting estimator with 677 features.
Fitting estimator with 676 features.
Fitting estimator with 675 features.
Fitting estimator with 674 features.
Fitting estimator with 673 features.
Fitting estimator with 672 features.
Fitting estimator with 671 features.
Fitting estimator with 670 features.
Fitting estimator with 669 features.
Fitting estimator with 668 features.
Fitting estimator with 667 features.
Fitting estimator with 666 features.
Fitting estimator with 665 features.
Fitting estimator with 664 features.
Fitting estimator with 663 features.
Fitting estimator with 662 features.
Fitting estimator with 661 features.
Fitting estimator with 660 features.
Fitting estimator with 659 features.
Fitting estimator with 658 features.
Fitting estimator with 657 features.
Fitting estimator with 656 features.
Fitting estimator with 655 features.
Fitting estimator with 654 features.
Fitting estimator with 653 features.
Fitting estimator with 652 features.
Fitting estimator with 651 features.
Fitting estimator with 650 features.
Fitting estimator with 649 features.
Fitting estimator with 648 features.
Fitting estimator with 647 features.
Fitting estimator with 646 features.
Fitting estimator with 645 features.
Fitting estimator with 644 features.
Fitting estimator with 643 features.
Fitting estimator with 642 features.
Fitting estimator with 641 features.
Fitting estimator with 640 features.
Fitting estimator with 639 features.
Fitting estimator with 638 features.
Fitting estimator with 637 features.
Fitting estimator with 636 features.
Fitting estimator with 635 features.
Fitting estimator with 634 features.
Fitting estimator with 633 features.
Fitting estimator with 632 features.
Fitting estimator with 631 features.
Fitting estimator with 630 features.
Fitting estimator with 629 features.
Fitting estimator with 628 features.
Fitting estimator with 627 features.
Fitting estimator with 626 features.
Fitting estimator with 625 features.
Fitting estimator with 624 features.
Fitting estimator with 623 features.
Fitting estimator with 622 features.
Fitting estimator with 621 features.
Fitting estimator with 620 features.
Fitting estimator with 619 features.
Fitting estimator with 618 features.
Fitting estimator with 617 features.
Fitting estimator with 616 features.
Fitting estimator with 615 features.
Fitting estimator with 614 features.
Fitting estimator with 613 features.
Fitting estimator with 612 features.
Fitting estimator with 611 features.
Fitting estimator with 610 features.
Fitting estimator with 609 features.
Fitting estimator with 608 features.
Fitting estimator with 607 features.
Fitting estimator with 606 features.
Fitting estimator with 605 features.
Fitting estimator with 604 features.
Fitting estimator with 603 features.
Fitting estimator with 602 features.
Fitting estimator with 601 features.
Fitting estimator with 600 features.
Fitting estimator with 599 features.
Fitting estimator with 598 features.
Fitting estimator with 597 features.
Fitting estimator with 596 features.
Fitting estimator with 595 features.
Fitting estimator with 594 features.
Fitting estimator with 593 features.
Fitting estimator with 592 features.
Fitting estimator with 591 features.
Fitting estimator with 590 features.
Fitting estimator with 589 features.
Fitting estimator with 588 features.
Fitting estimator with 587 features.
Fitting estimator with 586 features.
Fitting estimator with 585 features.
Fitting estimator with 584 features.
Fitting estimator with 583 features.
Fitting estimator with 582 features.
Fitting estimator with 581 features.
Fitting estimator with 580 features.
Fitting estimator with 579 features.
Fitting estimator with 578 features.
Fitting estimator with 577 features.
Fitting estimator with 576 features.
Fitting estimator with 575 features.
Fitting estimator with 574 features.
Fitting estimator with 573 features.
Fitting estimator with 572 features.
Fitting estimator with 571 features.
Fitting estimator with 570 features.
Fitting estimator with 569 features.
Fitting estimator with 568 features.
Fitting estimator with 567 features.
Fitting estimator with 566 features.
Fitting estimator with 565 features.
Fitting estimator with 564 features.
Fitting estimator with 563 features.
Fitting estimator with 562 features.
Fitting estimator with 561 features.
Fitting estimator with 560 features.
Fitting estimator with 559 features.
Fitting estimator with 558 features.
Fitting estimator with 557 features.
Fitting estimator with 556 features.
Fitting estimator with 555 features.
Fitting estimator with 554 features.
Fitting estimator with 553 features.
Fitting estimator with 552 features.
Fitting estimator with 551 features.
Fitting estimator with 550 features.
Fitting estimator with 549 features.
Fitting estimator with 548 features.
Fitting estimator with 547 features.
Fitting estimator with 546 features.
Fitting estimator with 545 features.
Fitting estimator with 544 features.
Fitting estimator with 543 features.
Fitting estimator with 542 features.
Fitting estimator with 541 features.
Fitting estimator with 540 features.
Fitting estimator with 539 features.
Fitting estimator with 538 features.
Fitting estimator with 537 features.
Fitting estimator with 536 features.
Fitting estimator with 535 features.
Fitting estimator with 534 features.
Fitting estimator with 533 features.
Fitting estimator with 532 features.
Fitting estimator with 531 features.
Fitting estimator with 530 features.
Fitting estimator with 529 features.
Fitting estimator with 528 features.
Fitting estimator with 527 features.
Fitting estimator with 526 features.
Fitting estimator with 525 features.
Fitting estimator with 524 features.
Fitting estimator with 523 features.
Fitting estimator with 522 features.
Fitting estimator with 521 features.
Fitting estimator with 520 features.
Fitting estimator with 519 features.
Fitting estimator with 518 features.
Fitting estimator with 517 features.
Fitting estimator with 516 features.
Fitting estimator with 515 features.
Fitting estimator with 514 features.
Fitting estimator with 513 features.
Fitting estimator with 512 features.
Fitting estimator with 511 features.
Fitting estimator with 510 features.
Fitting estimator with 509 features.
Fitting estimator with 508 features.
Fitting estimator with 507 features.
Fitting estimator with 506 features.
Fitting estimator with 505 features.
Fitting estimator with 504 features.
Fitting estimator with 503 features.
Fitting estimator with 502 features.
Fitting estimator with 501 features.
Fitting estimator with 500 features.
Fitting estimator with 499 features.
Fitting estimator with 498 features.
Fitting estimator with 497 features.
Fitting estimator with 496 features.
Fitting estimator with 495 features.
Fitting estimator with 494 features.
Fitting estimator with 493 features.
Fitting estimator with 492 features.
Fitting estimator with 491 features.
Fitting estimator with 490 features.
Fitting estimator with 489 features.
Fitting estimator with 488 features.
Fitting estimator with 487 features.
Fitting estimator with 486 features.
Fitting estimator with 485 features.
Fitting estimator with 484 features.
Fitting estimator with 483 features.
Fitting estimator with 482 features.
Fitting estimator with 481 features.
Fitting estimator with 480 features.
Fitting estimator with 479 features.
Fitting estimator with 478 features.
Fitting estimator with 477 features.
Fitting estimator with 476 features.
Fitting estimator with 475 features.
Fitting estimator with 474 features.
Fitting estimator with 473 features.
Fitting estimator with 472 features.
Fitting estimator with 471 features.
Fitting estimator with 470 features.
Fitting estimator with 469 features.
Fitting estimator with 468 features.
Fitting estimator with 467 features.
Fitting estimator with 466 features.
Fitting estimator with 465 features.
Fitting estimator with 464 features.
Fitting estimator with 463 features.
Fitting estimator with 462 features.
Fitting estimator with 461 features.
Fitting estimator with 460 features.
Fitting estimator with 459 features.
Fitting estimator with 458 features.
Fitting estimator with 457 features.
Fitting estimator with 456 features.
Fitting estimator with 455 features.
Fitting estimator with 454 features.
Fitting estimator with 453 features.
Fitting estimator with 452 features.
Fitting estimator with 451 features.
Fitting estimator with 450 features.
Fitting estimator with 449 features.
Fitting estimator with 448 features.
Fitting estimator with 447 features.
Fitting estimator with 446 features.
Fitting estimator with 445 features.
Fitting estimator with 444 features.
Fitting estimator with 443 features.
Fitting estimator with 442 features.
Fitting estimator with 441 features.
Fitting estimator with 440 features.
Fitting estimator with 439 features.
Fitting estimator with 438 features.
Fitting estimator with 437 features.
Fitting estimator with 436 features.
Fitting estimator with 435 features.
Fitting estimator with 434 features.
Fitting estimator with 433 features.
Fitting estimator with 432 features.
Fitting estimator with 431 features.
Fitting estimator with 430 features.
Fitting estimator with 429 features.
Fitting estimator with 428 features.
Fitting estimator with 427 features.
Fitting estimator with 426 features.
Fitting estimator with 425 features.
Fitting estimator with 424 features.
Fitting estimator with 423 features.
Fitting estimator with 422 features.
Fitting estimator with 421 features.
Fitting estimator with 420 features.
Fitting estimator with 419 features.
Fitting estimator with 418 features.
Fitting estimator with 417 features.
Fitting estimator with 416 features.
Fitting estimator with 415 features.
Fitting estimator with 414 features.
Fitting estimator with 413 features.
Fitting estimator with 412 features.
Fitting estimator with 411 features.
Fitting estimator with 410 features.
Fitting estimator with 409 features.
Fitting estimator with 408 features.
Fitting estimator with 407 features.
Fitting estimator with 406 features.
Fitting estimator with 405 features.
Fitting estimator with 404 features.
Fitting estimator with 403 features.
Fitting estimator with 402 features.
Fitting estimator with 401 features.
Fitting estimator with 400 features.
Fitting estimator with 399 features.
Fitting estimator with 398 features.
Fitting estimator with 397 features.
Fitting estimator with 396 features.
Fitting estimator with 395 features.
Fitting estimator with 394 features.
Fitting estimator with 393 features.
Fitting estimator with 392 features.
Fitting estimator with 391 features.
Fitting estimator with 390 features.
Fitting estimator with 389 features.
Fitting estimator with 388 features.
Fitting estimator with 387 features.
Fitting estimator with 386 features.
Fitting estimator with 385 features.
Fitting estimator with 384 features.
Fitting estimator with 383 features.
Fitting estimator with 382 features.
Fitting estimator with 381 features.
Fitting estimator with 380 features.
Fitting estimator with 379 features.
Fitting estimator with 378 features.
Fitting estimator with 377 features.
Fitting estimator with 376 features.
Fitting estimator with 375 features.
Fitting estimator with 374 features.
Fitting estimator with 373 features.
Fitting estimator with 372 features.
Fitting estimator with 371 features.
Fitting estimator with 370 features.
Fitting estimator with 369 features.
Fitting estimator with 368 features.
Fitting estimator with 367 features.
Fitting estimator with 366 features.
Fitting estimator with 365 features.
Fitting estimator with 364 features.
Fitting estimator with 363 features.
Fitting estimator with 362 features.
Fitting estimator with 361 features.
Fitting estimator with 360 features.
Fitting estimator with 359 features.
Fitting estimator with 358 features.
Fitting estimator with 357 features.
Fitting estimator with 356 features.
Fitting estimator with 355 features.
Fitting estimator with 354 features.
Fitting estimator with 353 features.
Fitting estimator with 352 features.
Fitting estimator with 351 features.
Fitting estimator with 350 features.
Fitting estimator with 349 features.
Fitting estimator with 348 features.
Fitting estimator with 347 features.
Fitting estimator with 346 features.
Fitting estimator with 345 features.
Fitting estimator with 344 features.
Fitting estimator with 343 features.
Fitting estimator with 342 features.
Fitting estimator with 341 features.
Fitting estimator with 340 features.
Fitting estimator with 339 features.
Fitting estimator with 338 features.
Fitting estimator with 337 features.
Fitting estimator with 336 features.
Fitting estimator with 335 features.
Fitting estimator with 334 features.
Fitting estimator with 333 features.
Fitting estimator with 332 features.
Fitting estimator with 331 features.
Fitting estimator with 330 features.
Fitting estimator with 329 features.
Fitting estimator with 328 features.
Fitting estimator with 327 features.
Fitting estimator with 326 features.
Fitting estimator with 325 features.
Fitting estimator with 324 features.
Fitting estimator with 323 features.
Fitting estimator with 322 features.
Fitting estimator with 321 features.
Fitting estimator with 320 features.
Fitting estimator with 319 features.
Fitting estimator with 318 features.
Fitting estimator with 317 features.
Fitting estimator with 316 features.
Fitting estimator with 315 features.
Fitting estimator with 314 features.
Fitting estimator with 313 features.
Fitting estimator with 312 features.
Fitting estimator with 311 features.
Fitting estimator with 310 features.
Fitting estimator with 309 features.
Fitting estimator with 308 features.
Fitting estimator with 307 features.
Fitting estimator with 306 features.
Fitting estimator with 305 features.
Fitting estimator with 304 features.
Fitting estimator with 303 features.
Fitting estimator with 302 features.
Fitting estimator with 301 features.
Fitting estimator with 300 features.
Fitting estimator with 299 features.
Fitting estimator with 298 features.
Fitting estimator with 297 features.
Fitting estimator with 296 features.
Fitting estimator with 295 features.
Fitting estimator with 294 features.
Fitting estimator with 293 features.
Fitting estimator with 292 features.
Fitting estimator with 291 features.
Fitting estimator with 290 features.
Fitting estimator with 289 features.
Fitting estimator with 288 features.
Fitting estimator with 287 features.
Fitting estimator with 286 features.
Fitting estimator with 285 features.
Fitting estimator with 284 features.
Fitting estimator with 283 features.
Fitting estimator with 282 features.
Fitting estimator with 281 features.
Fitting estimator with 280 features.
Fitting estimator with 279 features.
Fitting estimator with 278 features.
Fitting estimator with 277 features.
Fitting estimator with 276 features.
Fitting estimator with 275 features.
Fitting estimator with 274 features.
Fitting estimator with 273 features.
Fitting estimator with 272 features.
Fitting estimator with 271 features.
Fitting estimator with 270 features.
Fitting estimator with 269 features.
Fitting estimator with 268 features.
Fitting estimator with 267 features.
Fitting estimator with 266 features.
Fitting estimator with 265 features.
Fitting estimator with 264 features.
Fitting estimator with 263 features.
Fitting estimator with 262 features.
Fitting estimator with 261 features.
Fitting estimator with 260 features.
Fitting estimator with 259 features.
Fitting estimator with 258 features.
Fitting estimator with 257 features.
Fitting estimator with 256 features.
Fitting estimator with 255 features.
Fitting estimator with 254 features.
Fitting estimator with 253 features.
Fitting estimator with 252 features.
Fitting estimator with 251 features.
Fitting estimator with 250 features.
Fitting estimator with 249 features.
Fitting estimator with 248 features.
Fitting estimator with 247 features.
Fitting estimator with 246 features.
Fitting estimator with 245 features.
Fitting estimator with 244 features.
Fitting estimator with 243 features.
Fitting estimator with 242 features.
Fitting estimator with 241 features.
Fitting estimator with 240 features.
Fitting estimator with 239 features.
Fitting estimator with 238 features.
Fitting estimator with 237 features.
Fitting estimator with 236 features.
Fitting estimator with 235 features.
Fitting estimator with 234 features.
Fitting estimator with 233 features.
Fitting estimator with 232 features.
Fitting estimator with 231 features.
Fitting estimator with 230 features.
Fitting estimator with 229 features.
Fitting estimator with 228 features.
Fitting estimator with 227 features.
Fitting estimator with 226 features.
Fitting estimator with 225 features.
Fitting estimator with 224 features.
Fitting estimator with 223 features.
Fitting estimator with 222 features.
Fitting estimator with 221 features.
Fitting estimator with 220 features.
Fitting estimator with 219 features.
Fitting estimator with 218 features.
Fitting estimator with 217 features.
Fitting estimator with 216 features.
Fitting estimator with 215 features.
Fitting estimator with 214 features.
Fitting estimator with 213 features.
Fitting estimator with 212 features.
Fitting estimator with 211 features.
Fitting estimator with 210 features.
Fitting estimator with 209 features.
Fitting estimator with 208 features.
Fitting estimator with 207 features.
Fitting estimator with 206 features.
Fitting estimator with 205 features.
Fitting estimator with 204 features.
Fitting estimator with 203 features.
Fitting estimator with 202 features.
Fitting estimator with 201 features.
Fitting estimator with 200 features.
Fitting estimator with 199 features.
Fitting estimator with 198 features.
Fitting estimator with 197 features.
Fitting estimator with 196 features.
Fitting estimator with 195 features.
Fitting estimator with 194 features.
Fitting estimator with 193 features.
Fitting estimator with 192 features.
Fitting estimator with 191 features.
Fitting estimator with 190 features.
Fitting estimator with 189 features.
Fitting estimator with 188 features.
Fitting estimator with 187 features.
Fitting estimator with 186 features.
Fitting estimator with 185 features.
Fitting estimator with 184 features.
Fitting estimator with 183 features.
Fitting estimator with 182 features.
Fitting estimator with 181 features.
Fitting estimator with 180 features.
Fitting estimator with 179 features.
Fitting estimator with 178 features.
Fitting estimator with 177 features.
Fitting estimator with 176 features.
Fitting estimator with 175 features.
Fitting estimator with 174 features.
Fitting estimator with 173 features.
Fitting estimator with 172 features.
Fitting estimator with 171 features.
Fitting estimator with 170 features.
Fitting estimator with 169 features.
Fitting estimator with 168 features.
Fitting estimator with 167 features.
Fitting estimator with 166 features.
Fitting estimator with 165 features.
Fitting estimator with 164 features.
Fitting estimator with 163 features.
Fitting estimator with 162 features.
Fitting estimator with 161 features.
Fitting estimator with 160 features.
Fitting estimator with 159 features.
Fitting estimator with 158 features.
Fitting estimator with 157 features.
Fitting estimator with 156 features.
Fitting estimator with 155 features.
Fitting estimator with 154 features.
Fitting estimator with 153 features.
Fitting estimator with 152 features.
Fitting estimator with 151 features.
Fitting estimator with 150 features.
Fitting estimator with 149 features.
Fitting estimator with 148 features.
Fitting estimator with 147 features.
Fitting estimator with 146 features.
Fitting estimator with 145 features.
Fitting estimator with 144 features.
Fitting estimator with 143 features.
Fitting estimator with 142 features.
Fitting estimator with 141 features.
Fitting estimator with 140 features.
Fitting estimator with 139 features.
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
Fitting estimator with 40 features.
Fitting estimator with 39 features.
Fitting estimator with 38 features.
Fitting estimator with 37 features.
Fitting estimator with 36 features.
Fitting estimator with 35 features.
Fitting estimator with 34 features.
Fitting estimator with 33 features.
Fitting estimator with 32 features.
Fitting estimator with 31 features.
Fitting estimator with 30 features.
Fitting estimator with 29 features.
Fitting estimator with 28 features.
Fitting estimator with 27 features.
Fitting estimator with 26 features.
Fitting estimator with 25 features.
Fitting estimator with 24 features.
Fitting estimator with 23 features.
Fitting estimator with 22 features.
Fitting estimator with 21 features.
Fitting estimator with 20 features.
Fitting estimator with 19 features.
Fitting estimator with 18 features.
Fitting estimator with 17 features.
Fitting estimator with 16 features.
Fitting estimator with 15 features.
Fitting estimator with 14 features.
Fitting estimator with 13 features.
Fitting estimator with 12 features.
Fitting estimator with 11 features.
Fitting estimator with 10 features.
Fitting estimator with 9 features.
Fitting estimator with 8 features.
Fitting estimator with 7 features.
Fitting estimator with 6 features.
Fitting estimator with 5 features.
Fitting estimator with 4 features.
Fitting estimator with 3 features.
Fitting estimator with 2 features.
Fitting estimator with 880 features.
Fitting estimator with 879 features.
Fitting estimator with 878 features.
Fitting estimator with 877 features.
Fitting estimator with 876 features.
Fitting estimator with 875 features.
Fitting estimator with 874 features.
Fitting estimator with 873 features.
Fitting estimator with 872 features.
Fitting estimator with 871 features.
Fitting estimator with 870 features.
Fitting estimator with 869 features.
Fitting estimator with 868 features.
Fitting estimator with 867 features.
Fitting estimator with 866 features.
Fitting estimator with 865 features.
Fitting estimator with 864 features.
Fitting estimator with 863 features.
Fitting estimator with 862 features.
Fitting estimator with 861 features.
Fitting estimator with 860 features.
Fitting estimator with 859 features.
Fitting estimator with 858 features.
Fitting estimator with 857 features.
Fitting estimator with 856 features.
Fitting estimator with 855 features.
Fitting estimator with 854 features.
Fitting estimator with 853 features.
Fitting estimator with 852 features.
Fitting estimator with 851 features.
Fitting estimator with 850 features.
Fitting estimator with 849 features.
Fitting estimator with 848 features.
Fitting estimator with 847 features.
Fitting estimator with 846 features.
Fitting estimator with 845 features.
Fitting estimator with 844 features.
Fitting estimator with 843 features.
Fitting estimator with 842 features.
Fitting estimator with 841 features.
Fitting estimator with 840 features.
Fitting estimator with 839 features.
Fitting estimator with 838 features.
Fitting estimator with 837 features.
Fitting estimator with 836 features.
Fitting estimator with 835 features.
Fitting estimator with 834 features.
Fitting estimator with 833 features.
Fitting estimator with 832 features.
Fitting estimator with 831 features.
Fitting estimator with 830 features.
Fitting estimator with 829 features.
Fitting estimator with 828 features.
Fitting estimator with 827 features.
Fitting estimator with 826 features.
Fitting estimator with 825 features.
Fitting estimator with 824 features.
Fitting estimator with 823 features.
Fitting estimator with 822 features.
Fitting estimator with 821 features.
Fitting estimator with 820 features.
Fitting estimator with 819 features.
Fitting estimator with 818 features.
Fitting estimator with 817 features.
Fitting estimator with 816 features.
Fitting estimator with 815 features.
Fitting estimator with 814 features.
Fitting estimator with 813 features.
Fitting estimator with 812 features.
Fitting estimator with 811 features.
Fitting estimator with 810 features.
Fitting estimator with 809 features.
Fitting estimator with 808 features.
Fitting estimator with 807 features.
Fitting estimator with 806 features.
Fitting estimator with 805 features.
Fitting estimator with 804 features.
Fitting estimator with 803 features.
Fitting estimator with 802 features.
Fitting estimator with 801 features.
Fitting estimator with 800 features.
Fitting estimator with 799 features.
Fitting estimator with 798 features.
Fitting estimator with 797 features.
Fitting estimator with 796 features.
Fitting estimator with 795 features.
Fitting estimator with 794 features.
Fitting estimator with 793 features.
Fitting estimator with 792 features.
Fitting estimator with 791 features.
Fitting estimator with 790 features.
Fitting estimator with 789 features.
Fitting estimator with 788 features.
Fitting estimator with 787 features.
Fitting estimator with 786 features.
Fitting estimator with 785 features.
Fitting estimator with 784 features.
Fitting estimator with 783 features.
Fitting estimator with 782 features.
Fitting estimator with 781 features.
Fitting estimator with 780 features.
Fitting estimator with 779 features.
Fitting estimator with 778 features.
Fitting estimator with 777 features.
Fitting estimator with 776 features.
Fitting estimator with 775 features.
Fitting estimator with 774 features.
Fitting estimator with 773 features.
Fitting estimator with 772 features.
Fitting estimator with 771 features.
Fitting estimator with 770 features.
Fitting estimator with 769 features.
Fitting estimator with 768 features.
Fitting estimator with 767 features.
Fitting estimator with 766 features.
Fitting estimator with 765 features.
Fitting estimator with 764 features.
Fitting estimator with 763 features.
Fitting estimator with 762 features.
Fitting estimator with 761 features.
Fitting estimator with 760 features.
Fitting estimator with 759 features.
Fitting estimator with 758 features.
Fitting estimator with 757 features.
Fitting estimator with 756 features.
Fitting estimator with 755 features.
Fitting estimator with 754 features.
Fitting estimator with 753 features.
Fitting estimator with 752 features.
Fitting estimator with 751 features.
Fitting estimator with 750 features.
Fitting estimator with 749 features.
Fitting estimator with 748 features.
Fitting estimator with 747 features.
Fitting estimator with 746 features.
Fitting estimator with 745 features.
Fitting estimator with 744 features.
Fitting estimator with 743 features.
Fitting estimator with 742 features.
Fitting estimator with 741 features.
Fitting estimator with 740 features.
Fitting estimator with 739 features.
Fitting estimator with 738 features.
Fitting estimator with 737 features.
Fitting estimator with 736 features.
Fitting estimator with 735 features.
Fitting estimator with 734 features.
Fitting estimator with 733 features.
Fitting estimator with 732 features.
Fitting estimator with 731 features.
Fitting estimator with 730 features.
Fitting estimator with 729 features.
Fitting estimator with 728 features.
Fitting estimator with 727 features.
Fitting estimator with 726 features.
Fitting estimator with 725 features.
Fitting estimator with 724 features.
Fitting estimator with 723 features.
Fitting estimator with 722 features.
Fitting estimator with 721 features.
Fitting estimator with 720 features.
Fitting estimator with 719 features.
Fitting estimator with 718 features.
Fitting estimator with 717 features.
Fitting estimator with 716 features.
Fitting estimator with 715 features.
Fitting estimator with 714 features.
Fitting estimator with 713 features.
Fitting estimator with 712 features.
Fitting estimator with 711 features.
Fitting estimator with 710 features.
Fitting estimator with 709 features.
Fitting estimator with 708 features.
Fitting estimator with 707 features.
Fitting estimator with 706 features.
Fitting estimator with 705 features.
Fitting estimator with 704 features.
Fitting estimator with 703 features.
Fitting estimator with 702 features.
Fitting estimator with 701 features.
Fitting estimator with 700 features.
Fitting estimator with 699 features.
Fitting estimator with 698 features.
Fitting estimator with 697 features.
Fitting estimator with 696 features.
Fitting estimator with 695 features.
Fitting estimator with 694 features.
Fitting estimator with 693 features.
Fitting estimator with 692 features.
Fitting estimator with 691 features.
Fitting estimator with 690 features.
Fitting estimator with 689 features.
Fitting estimator with 688 features.
Fitting estimator with 687 features.
Fitting estimator with 686 features.
Fitting estimator with 685 features.
Fitting estimator with 684 features.
Fitting estimator with 683 features.
Fitting estimator with 682 features.
Fitting estimator with 681 features.
Fitting estimator with 680 features.
Fitting estimator with 679 features.
Fitting estimator with 678 features.
Fitting estimator with 677 features.
Fitting estimator with 676 features.
Fitting estimator with 675 features.
Fitting estimator with 674 features.
Fitting estimator with 673 features.
Fitting estimator with 672 features.
Fitting estimator with 671 features.
Fitting estimator with 670 features.
Fitting estimator with 669 features.
Fitting estimator with 668 features.
Fitting estimator with 667 features.
Fitting estimator with 666 features.
Fitting estimator with 665 features.
Fitting estimator with 664 features.
Fitting estimator with 663 features.
Fitting estimator with 662 features.
Fitting estimator with 661 features.
Fitting estimator with 660 features.
Fitting estimator with 659 features.
Fitting estimator with 658 features.
Fitting estimator with 657 features.
Fitting estimator with 656 features.
Fitting estimator with 655 features.
Fitting estimator with 654 features.
Fitting estimator with 653 features.
Fitting estimator with 652 features.
Fitting estimator with 651 features.
Fitting estimator with 650 features.
Fitting estimator with 649 features.
Fitting estimator with 648 features.
Fitting estimator with 647 features.
Fitting estimator with 646 features.
Fitting estimator with 645 features.
Fitting estimator with 644 features.
Fitting estimator with 643 features.
Fitting estimator with 642 features.
Fitting estimator with 641 features.
Fitting estimator with 640 features.
Fitting estimator with 639 features.
Fitting estimator with 638 features.
Fitting estimator with 637 features.
Fitting estimator with 636 features.
Fitting estimator with 635 features.
Fitting estimator with 634 features.
Fitting estimator with 633 features.
Fitting estimator with 632 features.
Fitting estimator with 631 features.
Fitting estimator with 630 features.
Fitting estimator with 629 features.
Fitting estimator with 628 features.
Fitting estimator with 627 features.
Fitting estimator with 626 features.
Fitting estimator with 625 features.
Fitting estimator with 624 features.
Fitting estimator with 623 features.
Fitting estimator with 622 features.
Fitting estimator with 621 features.
Fitting estimator with 620 features.
Fitting estimator with 619 features.
Fitting estimator with 618 features.
Fitting estimator with 617 features.
Fitting estimator with 616 features.
Fitting estimator with 615 features.
Fitting estimator with 614 features.
Fitting estimator with 613 features.
Fitting estimator with 612 features.
Fitting estimator with 611 features.
Fitting estimator with 610 features.
Fitting estimator with 609 features.
Fitting estimator with 608 features.
Fitting estimator with 607 features.
Fitting estimator with 606 features.
Fitting estimator with 605 features.
Fitting estimator with 604 features.
Fitting estimator with 603 features.
Fitting estimator with 602 features.
Fitting estimator with 601 features.
Fitting estimator with 600 features.
Fitting estimator with 599 features.
Fitting estimator with 598 features.
Fitting estimator with 597 features.
Fitting estimator with 596 features.
Fitting estimator with 595 features.
Fitting estimator with 594 features.
Fitting estimator with 593 features.
Fitting estimator with 592 features.
Fitting estimator with 591 features.
Fitting estimator with 590 features.
Fitting estimator with 589 features.
Fitting estimator with 588 features.
Fitting estimator with 587 features.
Fitting estimator with 586 features.
Fitting estimator with 585 features.
Fitting estimator with 584 features.
Fitting estimator with 583 features.
Fitting estimator with 582 features.
Fitting estimator with 581 features.
Fitting estimator with 580 features.
Fitting estimator with 579 features.
Fitting estimator with 578 features.
Fitting estimator with 577 features.
Fitting estimator with 576 features.
Fitting estimator with 575 features.
Fitting estimator with 574 features.
Fitting estimator with 573 features.
Fitting estimator with 572 features.
Fitting estimator with 571 features.
Fitting estimator with 570 features.
Fitting estimator with 569 features.
Fitting estimator with 568 features.
Fitting estimator with 567 features.
Fitting estimator with 566 features.
Fitting estimator with 565 features.
Fitting estimator with 564 features.
Fitting estimator with 563 features.
Fitting estimator with 562 features.
Fitting estimator with 561 features.
Fitting estimator with 560 features.
Fitting estimator with 559 features.
Fitting estimator with 558 features.
Fitting estimator with 557 features.
Fitting estimator with 556 features.
Fitting estimator with 555 features.
Fitting estimator with 554 features.
Fitting estimator with 553 features.
Fitting estimator with 552 features.
Fitting estimator with 551 features.
Fitting estimator with 550 features.
Fitting estimator with 549 features.
Fitting estimator with 548 features.
Fitting estimator with 547 features.
Fitting estimator with 546 features.
Fitting estimator with 545 features.
Fitting estimator with 544 features.
Fitting estimator with 543 features.
Fitting estimator with 542 features.
Fitting estimator with 541 features.
Fitting estimator with 540 features.
Fitting estimator with 539 features.
Fitting estimator with 538 features.
Fitting estimator with 537 features.
Fitting estimator with 536 features.
Fitting estimator with 535 features.
Fitting estimator with 534 features.
Fitting estimator with 533 features.
Fitting estimator with 532 features.
Fitting estimator with 531 features.
Fitting estimator with 530 features.
Fitting estimator with 529 features.
Fitting estimator with 528 features.
Fitting estimator with 527 features.
Fitting estimator with 526 features.
Fitting estimator with 525 features.
Fitting estimator with 524 features.
Fitting estimator with 523 features.
Fitting estimator with 522 features.
Fitting estimator with 521 features.
Fitting estimator with 520 features.
Fitting estimator with 519 features.
Fitting estimator with 518 features.
Fitting estimator with 517 features.
Fitting estimator with 516 features.
Fitting estimator with 515 features.
Fitting estimator with 514 features.
Fitting estimator with 513 features.
Fitting estimator with 512 features.
Fitting estimator with 511 features.
Fitting estimator with 510 features.
Fitting estimator with 509 features.
Fitting estimator with 508 features.
Fitting estimator with 507 features.
Fitting estimator with 506 features.
Fitting estimator with 505 features.
Fitting estimator with 504 features.
Fitting estimator with 503 features.
Fitting estimator with 502 features.
Fitting estimator with 501 features.
Fitting estimator with 500 features.
Fitting estimator with 499 features.
Fitting estimator with 498 features.
Fitting estimator with 497 features.
Fitting estimator with 496 features.
Fitting estimator with 495 features.
Fitting estimator with 494 features.
Fitting estimator with 493 features.
Fitting estimator with 492 features.
Fitting estimator with 491 features.
Fitting estimator with 490 features.
Fitting estimator with 489 features.
Fitting estimator with 488 features.
Fitting estimator with 487 features.
Fitting estimator with 486 features.
Fitting estimator with 485 features.
Fitting estimator with 484 features.
Fitting estimator with 483 features.
Fitting estimator with 482 features.
Fitting estimator with 481 features.
Fitting estimator with 480 features.
Fitting estimator with 479 features.
Fitting estimator with 478 features.
Fitting estimator with 477 features.
Fitting estimator with 476 features.
Fitting estimator with 475 features.
Fitting estimator with 474 features.
Fitting estimator with 473 features.
Fitting estimator with 472 features.
Fitting estimator with 471 features.
Fitting estimator with 470 features.
Fitting estimator with 469 features.
Fitting estimator with 468 features.
Fitting estimator with 467 features.
Fitting estimator with 466 features.
Fitting estimator with 465 features.
Fitting estimator with 464 features.
Fitting estimator with 463 features.
Fitting estimator with 462 features.
Fitting estimator with 461 features.
Fitting estimator with 460 features.
Fitting estimator with 459 features.
Fitting estimator with 458 features.
Fitting estimator with 457 features.
Fitting estimator with 456 features.
Fitting estimator with 455 features.
Fitting estimator with 454 features.
Fitting estimator with 453 features.
Fitting estimator with 452 features.
Fitting estimator with 451 features.
Fitting estimator with 450 features.
Fitting estimator with 449 features.
Fitting estimator with 448 features.
Fitting estimator with 447 features.
Fitting estimator with 446 features.
Fitting estimator with 445 features.
Fitting estimator with 444 features.
Fitting estimator with 443 features.
Fitting estimator with 442 features.
Fitting estimator with 441 features.
Fitting estimator with 440 features.
Fitting estimator with 439 features.
Fitting estimator with 438 features.
Fitting estimator with 437 features.
Fitting estimator with 436 features.
Fitting estimator with 435 features.
Fitting estimator with 434 features.
Fitting estimator with 433 features.
Fitting estimator with 432 features.
Fitting estimator with 431 features.
Fitting estimator with 430 features.
Fitting estimator with 429 features.
Fitting estimator with 428 features.
Fitting estimator with 427 features.
Fitting estimator with 426 features.
Fitting estimator with 425 features.
Fitting estimator with 424 features.
Fitting estimator with 423 features.
Fitting estimator with 422 features.
Fitting estimator with 421 features.
Fitting estimator with 420 features.
Fitting estimator with 419 features.
Fitting estimator with 418 features.
Fitting estimator with 417 features.
Fitting estimator with 416 features.
Fitting estimator with 415 features.
Fitting estimator with 414 features.
Fitting estimator with 413 features.
Fitting estimator with 412 features.
Fitting estimator with 411 features.
Fitting estimator with 410 features.
Fitting estimator with 409 features.
Fitting estimator with 408 features.
Fitting estimator with 407 features.
Fitting estimator with 406 features.
Fitting estimator with 405 features.
Fitting estimator with 404 features.
Fitting estimator with 403 features.
Fitting estimator with 402 features.
Fitting estimator with 401 features.
Fitting estimator with 400 features.
Fitting estimator with 399 features.
Fitting estimator with 398 features.
Fitting estimator with 397 features.
Fitting estimator with 396 features.
Fitting estimator with 395 features.
Fitting estimator with 394 features.
Fitting estimator with 393 features.
Fitting estimator with 392 features.
Fitting estimator with 391 features.
Fitting estimator with 390 features.
Fitting estimator with 389 features.
Fitting estimator with 388 features.
Fitting estimator with 387 features.
Fitting estimator with 386 features.
Fitting estimator with 385 features.
Fitting estimator with 384 features.
Fitting estimator with 383 features.
Fitting estimator with 382 features.
Fitting estimator with 381 features.
Fitting estimator with 380 features.
Fitting estimator with 379 features.
Fitting estimator with 378 features.
Fitting estimator with 377 features.
Fitting estimator with 376 features.
Fitting estimator with 375 features.
Fitting estimator with 374 features.
Fitting estimator with 373 features.
Fitting estimator with 372 features.
Fitting estimator with 371 features.
Fitting estimator with 370 features.
Fitting estimator with 369 features.
Fitting estimator with 368 features.
Fitting estimator with 367 features.
Fitting estimator with 366 features.
Fitting estimator with 365 features.
Fitting estimator with 364 features.
Fitting estimator with 363 features.
Fitting estimator with 362 features.
Fitting estimator with 361 features.
Fitting estimator with 360 features.
Fitting estimator with 359 features.
Fitting estimator with 358 features.
Fitting estimator with 357 features.
Fitting estimator with 356 features.
Fitting estimator with 355 features.
Fitting estimator with 354 features.
Fitting estimator with 353 features.
Fitting estimator with 352 features.
Fitting estimator with 351 features.
Fitting estimator with 350 features.
Fitting estimator with 349 features.
Fitting estimator with 348 features.
Fitting estimator with 347 features.
Fitting estimator with 346 features.
Fitting estimator with 345 features.
Fitting estimator with 344 features.
Fitting estimator with 343 features.
Fitting estimator with 342 features.
Fitting estimator with 341 features.
Fitting estimator with 340 features.
Fitting estimator with 339 features.
Fitting estimator with 338 features.
Fitting estimator with 337 features.
Fitting estimator with 336 features.
Fitting estimator with 335 features.
Fitting estimator with 334 features.
Fitting estimator with 333 features.
Fitting estimator with 332 features.
Fitting estimator with 331 features.
Fitting estimator with 330 features.
Fitting estimator with 329 features.
Fitting estimator with 328 features.
Fitting estimator with 327 features.
Fitting estimator with 326 features.
Fitting estimator with 325 features.
Fitting estimator with 324 features.
Fitting estimator with 323 features.
Fitting estimator with 322 features.
Fitting estimator with 321 features.
Fitting estimator with 320 features.
Fitting estimator with 319 features.
Fitting estimator with 318 features.
Fitting estimator with 317 features.
Fitting estimator with 316 features.
Fitting estimator with 315 features.
Fitting estimator with 314 features.
Fitting estimator with 313 features.
Fitting estimator with 312 features.
Fitting estimator with 311 features.
Fitting estimator with 310 features.
Fitting estimator with 309 features.
Fitting estimator with 308 features.
Fitting estimator with 307 features.
Fitting estimator with 306 features.
Fitting estimator with 305 features.
Fitting estimator with 304 features.
Fitting estimator with 303 features.
Fitting estimator with 302 features.
Fitting estimator with 301 features.
Fitting estimator with 300 features.
Fitting estimator with 299 features.
Fitting estimator with 298 features.
Fitting estimator with 297 features.
Fitting estimator with 296 features.
Fitting estimator with 295 features.
Fitting estimator with 294 features.
Fitting estimator with 293 features.
Fitting estimator with 292 features.
Fitting estimator with 291 features.
Fitting estimator with 290 features.
Fitting estimator with 289 features.
Fitting estimator with 288 features.
Fitting estimator with 287 features.
Fitting estimator with 286 features.
Fitting estimator with 285 features.
Fitting estimator with 284 features.
Fitting estimator with 283 features.
Fitting estimator with 282 features.
Fitting estimator with 281 features.
Fitting estimator with 280 features.
Fitting estimator with 279 features.
Fitting estimator with 278 features.
Fitting estimator with 277 features.
Fitting estimator with 276 features.
Fitting estimator with 275 features.
Fitting estimator with 274 features.
Fitting estimator with 273 features.
Fitting estimator with 272 features.
Fitting estimator with 271 features.
Fitting estimator with 270 features.
Fitting estimator with 269 features.
Fitting estimator with 268 features.
Fitting estimator with 267 features.
Fitting estimator with 266 features.
Fitting estimator with 265 features.
Fitting estimator with 264 features.
Fitting estimator with 263 features.
Fitting estimator with 262 features.
Fitting estimator with 261 features.
Fitting estimator with 260 features.
Fitting estimator with 259 features.
Fitting estimator with 258 features.
Fitting estimator with 257 features.
Fitting estimator with 256 features.
Fitting estimator with 255 features.
Fitting estimator with 254 features.
Fitting estimator with 253 features.
Fitting estimator with 252 features.
Fitting estimator with 251 features.
Fitting estimator with 250 features.
Fitting estimator with 249 features.
Fitting estimator with 248 features.
Fitting estimator with 247 features.
Fitting estimator with 246 features.
Fitting estimator with 245 features.
Fitting estimator with 244 features.
Fitting estimator with 243 features.
Fitting estimator with 242 features.
Fitting estimator with 241 features.
Fitting estimator with 240 features.
Fitting estimator with 239 features.
Fitting estimator with 238 features.
Fitting estimator with 237 features.
Fitting estimator with 236 features.
Fitting estimator with 235 features.
Fitting estimator with 234 features.
Fitting estimator with 233 features.
Fitting estimator with 232 features.
Fitting estimator with 231 features.
Fitting estimator with 230 features.
Fitting estimator with 229 features.
Fitting estimator with 228 features.
Fitting estimator with 227 features.
Fitting estimator with 226 features.
Fitting estimator with 225 features.
Fitting estimator with 224 features.
Fitting estimator with 223 features.
Fitting estimator with 222 features.
Fitting estimator with 221 features.
Fitting estimator with 220 features.
Fitting estimator with 219 features.
Fitting estimator with 218 features.
Fitting estimator with 217 features.
Fitting estimator with 216 features.
Fitting estimator with 215 features.
Fitting estimator with 214 features.
Fitting estimator with 213 features.
Fitting estimator with 212 features.
Fitting estimator with 211 features.
Fitting estimator with 210 features.
Fitting estimator with 209 features.
Fitting estimator with 208 features.
Fitting estimator with 207 features.
Fitting estimator with 206 features.
Fitting estimator with 205 features.
Fitting estimator with 204 features.
Fitting estimator with 203 features.
Fitting estimator with 202 features.
Fitting estimator with 201 features.
Fitting estimator with 200 features.
Fitting estimator with 199 features.
Fitting estimator with 198 features.
Fitting estimator with 197 features.
Fitting estimator with 196 features.
Fitting estimator with 195 features.
Fitting estimator with 194 features.
Fitting estimator with 193 features.
Fitting estimator with 192 features.
Fitting estimator with 191 features.
Fitting estimator with 190 features.
Fitting estimator with 189 features.
Fitting estimator with 188 features.
Fitting estimator with 187 features.
Fitting estimator with 186 features.
Fitting estimator with 185 features.
Fitting estimator with 184 features.
Fitting estimator with 183 features.
Fitting estimator with 182 features.
Fitting estimator with 181 features.
Fitting estimator with 180 features.
Fitting estimator with 179 features.
Fitting estimator with 178 features.
Fitting estimator with 177 features.
Fitting estimator with 176 features.
Fitting estimator with 175 features.
Fitting estimator with 174 features.
Fitting estimator with 173 features.
Fitting estimator with 172 features.
Fitting estimator with 171 features.
Fitting estimator with 170 features.
Fitting estimator with 169 features.
Fitting estimator with 168 features.
Fitting estimator with 167 features.
Fitting estimator with 166 features.
Fitting estimator with 165 features.
Fitting estimator with 164 features.
Fitting estimator with 163 features.
Fitting estimator with 162 features.
Fitting estimator with 161 features.
Fitting estimator with 160 features.
Fitting estimator with 159 features.
Fitting estimator with 158 features.
Fitting estimator with 157 features.
Fitting estimator with 156 features.
Fitting estimator with 155 features.
Fitting estimator with 154 features.
Fitting estimator with 153 features.
Fitting estimator with 152 features.
Fitting estimator with 151 features.
Fitting estimator with 150 features.
Fitting estimator with 149 features.
Fitting estimator with 148 features.
Fitting estimator with 147 features.
Fitting estimator with 146 features.
Fitting estimator with 145 features.
Fitting estimator with 144 features.
Fitting estimator with 143 features.
Fitting estimator with 142 features.
Fitting estimator with 141 features.
Fitting estimator with 140 features.
Fitting estimator with 139 features.
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
Fitting estimator with 40 features.
Fitting estimator with 39 features.
Fitting estimator with 38 features.
Fitting estimator with 37 features.
Fitting estimator with 36 features.
Fitting estimator with 35 features.
Fitting estimator with 34 features.
Fitting estimator with 33 features.
Fitting estimator with 32 features.
Fitting estimator with 31 features.
Fitting estimator with 30 features.
Fitting estimator with 29 features.
Fitting estimator with 28 features.
Fitting estimator with 27 features.
Fitting estimator with 26 features.
Fitting estimator with 25 features.
Fitting estimator with 24 features.
Fitting estimator with 23 features.
Fitting estimator with 22 features.
Fitting estimator with 21 features.
Fitting estimator with 20 features.
Fitting estimator with 19 features.
Fitting estimator with 18 features.
Fitting estimator with 17 features.
Fitting estimator with 16 features.
Fitting estimator with 15 features.
Fitting estimator with 14 features.
Fitting estimator with 13 features.
Fitting estimator with 12 features.
Fitting estimator with 11 features.
Fitting estimator with 10 features.
Fitting estimator with 9 features.
Fitting estimator with 8 features.
Fitting estimator with 7 features.
Fitting estimator with 6 features.
Fitting estimator with 5 features.
Fitting estimator with 4 features.
Fitting estimator with 3 features.
Fitting estimator with 2 features.
Fitting estimator with 880 features.
Fitting estimator with 879 features.
Fitting estimator with 878 features.
Fitting estimator with 877 features.
Fitting estimator with 876 features.
Fitting estimator with 875 features.
Fitting estimator with 874 features.
Fitting estimator with 873 features.
Fitting estimator with 872 features.
Fitting estimator with 871 features.
Fitting estimator with 870 features.
Fitting estimator with 869 features.
Fitting estimator with 868 features.
Fitting estimator with 867 features.
Fitting estimator with 866 features.
Fitting estimator with 865 features.
Fitting estimator with 864 features.
Fitting estimator with 863 features.
Fitting estimator with 862 features.
Fitting estimator with 861 features.
Fitting estimator with 860 features.
Fitting estimator with 859 features.
Fitting estimator with 858 features.
Fitting estimator with 857 features.
Fitting estimator with 856 features.
Fitting estimator with 855 features.
Fitting estimator with 854 features.
Fitting estimator with 853 features.
Fitting estimator with 852 features.
Fitting estimator with 851 features.
Fitting estimator with 850 features.
Fitting estimator with 849 features.
Fitting estimator with 848 features.
Fitting estimator with 847 features.
Fitting estimator with 846 features.
Fitting estimator with 845 features.
Fitting estimator with 844 features.
Fitting estimator with 843 features.
Fitting estimator with 842 features.
Fitting estimator with 841 features.
Fitting estimator with 840 features.
Fitting estimator with 839 features.
Fitting estimator with 838 features.
Fitting estimator with 837 features.
Fitting estimator with 836 features.
Fitting estimator with 835 features.
Fitting estimator with 834 features.
Fitting estimator with 833 features.
Fitting estimator with 832 features.
Fitting estimator with 831 features.
Fitting estimator with 830 features.
Fitting estimator with 829 features.
Fitting estimator with 828 features.
Fitting estimator with 827 features.
Fitting estimator with 826 features.
Fitting estimator with 825 features.
Fitting estimator with 824 features.
Fitting estimator with 823 features.
Fitting estimator with 822 features.
Fitting estimator with 821 features.
Fitting estimator with 820 features.
Fitting estimator with 819 features.
Fitting estimator with 818 features.
Fitting estimator with 817 features.
Fitting estimator with 816 features.
Fitting estimator with 815 features.
Fitting estimator with 814 features.
Fitting estimator with 813 features.
Fitting estimator with 812 features.
Fitting estimator with 811 features.
Fitting estimator with 810 features.
Fitting estimator with 809 features.
Fitting estimator with 808 features.
Fitting estimator with 807 features.
Fitting estimator with 806 features.
Fitting estimator with 805 features.
Fitting estimator with 804 features.
Fitting estimator with 803 features.
Fitting estimator with 802 features.
Fitting estimator with 801 features.
Fitting estimator with 800 features.
Fitting estimator with 799 features.
Fitting estimator with 798 features.
Fitting estimator with 797 features.
Fitting estimator with 796 features.
Fitting estimator with 795 features.
Fitting estimator with 794 features.
Fitting estimator with 793 features.
Fitting estimator with 792 features.
Fitting estimator with 791 features.
Fitting estimator with 790 features.
Fitting estimator with 789 features.
Fitting estimator with 788 features.
Fitting estimator with 787 features.
Fitting estimator with 786 features.
Fitting estimator with 785 features.
Fitting estimator with 784 features.
Fitting estimator with 783 features.
Fitting estimator with 782 features.
Fitting estimator with 781 features.
Fitting estimator with 780 features.
Fitting estimator with 779 features.
Fitting estimator with 778 features.
Fitting estimator with 777 features.
Fitting estimator with 776 features.
Fitting estimator with 775 features.
Fitting estimator with 774 features.
Fitting estimator with 773 features.
Fitting estimator with 772 features.
Fitting estimator with 771 features.
Fitting estimator with 770 features.
Fitting estimator with 769 features.
Fitting estimator with 768 features.
Fitting estimator with 767 features.
Fitting estimator with 766 features.
Fitting estimator with 765 features.
Fitting estimator with 764 features.
Fitting estimator with 763 features.
Fitting estimator with 762 features.
Fitting estimator with 761 features.
Fitting estimator with 760 features.
Fitting estimator with 759 features.
Fitting estimator with 758 features.
Fitting estimator with 757 features.
Fitting estimator with 756 features.
Fitting estimator with 755 features.
Fitting estimator with 754 features.
Fitting estimator with 753 features.
Fitting estimator with 752 features.
Fitting estimator with 751 features.
Fitting estimator with 750 features.
Fitting estimator with 749 features.
Fitting estimator with 748 features.
Fitting estimator with 747 features.
Fitting estimator with 746 features.
Fitting estimator with 745 features.
Fitting estimator with 744 features.
Fitting estimator with 743 features.
Fitting estimator with 742 features.
Fitting estimator with 741 features.
Fitting estimator with 740 features.
Fitting estimator with 739 features.
Fitting estimator with 738 features.
Fitting estimator with 737 features.
Fitting estimator with 736 features.
Fitting estimator with 735 features.
Fitting estimator with 734 features.
Fitting estimator with 733 features.
Fitting estimator with 732 features.
Fitting estimator with 731 features.
Fitting estimator with 730 features.
Fitting estimator with 729 features.
Fitting estimator with 728 features.
Fitting estimator with 727 features.
Fitting estimator with 726 features.
Fitting estimator with 725 features.
Fitting estimator with 724 features.
Fitting estimator with 723 features.
Fitting estimator with 722 features.
Fitting estimator with 721 features.
Fitting estimator with 720 features.
Fitting estimator with 719 features.
Fitting estimator with 718 features.
Fitting estimator with 717 features.
Fitting estimator with 716 features.
Fitting estimator with 715 features.
Fitting estimator with 714 features.
Fitting estimator with 713 features.
Fitting estimator with 712 features.
Fitting estimator with 711 features.
Fitting estimator with 710 features.
Fitting estimator with 709 features.
Fitting estimator with 708 features.
Fitting estimator with 707 features.
Fitting estimator with 706 features.
Fitting estimator with 705 features.
Fitting estimator with 704 features.
Fitting estimator with 703 features.
Fitting estimator with 702 features.
Fitting estimator with 701 features.
Fitting estimator with 700 features.
Fitting estimator with 699 features.
Fitting estimator with 698 features.
Fitting estimator with 697 features.
Fitting estimator with 696 features.
Fitting estimator with 695 features.
Fitting estimator with 694 features.
Fitting estimator with 693 features.
Fitting estimator with 692 features.
Fitting estimator with 691 features.
Fitting estimator with 690 features.
Fitting estimator with 689 features.
Fitting estimator with 688 features.
Fitting estimator with 687 features.
Fitting estimator with 686 features.
Fitting estimator with 685 features.
Fitting estimator with 684 features.
Fitting estimator with 683 features.
Fitting estimator with 682 features.
Fitting estimator with 681 features.
Fitting estimator with 680 features.
Fitting estimator with 679 features.
Fitting estimator with 678 features.
Fitting estimator with 677 features.
Fitting estimator with 676 features.
Fitting estimator with 675 features.
Fitting estimator with 674 features.
Fitting estimator with 673 features.
Fitting estimator with 672 features.
Fitting estimator with 671 features.
Fitting estimator with 670 features.
Fitting estimator with 669 features.
Fitting estimator with 668 features.
Fitting estimator with 667 features.
Fitting estimator with 666 features.
Fitting estimator with 665 features.
Fitting estimator with 664 features.
Fitting estimator with 663 features.
Fitting estimator with 662 features.
Fitting estimator with 661 features.
Fitting estimator with 660 features.
Fitting estimator with 659 features.
Fitting estimator with 658 features.
Fitting estimator with 657 features.
Fitting estimator with 656 features.
Fitting estimator with 655 features.
Fitting estimator with 654 features.
Fitting estimator with 653 features.
Fitting estimator with 652 features.
Fitting estimator with 651 features.
Fitting estimator with 650 features.
Fitting estimator with 649 features.
Fitting estimator with 648 features.
Fitting estimator with 647 features.
Fitting estimator with 646 features.
Fitting estimator with 645 features.
Fitting estimator with 644 features.
Fitting estimator with 643 features.
Fitting estimator with 642 features.
Fitting estimator with 641 features.
Fitting estimator with 640 features.
Fitting estimator with 639 features.
Fitting estimator with 638 features.
Fitting estimator with 637 features.
Fitting estimator with 636 features.
Fitting estimator with 635 features.
Fitting estimator with 634 features.
Fitting estimator with 633 features.
Fitting estimator with 632 features.
Fitting estimator with 631 features.
Fitting estimator with 630 features.
Fitting estimator with 629 features.
Fitting estimator with 628 features.
Fitting estimator with 627 features.
Fitting estimator with 626 features.
Fitting estimator with 625 features.
Fitting estimator with 624 features.
Fitting estimator with 623 features.
Fitting estimator with 622 features.
Fitting estimator with 621 features.
Fitting estimator with 620 features.
Fitting estimator with 619 features.
Fitting estimator with 618 features.
Fitting estimator with 617 features.
Fitting estimator with 616 features.
Fitting estimator with 615 features.
Fitting estimator with 614 features.
Fitting estimator with 613 features.
Fitting estimator with 612 features.
Fitting estimator with 611 features.
Fitting estimator with 610 features.
Fitting estimator with 609 features.
Fitting estimator with 608 features.
Fitting estimator with 607 features.
Fitting estimator with 606 features.
Fitting estimator with 605 features.
Fitting estimator with 604 features.
Fitting estimator with 603 features.
Fitting estimator with 602 features.
Fitting estimator with 601 features.
Fitting estimator with 600 features.
Fitting estimator with 599 features.
Fitting estimator with 598 features.
Fitting estimator with 597 features.
Fitting estimator with 596 features.
Fitting estimator with 595 features.
Fitting estimator with 594 features.
Fitting estimator with 593 features.
Fitting estimator with 592 features.
Fitting estimator with 591 features.
Fitting estimator with 590 features.
Fitting estimator with 589 features.
Fitting estimator with 588 features.
Fitting estimator with 587 features.
Fitting estimator with 586 features.
Fitting estimator with 585 features.
Fitting estimator with 584 features.
Fitting estimator with 583 features.
Fitting estimator with 582 features.
Fitting estimator with 581 features.
Fitting estimator with 580 features.
Fitting estimator with 579 features.
Fitting estimator with 578 features.
Fitting estimator with 577 features.
Fitting estimator with 576 features.
Fitting estimator with 575 features.
Fitting estimator with 574 features.
Fitting estimator with 573 features.
Fitting estimator with 572 features.
Fitting estimator with 571 features.
Fitting estimator with 570 features.
Fitting estimator with 569 features.
Fitting estimator with 568 features.
Fitting estimator with 567 features.
Fitting estimator with 566 features.
Fitting estimator with 565 features.
Fitting estimator with 564 features.
Fitting estimator with 563 features.
Fitting estimator with 562 features.
Fitting estimator with 561 features.
Fitting estimator with 560 features.
Fitting estimator with 559 features.
Fitting estimator with 558 features.
Fitting estimator with 557 features.
Fitting estimator with 556 features.
Fitting estimator with 555 features.
Fitting estimator with 554 features.
Fitting estimator with 553 features.
Fitting estimator with 552 features.
Fitting estimator with 551 features.
Fitting estimator with 550 features.
Fitting estimator with 549 features.
Fitting estimator with 548 features.
Fitting estimator with 547 features.
Fitting estimator with 546 features.
Fitting estimator with 545 features.
Fitting estimator with 544 features.
Fitting estimator with 543 features.
Fitting estimator with 542 features.
Fitting estimator with 541 features.
Fitting estimator with 540 features.
Fitting estimator with 539 features.
Fitting estimator with 538 features.
Fitting estimator with 537 features.
Fitting estimator with 536 features.
Fitting estimator with 535 features.
Fitting estimator with 534 features.
Fitting estimator with 533 features.
Fitting estimator with 532 features.
Fitting estimator with 531 features.
Fitting estimator with 530 features.
Fitting estimator with 529 features.
Fitting estimator with 528 features.
Fitting estimator with 527 features.
Fitting estimator with 526 features.
Fitting estimator with 525 features.
Fitting estimator with 524 features.
Fitting estimator with 523 features.
Fitting estimator with 522 features.
Fitting estimator with 521 features.
Fitting estimator with 520 features.
Fitting estimator with 519 features.
Fitting estimator with 518 features.
Fitting estimator with 517 features.
Fitting estimator with 516 features.
Fitting estimator with 515 features.
Fitting estimator with 514 features.
Fitting estimator with 513 features.
Fitting estimator with 512 features.
Fitting estimator with 511 features.
Fitting estimator with 510 features.
Fitting estimator with 509 features.
Fitting estimator with 508 features.
Fitting estimator with 507 features.
Fitting estimator with 506 features.
Fitting estimator with 505 features.
Fitting estimator with 504 features.
Fitting estimator with 503 features.
Fitting estimator with 502 features.
Fitting estimator with 501 features.
Fitting estimator with 500 features.
Fitting estimator with 499 features.
Fitting estimator with 498 features.
Fitting estimator with 497 features.
Fitting estimator with 496 features.
Fitting estimator with 495 features.
Fitting estimator with 494 features.
Fitting estimator with 493 features.
Fitting estimator with 492 features.
Fitting estimator with 491 features.
Fitting estimator with 490 features.
Fitting estimator with 489 features.
Fitting estimator with 488 features.
Fitting estimator with 487 features.
Fitting estimator with 486 features.
Fitting estimator with 485 features.
Fitting estimator with 484 features.
Fitting estimator with 483 features.
Fitting estimator with 482 features.
Fitting estimator with 481 features.
Fitting estimator with 480 features.
Fitting estimator with 479 features.
Fitting estimator with 478 features.
Fitting estimator with 477 features.
Fitting estimator with 476 features.
Fitting estimator with 475 features.
Fitting estimator with 474 features.
Fitting estimator with 473 features.
Fitting estimator with 472 features.
Fitting estimator with 471 features.
Fitting estimator with 470 features.
Fitting estimator with 469 features.
Fitting estimator with 468 features.
Fitting estimator with 467 features.
Fitting estimator with 466 features.
Fitting estimator with 465 features.
Fitting estimator with 464 features.
Fitting estimator with 463 features.
Fitting estimator with 462 features.
Fitting estimator with 461 features.
Fitting estimator with 460 features.
Fitting estimator with 459 features.
Fitting estimator with 458 features.
Fitting estimator with 457 features.
Fitting estimator with 456 features.
Fitting estimator with 455 features.
Fitting estimator with 454 features.
Fitting estimator with 453 features.
Fitting estimator with 452 features.
Fitting estimator with 451 features.
Fitting estimator with 450 features.
Fitting estimator with 449 features.
Fitting estimator with 448 features.
Fitting estimator with 447 features.
Fitting estimator with 446 features.
Fitting estimator with 445 features.
Fitting estimator with 444 features.
Fitting estimator with 443 features.
Fitting estimator with 442 features.
Fitting estimator with 441 features.
Fitting estimator with 440 features.
Fitting estimator with 439 features.
Fitting estimator with 438 features.
Fitting estimator with 437 features.
Fitting estimator with 436 features.
Fitting estimator with 435 features.
Fitting estimator with 434 features.
Fitting estimator with 433 features.
Fitting estimator with 432 features.
Fitting estimator with 431 features.
Fitting estimator with 430 features.
Fitting estimator with 429 features.
Fitting estimator with 428 features.
Fitting estimator with 427 features.
Fitting estimator with 426 features.
Fitting estimator with 425 features.
Fitting estimator with 424 features.
Fitting estimator with 423 features.
Fitting estimator with 422 features.
Fitting estimator with 421 features.
Fitting estimator with 420 features.
Fitting estimator with 419 features.
Fitting estimator with 418 features.
Fitting estimator with 417 features.
Fitting estimator with 416 features.
Fitting estimator with 415 features.
Fitting estimator with 414 features.
Fitting estimator with 413 features.
Fitting estimator with 412 features.
Fitting estimator with 411 features.
Fitting estimator with 410 features.
Fitting estimator with 409 features.
Fitting estimator with 408 features.
Fitting estimator with 407 features.
Fitting estimator with 406 features.
Fitting estimator with 405 features.
Fitting estimator with 404 features.
Fitting estimator with 403 features.
Fitting estimator with 402 features.
Fitting estimator with 401 features.
Fitting estimator with 400 features.
Fitting estimator with 399 features.
Fitting estimator with 398 features.
Fitting estimator with 397 features.
Fitting estimator with 396 features.
Fitting estimator with 395 features.
Fitting estimator with 394 features.
Fitting estimator with 393 features.
Fitting estimator with 392 features.
Fitting estimator with 391 features.
Fitting estimator with 390 features.
Fitting estimator with 389 features.
Fitting estimator with 388 features.
Fitting estimator with 387 features.
Fitting estimator with 386 features.
Fitting estimator with 385 features.
Fitting estimator with 384 features.
Fitting estimator with 383 features.
Fitting estimator with 382 features.
Fitting estimator with 381 features.
Fitting estimator with 380 features.
Fitting estimator with 379 features.
Fitting estimator with 378 features.
Fitting estimator with 377 features.
Fitting estimator with 376 features.
Fitting estimator with 375 features.
Fitting estimator with 374 features.
Fitting estimator with 373 features.
Fitting estimator with 372 features.
Fitting estimator with 371 features.
Fitting estimator with 370 features.
Fitting estimator with 369 features.
Fitting estimator with 368 features.
Fitting estimator with 367 features.
Fitting estimator with 366 features.
Fitting estimator with 365 features.
Fitting estimator with 364 features.
Fitting estimator with 363 features.
Fitting estimator with 362 features.
Fitting estimator with 361 features.
Fitting estimator with 360 features.
Fitting estimator with 359 features.
Fitting estimator with 358 features.
Fitting estimator with 357 features.
Fitting estimator with 356 features.
Fitting estimator with 355 features.
Fitting estimator with 354 features.
Fitting estimator with 353 features.
Fitting estimator with 352 features.
Fitting estimator with 351 features.
Fitting estimator with 350 features.
Fitting estimator with 349 features.
Fitting estimator with 348 features.
Fitting estimator with 347 features.
Fitting estimator with 346 features.
Fitting estimator with 345 features.
Fitting estimator with 344 features.
Fitting estimator with 343 features.
Fitting estimator with 342 features.
Fitting estimator with 341 features.
Fitting estimator with 340 features.
Fitting estimator with 339 features.
Fitting estimator with 338 features.
Fitting estimator with 337 features.
Fitting estimator with 336 features.
Fitting estimator with 335 features.
Fitting estimator with 334 features.
Fitting estimator with 333 features.
Fitting estimator with 332 features.
Fitting estimator with 331 features.
Fitting estimator with 330 features.
Fitting estimator with 329 features.
Fitting estimator with 328 features.
Fitting estimator with 327 features.
Fitting estimator with 326 features.
Fitting estimator with 325 features.
Fitting estimator with 324 features.
Fitting estimator with 323 features.
Fitting estimator with 322 features.
Fitting estimator with 321 features.
Fitting estimator with 320 features.
Fitting estimator with 319 features.
Fitting estimator with 318 features.
Fitting estimator with 317 features.
Fitting estimator with 316 features.
Fitting estimator with 315 features.
Fitting estimator with 314 features.
Fitting estimator with 313 features.
Fitting estimator with 312 features.
Fitting estimator with 311 features.
Fitting estimator with 310 features.
Fitting estimator with 309 features.
Fitting estimator with 308 features.
Fitting estimator with 307 features.
Fitting estimator with 306 features.
Fitting estimator with 305 features.
Fitting estimator with 304 features.
Fitting estimator with 303 features.
Fitting estimator with 302 features.
Fitting estimator with 301 features.
Fitting estimator with 300 features.
Fitting estimator with 299 features.
Fitting estimator with 298 features.
Fitting estimator with 297 features.
Fitting estimator with 296 features.
Fitting estimator with 295 features.
Fitting estimator with 294 features.
Fitting estimator with 293 features.
Fitting estimator with 292 features.
Fitting estimator with 291 features.
Fitting estimator with 290 features.
Fitting estimator with 289 features.
Fitting estimator with 288 features.
Fitting estimator with 287 features.
Fitting estimator with 286 features.
Fitting estimator with 285 features.
Fitting estimator with 284 features.
Fitting estimator with 283 features.
Fitting estimator with 282 features.
Fitting estimator with 281 features.
Fitting estimator with 280 features.
Fitting estimator with 279 features.
Fitting estimator with 278 features.
Fitting estimator with 277 features.
Fitting estimator with 276 features.
Fitting estimator with 275 features.
Fitting estimator with 274 features.
Fitting estimator with 273 features.
Fitting estimator with 272 features.
Fitting estimator with 271 features.
Fitting estimator with 270 features.
Fitting estimator with 269 features.
Fitting estimator with 268 features.
Fitting estimator with 267 features.
Fitting estimator with 266 features.
Fitting estimator with 265 features.
Fitting estimator with 264 features.
Fitting estimator with 263 features.
Fitting estimator with 262 features.
Fitting estimator with 261 features.
Fitting estimator with 260 features.
Fitting estimator with 259 features.
Fitting estimator with 258 features.
Fitting estimator with 257 features.
Fitting estimator with 256 features.
Fitting estimator with 255 features.
Fitting estimator with 254 features.
Fitting estimator with 253 features.
Fitting estimator with 252 features.
Fitting estimator with 251 features.
Fitting estimator with 250 features.
Fitting estimator with 249 features.
Fitting estimator with 248 features.
Fitting estimator with 247 features.
Fitting estimator with 246 features.
Fitting estimator with 245 features.
Fitting estimator with 244 features.
Fitting estimator with 243 features.
Fitting estimator with 242 features.
Fitting estimator with 241 features.
Fitting estimator with 240 features.
Fitting estimator with 239 features.
Fitting estimator with 238 features.
Fitting estimator with 237 features.
Fitting estimator with 236 features.
Fitting estimator with 235 features.
Fitting estimator with 234 features.
Fitting estimator with 233 features.
Fitting estimator with 232 features.
Fitting estimator with 231 features.
Fitting estimator with 230 features.
Fitting estimator with 229 features.
Fitting estimator with 228 features.
Fitting estimator with 227 features.
Fitting estimator with 226 features.
Fitting estimator with 225 features.
Fitting estimator with 224 features.
Fitting estimator with 223 features.
Fitting estimator with 222 features.
Fitting estimator with 221 features.
Fitting estimator with 220 features.
Fitting estimator with 219 features.
Fitting estimator with 218 features.
Fitting estimator with 217 features.
Fitting estimator with 216 features.
Fitting estimator with 215 features.
Fitting estimator with 214 features.
Fitting estimator with 213 features.
Fitting estimator with 212 features.
Fitting estimator with 211 features.
Fitting estimator with 210 features.
Fitting estimator with 209 features.
Fitting estimator with 208 features.
Fitting estimator with 207 features.
Fitting estimator with 206 features.
Fitting estimator with 205 features.
Fitting estimator with 204 features.
Fitting estimator with 203 features.
Fitting estimator with 202 features.
Fitting estimator with 201 features.
Fitting estimator with 200 features.
Fitting estimator with 199 features.
Fitting estimator with 198 features.
Fitting estimator with 197 features.
Fitting estimator with 196 features.
Fitting estimator with 195 features.
Fitting estimator with 194 features.
Fitting estimator with 193 features.
Fitting estimator with 192 features.
Fitting estimator with 191 features.
Fitting estimator with 190 features.
Fitting estimator with 189 features.
Fitting estimator with 188 features.
Fitting estimator with 187 features.
Fitting estimator with 186 features.
Fitting estimator with 185 features.
Fitting estimator with 184 features.
Fitting estimator with 183 features.
Fitting estimator with 182 features.
Fitting estimator with 181 features.
Fitting estimator with 180 features.
Fitting estimator with 179 features.
Fitting estimator with 178 features.
Fitting estimator with 177 features.
Fitting estimator with 176 features.
Fitting estimator with 175 features.
Fitting estimator with 174 features.
Fitting estimator with 173 features.
Fitting estimator with 172 features.
Fitting estimator with 171 features.
Fitting estimator with 170 features.
Fitting estimator with 169 features.
Fitting estimator with 168 features.
Fitting estimator with 167 features.
Fitting estimator with 166 features.
Fitting estimator with 165 features.
Fitting estimator with 164 features.
Fitting estimator with 163 features.
Fitting estimator with 162 features.
Fitting estimator with 161 features.
Fitting estimator with 160 features.
Fitting estimator with 159 features.
Fitting estimator with 158 features.
Fitting estimator with 157 features.
Fitting estimator with 156 features.
Fitting estimator with 155 features.
Fitting estimator with 154 features.
Fitting estimator with 153 features.
Fitting estimator with 152 features.
Fitting estimator with 151 features.
Fitting estimator with 150 features.
Fitting estimator with 149 features.
Fitting estimator with 148 features.
Fitting estimator with 147 features.
Fitting estimator with 146 features.
Fitting estimator with 145 features.
Fitting estimator with 144 features.
Fitting estimator with 143 features.
Fitting estimator with 142 features.
Fitting estimator with 141 features.
Fitting estimator with 140 features.
Fitting estimator with 139 features.
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
Fitting estimator with 40 features.
Fitting estimator with 39 features.
Fitting estimator with 38 features.
Fitting estimator with 37 features.
Fitting estimator with 36 features.
Fitting estimator with 35 features.
Fitting estimator with 34 features.
Fitting estimator with 33 features.
Fitting estimator with 32 features.
Fitting estimator with 31 features.
Fitting estimator with 30 features.
Fitting estimator with 29 features.
Fitting estimator with 28 features.
Fitting estimator with 27 features.
Fitting estimator with 26 features.
Fitting estimator with 25 features.
Fitting estimator with 24 features.
Fitting estimator with 23 features.
Fitting estimator with 22 features.
Fitting estimator with 21 features.
Fitting estimator with 20 features.
Fitting estimator with 19 features.
Fitting estimator with 18 features.
Fitting estimator with 17 features.
Fitting estimator with 16 features.
Fitting estimator with 15 features.
Fitting estimator with 14 features.
Fitting estimator with 13 features.
Fitting estimator with 12 features.
Fitting estimator with 11 features.
Fitting estimator with 10 features.
Fitting estimator with 9 features.
Fitting estimator with 8 features.
Fitting estimator with 7 features.
Fitting estimator with 6 features.
Fitting estimator with 5 features.
Fitting estimator with 4 features.
Fitting estimator with 3 features.
Fitting estimator with 2 features.
Fitting estimator with 880 features.
Fitting estimator with 879 features.
Fitting estimator with 878 features.
Fitting estimator with 877 features.
Fitting estimator with 876 features.
Fitting estimator with 875 features.
Fitting estimator with 874 features.
Fitting estimator with 873 features.
Fitting estimator with 872 features.
Fitting estimator with 871 features.
Fitting estimator with 870 features.
Fitting estimator with 869 features.
Fitting estimator with 868 features.
Fitting estimator with 867 features.
Fitting estimator with 866 features.
Fitting estimator with 865 features.
Fitting estimator with 864 features.
Fitting estimator with 863 features.
Fitting estimator with 862 features.
Fitting estimator with 861 features.
Fitting estimator with 860 features.
Fitting estimator with 859 features.
Fitting estimator with 858 features.
Fitting estimator with 857 features.
Fitting estimator with 856 features.
Fitting estimator with 855 features.
Fitting estimator with 854 features.
Fitting estimator with 853 features.
Fitting estimator with 852 features.
Fitting estimator with 851 features.
Fitting estimator with 850 features.
Fitting estimator with 849 features.
Fitting estimator with 848 features.
Fitting estimator with 847 features.
Fitting estimator with 846 features.
Fitting estimator with 845 features.
Fitting estimator with 844 features.
Fitting estimator with 843 features.
Fitting estimator with 842 features.
Fitting estimator with 841 features.
Fitting estimator with 840 features.
Fitting estimator with 839 features.
Fitting estimator with 838 features.
Fitting estimator with 837 features.
Fitting estimator with 836 features.
Fitting estimator with 835 features.
Fitting estimator with 834 features.
Fitting estimator with 833 features.
Fitting estimator with 832 features.
Fitting estimator with 831 features.
Fitting estimator with 830 features.
Fitting estimator with 829 features.
Fitting estimator with 828 features.
Fitting estimator with 827 features.
Fitting estimator with 826 features.
Fitting estimator with 825 features.
Fitting estimator with 824 features.
Fitting estimator with 823 features.
Fitting estimator with 822 features.
Fitting estimator with 821 features.
Fitting estimator with 820 features.
Fitting estimator with 819 features.
Fitting estimator with 818 features.
Fitting estimator with 817 features.
Fitting estimator with 816 features.
Fitting estimator with 815 features.
Fitting estimator with 814 features.
Fitting estimator with 813 features.
Fitting estimator with 812 features.
Fitting estimator with 811 features.
Fitting estimator with 810 features.
Fitting estimator with 809 features.
Fitting estimator with 808 features.
Fitting estimator with 807 features.
Fitting estimator with 806 features.
Fitting estimator with 805 features.
Fitting estimator with 804 features.
Fitting estimator with 803 features.
Fitting estimator with 802 features.
Fitting estimator with 801 features.
Fitting estimator with 800 features.
Fitting estimator with 799 features.
Fitting estimator with 798 features.
Fitting estimator with 797 features.
Fitting estimator with 796 features.
Fitting estimator with 795 features.
Fitting estimator with 794 features.
Fitting estimator with 793 features.
Fitting estimator with 792 features.
Fitting estimator with 791 features.
Fitting estimator with 790 features.
Fitting estimator with 789 features.
Fitting estimator with 788 features.
Fitting estimator with 787 features.
Fitting estimator with 786 features.
Fitting estimator with 785 features.
Fitting estimator with 784 features.
Fitting estimator with 783 features.
Fitting estimator with 782 features.
Fitting estimator with 781 features.
Fitting estimator with 780 features.
Fitting estimator with 779 features.
Fitting estimator with 778 features.
Fitting estimator with 777 features.
Fitting estimator with 776 features.
Fitting estimator with 775 features.
Fitting estimator with 774 features.
Fitting estimator with 773 features.
Fitting estimator with 772 features.
Fitting estimator with 771 features.
Fitting estimator with 770 features.
Fitting estimator with 769 features.
Fitting estimator with 768 features.
Fitting estimator with 767 features.
Fitting estimator with 766 features.
Fitting estimator with 765 features.
Fitting estimator with 764 features.
Fitting estimator with 763 features.
Fitting estimator with 762 features.
Fitting estimator with 761 features.
Fitting estimator with 760 features.
Fitting estimator with 759 features.
Fitting estimator with 758 features.
Fitting estimator with 757 features.
Fitting estimator with 756 features.
Fitting estimator with 755 features.
Fitting estimator with 754 features.
Fitting estimator with 753 features.
Fitting estimator with 752 features.
Fitting estimator with 751 features.
Fitting estimator with 750 features.
Fitting estimator with 749 features.
Fitting estimator with 748 features.
Fitting estimator with 747 features.
Fitting estimator with 746 features.
Fitting estimator with 745 features.
Fitting estimator with 744 features.
Fitting estimator with 743 features.
Fitting estimator with 742 features.
Fitting estimator with 741 features.
Fitting estimator with 740 features.
Fitting estimator with 739 features.
Fitting estimator with 738 features.
Fitting estimator with 737 features.
Fitting estimator with 736 features.
Fitting estimator with 735 features.
Fitting estimator with 734 features.
Fitting estimator with 733 features.
Fitting estimator with 732 features.
Fitting estimator with 731 features.
Fitting estimator with 730 features.
Fitting estimator with 729 features.
Fitting estimator with 728 features.
Fitting estimator with 727 features.
Fitting estimator with 726 features.
Fitting estimator with 725 features.
Fitting estimator with 724 features.
Fitting estimator with 723 features.
Fitting estimator with 722 features.
Fitting estimator with 721 features.
Fitting estimator with 720 features.
Fitting estimator with 719 features.
Fitting estimator with 718 features.
Fitting estimator with 717 features.
Fitting estimator with 716 features.
Fitting estimator with 715 features.
Fitting estimator with 714 features.
Fitting estimator with 713 features.
Fitting estimator with 712 features.
Fitting estimator with 711 features.
Fitting estimator with 710 features.
Fitting estimator with 709 features.
Fitting estimator with 708 features.
Fitting estimator with 707 features.
Fitting estimator with 706 features.
Fitting estimator with 705 features.
Fitting estimator with 704 features.
Fitting estimator with 703 features.
Fitting estimator with 702 features.
Fitting estimator with 701 features.
Fitting estimator with 700 features.
Fitting estimator with 699 features.
Fitting estimator with 698 features.
Fitting estimator with 697 features.
Fitting estimator with 696 features.
Fitting estimator with 695 features.
Fitting estimator with 694 features.
Fitting estimator with 693 features.
Fitting estimator with 692 features.
Fitting estimator with 691 features.
Fitting estimator with 690 features.
Fitting estimator with 689 features.
Fitting estimator with 688 features.
Fitting estimator with 687 features.
Fitting estimator with 686 features.
Fitting estimator with 685 features.
Fitting estimator with 684 features.
Fitting estimator with 683 features.
Fitting estimator with 682 features.
Fitting estimator with 681 features.
Fitting estimator with 680 features.
Fitting estimator with 679 features.
Fitting estimator with 678 features.
Fitting estimator with 677 features.
Fitting estimator with 676 features.
Fitting estimator with 675 features.
Fitting estimator with 674 features.
Fitting estimator with 673 features.
Fitting estimator with 672 features.
Fitting estimator with 671 features.
Fitting estimator with 670 features.
Fitting estimator with 669 features.
Fitting estimator with 668 features.
Fitting estimator with 667 features.
Fitting estimator with 666 features.
Fitting estimator with 665 features.
Fitting estimator with 664 features.
Fitting estimator with 663 features.
Fitting estimator with 662 features.
Fitting estimator with 661 features.
Fitting estimator with 660 features.
Fitting estimator with 659 features.
Fitting estimator with 658 features.
Fitting estimator with 657 features.
Fitting estimator with 656 features.
Fitting estimator with 655 features.
Fitting estimator with 654 features.
Fitting estimator with 653 features.
Fitting estimator with 652 features.
Fitting estimator with 651 features.
Fitting estimator with 650 features.
Fitting estimator with 649 features.
Fitting estimator with 648 features.
Fitting estimator with 647 features.
Fitting estimator with 646 features.
Fitting estimator with 645 features.
Fitting estimator with 644 features.
Fitting estimator with 643 features.
Fitting estimator with 642 features.
Fitting estimator with 641 features.
Fitting estimator with 640 features.
Fitting estimator with 639 features.
Fitting estimator with 638 features.
Fitting estimator with 637 features.
Fitting estimator with 636 features.
Fitting estimator with 635 features.
Fitting estimator with 634 features.
Fitting estimator with 633 features.
Fitting estimator with 632 features.
Fitting estimator with 631 features.
Fitting estimator with 630 features.
Fitting estimator with 629 features.
Fitting estimator with 628 features.
Fitting estimator with 627 features.
Fitting estimator with 626 features.
Fitting estimator with 625 features.
Fitting estimator with 624 features.
Fitting estimator with 623 features.
Fitting estimator with 622 features.
Fitting estimator with 621 features.
Fitting estimator with 620 features.
Fitting estimator with 619 features.
Fitting estimator with 618 features.
Fitting estimator with 617 features.
Fitting estimator with 616 features.
Fitting estimator with 615 features.
Fitting estimator with 614 features.
Fitting estimator with 613 features.
Fitting estimator with 612 features.
Fitting estimator with 611 features.
Fitting estimator with 610 features.
Fitting estimator with 609 features.
Fitting estimator with 608 features.
Fitting estimator with 607 features.
Fitting estimator with 606 features.
Fitting estimator with 605 features.
Fitting estimator with 604 features.
Fitting estimator with 603 features.
Fitting estimator with 602 features.
Fitting estimator with 601 features.
Fitting estimator with 600 features.
Fitting estimator with 599 features.
Fitting estimator with 598 features.
Fitting estimator with 597 features.
Fitting estimator with 596 features.
Fitting estimator with 595 features.
Fitting estimator with 594 features.
Fitting estimator with 593 features.
Fitting estimator with 592 features.
Fitting estimator with 591 features.
Fitting estimator with 590 features.
Fitting estimator with 589 features.
Fitting estimator with 588 features.
Fitting estimator with 587 features.
Fitting estimator with 586 features.
Fitting estimator with 585 features.
Fitting estimator with 584 features.
Fitting estimator with 583 features.
Fitting estimator with 582 features.
Fitting estimator with 581 features.
Fitting estimator with 580 features.
Fitting estimator with 579 features.
Fitting estimator with 578 features.
Fitting estimator with 577 features.
Fitting estimator with 576 features.
Fitting estimator with 575 features.
Fitting estimator with 574 features.
Fitting estimator with 573 features.
Fitting estimator with 572 features.
Fitting estimator with 571 features.
Fitting estimator with 570 features.
Fitting estimator with 569 features.
Fitting estimator with 568 features.
Fitting estimator with 567 features.
Fitting estimator with 566 features.
Fitting estimator with 565 features.
Fitting estimator with 564 features.
Fitting estimator with 563 features.
Fitting estimator with 562 features.
Fitting estimator with 561 features.
Fitting estimator with 560 features.
Fitting estimator with 559 features.
Fitting estimator with 558 features.
Fitting estimator with 557 features.
Fitting estimator with 556 features.
Fitting estimator with 555 features.
Fitting estimator with 554 features.
Fitting estimator with 553 features.
Fitting estimator with 552 features.
Fitting estimator with 551 features.
Fitting estimator with 550 features.
Fitting estimator with 549 features.
Fitting estimator with 548 features.
Fitting estimator with 547 features.
Fitting estimator with 546 features.
Fitting estimator with 545 features.
Fitting estimator with 544 features.
Fitting estimator with 543 features.
Fitting estimator with 542 features.
Fitting estimator with 541 features.
Fitting estimator with 540 features.
Fitting estimator with 539 features.
Fitting estimator with 538 features.
Fitting estimator with 537 features.
Fitting estimator with 536 features.
Fitting estimator with 535 features.
Fitting estimator with 534 features.
Fitting estimator with 533 features.
Fitting estimator with 532 features.
Fitting estimator with 531 features.
Fitting estimator with 530 features.
Fitting estimator with 529 features.
Fitting estimator with 528 features.
Fitting estimator with 527 features.
Fitting estimator with 526 features.
Fitting estimator with 525 features.
Fitting estimator with 524 features.
Fitting estimator with 523 features.
Fitting estimator with 522 features.
Fitting estimator with 521 features.
Fitting estimator with 520 features.
Fitting estimator with 519 features.
Fitting estimator with 518 features.
Fitting estimator with 517 features.
Fitting estimator with 516 features.
Fitting estimator with 515 features.
Fitting estimator with 514 features.
Fitting estimator with 513 features.
Fitting estimator with 512 features.
Fitting estimator with 511 features.
Fitting estimator with 510 features.
Fitting estimator with 509 features.
Fitting estimator with 508 features.
Fitting estimator with 507 features.
Fitting estimator with 506 features.
Fitting estimator with 505 features.
Fitting estimator with 504 features.
Fitting estimator with 503 features.
Fitting estimator with 502 features.
Fitting estimator with 501 features.
Fitting estimator with 500 features.
Fitting estimator with 499 features.
Fitting estimator with 498 features.
Fitting estimator with 497 features.
Fitting estimator with 496 features.
Fitting estimator with 495 features.
Fitting estimator with 494 features.
Fitting estimator with 493 features.
Fitting estimator with 492 features.
Fitting estimator with 491 features.
Fitting estimator with 490 features.
Fitting estimator with 489 features.
Fitting estimator with 488 features.
Fitting estimator with 487 features.
Fitting estimator with 486 features.
Fitting estimator with 485 features.
Fitting estimator with 484 features.
Fitting estimator with 483 features.
Fitting estimator with 482 features.
Fitting estimator with 481 features.
Fitting estimator with 480 features.
Fitting estimator with 479 features.
Fitting estimator with 478 features.
Fitting estimator with 477 features.
Fitting estimator with 476 features.
Fitting estimator with 475 features.
Fitting estimator with 474 features.
Fitting estimator with 473 features.
Fitting estimator with 472 features.
Fitting estimator with 471 features.
Fitting estimator with 470 features.
Fitting estimator with 469 features.
Fitting estimator with 468 features.
Fitting estimator with 467 features.
Fitting estimator with 466 features.
Fitting estimator with 465 features.
Fitting estimator with 464 features.
Fitting estimator with 463 features.
Fitting estimator with 462 features.
Fitting estimator with 461 features.
Fitting estimator with 460 features.
Fitting estimator with 459 features.
Fitting estimator with 458 features.
Fitting estimator with 457 features.
Fitting estimator with 456 features.
Fitting estimator with 455 features.
Fitting estimator with 454 features.
Fitting estimator with 453 features.
Fitting estimator with 452 features.
Fitting estimator with 451 features.
Fitting estimator with 450 features.
Fitting estimator with 449 features.
Fitting estimator with 448 features.
Fitting estimator with 447 features.
Fitting estimator with 446 features.
Fitting estimator with 445 features.
Fitting estimator with 444 features.
Fitting estimator with 443 features.
Fitting estimator with 442 features.
Fitting estimator with 441 features.
Fitting estimator with 440 features.
Fitting estimator with 439 features.
Fitting estimator with 438 features.
Fitting estimator with 437 features.
Fitting estimator with 436 features.
Fitting estimator with 435 features.
Fitting estimator with 434 features.
Fitting estimator with 433 features.
Fitting estimator with 432 features.
Fitting estimator with 431 features.
Fitting estimator with 430 features.
Fitting estimator with 429 features.
Fitting estimator with 428 features.
Fitting estimator with 427 features.
Fitting estimator with 426 features.
Fitting estimator with 425 features.
Fitting estimator with 424 features.
Fitting estimator with 423 features.
Fitting estimator with 422 features.
Fitting estimator with 421 features.
Fitting estimator with 420 features.
Fitting estimator with 419 features.
Fitting estimator with 418 features.
Fitting estimator with 417 features.
Fitting estimator with 416 features.
Fitting estimator with 415 features.
Fitting estimator with 414 features.
Fitting estimator with 413 features.
Fitting estimator with 412 features.
Fitting estimator with 411 features.
Fitting estimator with 410 features.
Fitting estimator with 409 features.
Fitting estimator with 408 features.
Fitting estimator with 407 features.
Fitting estimator with 406 features.
Fitting estimator with 405 features.
Fitting estimator with 404 features.
Fitting estimator with 403 features.
Fitting estimator with 402 features.
Fitting estimator with 401 features.
Fitting estimator with 400 features.
Fitting estimator with 399 features.
Fitting estimator with 398 features.
Fitting estimator with 397 features.
Fitting estimator with 396 features.
Fitting estimator with 395 features.
Fitting estimator with 394 features.
Fitting estimator with 393 features.
Fitting estimator with 392 features.
Fitting estimator with 391 features.
Fitting estimator with 390 features.
Fitting estimator with 389 features.
Fitting estimator with 388 features.
Fitting estimator with 387 features.
Fitting estimator with 386 features.
Fitting estimator with 385 features.
Fitting estimator with 384 features.
Fitting estimator with 383 features.
Fitting estimator with 382 features.
Fitting estimator with 381 features.
Fitting estimator with 380 features.
Fitting estimator with 379 features.
Fitting estimator with 378 features.
Fitting estimator with 377 features.
Fitting estimator with 376 features.
Fitting estimator with 375 features.
Fitting estimator with 374 features.
Fitting estimator with 373 features.
Fitting estimator with 372 features.
Fitting estimator with 371 features.
Fitting estimator with 370 features.
Fitting estimator with 369 features.
Fitting estimator with 368 features.
Fitting estimator with 367 features.
Fitting estimator with 366 features.
Fitting estimator with 365 features.
Fitting estimator with 364 features.
Fitting estimator with 363 features.
Fitting estimator with 362 features.
Fitting estimator with 361 features.
Fitting estimator with 360 features.
Fitting estimator with 359 features.
Fitting estimator with 358 features.
Fitting estimator with 357 features.
Fitting estimator with 356 features.
Fitting estimator with 355 features.
Fitting estimator with 354 features.
Fitting estimator with 353 features.
Fitting estimator with 352 features.
Fitting estimator with 351 features.
Fitting estimator with 350 features.
Fitting estimator with 349 features.
Fitting estimator with 348 features.
Fitting estimator with 347 features.
Fitting estimator with 346 features.
Fitting estimator with 345 features.
Fitting estimator with 344 features.
Fitting estimator with 343 features.
Fitting estimator with 342 features.
Fitting estimator with 341 features.
Fitting estimator with 340 features.
Fitting estimator with 339 features.
Fitting estimator with 338 features.
Fitting estimator with 337 features.
Fitting estimator with 336 features.
Fitting estimator with 335 features.
Fitting estimator with 334 features.
Fitting estimator with 333 features.
Fitting estimator with 332 features.
Fitting estimator with 331 features.
Fitting estimator with 330 features.
Fitting estimator with 329 features.
Fitting estimator with 328 features.
Fitting estimator with 327 features.
Fitting estimator with 326 features.
Fitting estimator with 325 features.
Fitting estimator with 324 features.
Fitting estimator with 323 features.
Fitting estimator with 322 features.
Fitting estimator with 321 features.
Fitting estimator with 320 features.
Fitting estimator with 319 features.
Fitting estimator with 318 features.
Fitting estimator with 317 features.
Fitting estimator with 316 features.
Fitting estimator with 315 features.
Fitting estimator with 314 features.
Fitting estimator with 313 features.
Fitting estimator with 312 features.
Fitting estimator with 311 features.
Fitting estimator with 310 features.
Fitting estimator with 309 features.
Fitting estimator with 308 features.
Fitting estimator with 307 features.
Fitting estimator with 306 features.
Fitting estimator with 305 features.
Fitting estimator with 304 features.
Fitting estimator with 303 features.
Fitting estimator with 302 features.
Fitting estimator with 301 features.
Fitting estimator with 300 features.
Fitting estimator with 299 features.
Fitting estimator with 298 features.
Fitting estimator with 297 features.
Fitting estimator with 296 features.
Fitting estimator with 295 features.
Fitting estimator with 294 features.
Fitting estimator with 293 features.
Fitting estimator with 292 features.
Fitting estimator with 291 features.
Fitting estimator with 290 features.
Fitting estimator with 289 features.
Fitting estimator with 288 features.
Fitting estimator with 287 features.
Fitting estimator with 286 features.
Fitting estimator with 285 features.
Fitting estimator with 284 features.
Fitting estimator with 283 features.
Fitting estimator with 282 features.
Fitting estimator with 281 features.
Fitting estimator with 280 features.
Fitting estimator with 279 features.
Fitting estimator with 278 features.
Fitting estimator with 277 features.
Fitting estimator with 276 features.
Fitting estimator with 275 features.
Fitting estimator with 274 features.
Fitting estimator with 273 features.
Fitting estimator with 272 features.
Fitting estimator with 271 features.
Fitting estimator with 270 features.
Fitting estimator with 269 features.
Fitting estimator with 268 features.
Fitting estimator with 267 features.
Fitting estimator with 266 features.
Fitting estimator with 265 features.
Fitting estimator with 264 features.
Fitting estimator with 263 features.
Fitting estimator with 262 features.
Fitting estimator with 261 features.
Fitting estimator with 260 features.
Fitting estimator with 259 features.
Fitting estimator with 258 features.
Fitting estimator with 257 features.
Fitting estimator with 256 features.
Fitting estimator with 255 features.
Fitting estimator with 254 features.
Fitting estimator with 253 features.
Fitting estimator with 252 features.
Fitting estimator with 251 features.
Fitting estimator with 250 features.
Fitting estimator with 249 features.
Fitting estimator with 248 features.
Fitting estimator with 247 features.
Fitting estimator with 246 features.
Fitting estimator with 245 features.
Fitting estimator with 244 features.
Fitting estimator with 243 features.
Fitting estimator with 242 features.
Fitting estimator with 241 features.
Fitting estimator with 240 features.
Fitting estimator with 239 features.
Fitting estimator with 238 features.
Fitting estimator with 237 features.
Fitting estimator with 236 features.
Fitting estimator with 235 features.
Fitting estimator with 234 features.
Fitting estimator with 233 features.
Fitting estimator with 232 features.
Fitting estimator with 231 features.
Fitting estimator with 230 features.
Fitting estimator with 229 features.
Fitting estimator with 228 features.
Fitting estimator with 227 features.
Fitting estimator with 226 features.
Fitting estimator with 225 features.
Fitting estimator with 224 features.
Fitting estimator with 223 features.
Fitting estimator with 222 features.
Fitting estimator with 221 features.
Fitting estimator with 220 features.
Fitting estimator with 219 features.
Fitting estimator with 218 features.
Fitting estimator with 217 features.
Fitting estimator with 216 features.
Fitting estimator with 215 features.
Fitting estimator with 214 features.
Fitting estimator with 213 features.
Fitting estimator with 212 features.
Fitting estimator with 211 features.
Fitting estimator with 210 features.
Fitting estimator with 209 features.
Fitting estimator with 208 features.
Fitting estimator with 207 features.
Fitting estimator with 206 features.
Fitting estimator with 205 features.
Fitting estimator with 204 features.
Fitting estimator with 203 features.
Fitting estimator with 202 features.
Fitting estimator with 201 features.
Fitting estimator with 200 features.
Fitting estimator with 199 features.
Fitting estimator with 198 features.
Fitting estimator with 197 features.
Fitting estimator with 196 features.
Fitting estimator with 195 features.
Fitting estimator with 194 features.
Fitting estimator with 193 features.
Fitting estimator with 192 features.
Fitting estimator with 191 features.
Fitting estimator with 190 features.
Fitting estimator with 189 features.
Fitting estimator with 188 features.
Fitting estimator with 187 features.
Fitting estimator with 186 features.
Fitting estimator with 185 features.
Fitting estimator with 184 features.
Fitting estimator with 183 features.
Fitting estimator with 182 features.
Fitting estimator with 181 features.
Fitting estimator with 180 features.
Fitting estimator with 179 features.
Fitting estimator with 178 features.
Fitting estimator with 177 features.
Fitting estimator with 176 features.
Fitting estimator with 175 features.
Fitting estimator with 174 features.
Fitting estimator with 173 features.
Fitting estimator with 172 features.
Fitting estimator with 171 features.
Fitting estimator with 170 features.
Fitting estimator with 169 features.
Fitting estimator with 168 features.
Fitting estimator with 167 features.
Fitting estimator with 166 features.
Fitting estimator with 165 features.
Fitting estimator with 164 features.
Fitting estimator with 163 features.
Fitting estimator with 162 features.
Fitting estimator with 161 features.
Fitting estimator with 160 features.
Fitting estimator with 159 features.
Fitting estimator with 158 features.
Fitting estimator with 157 features.
Fitting estimator with 156 features.
Fitting estimator with 155 features.
Fitting estimator with 154 features.
Fitting estimator with 153 features.
Fitting estimator with 152 features.
Fitting estimator with 151 features.
Fitting estimator with 150 features.
Fitting estimator with 149 features.
Fitting estimator with 148 features.
Fitting estimator with 147 features.
Fitting estimator with 146 features.
Fitting estimator with 145 features.
Fitting estimator with 144 features.
Fitting estimator with 143 features.
Fitting estimator with 142 features.
Fitting estimator with 141 features.
Fitting estimator with 140 features.
Fitting estimator with 139 features.
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
Fitting estimator with 40 features.
Fitting estimator with 39 features.
Fitting estimator with 38 features.
Fitting estimator with 37 features.
Fitting estimator with 36 features.
Fitting estimator with 35 features.
Fitting estimator with 34 features.
Fitting estimator with 33 features.
Fitting estimator with 32 features.
Fitting estimator with 31 features.
Fitting estimator with 30 features.
Fitting estimator with 29 features.
Fitting estimator with 28 features.
Fitting estimator with 27 features.
Fitting estimator with 26 features.
Fitting estimator with 25 features.
Fitting estimator with 24 features.
Fitting estimator with 23 features.
Fitting estimator with 22 features.
Fitting estimator with 21 features.
Fitting estimator with 20 features.
Fitting estimator with 19 features.
Fitting estimator with 18 features.
Fitting estimator with 17 features.
Fitting estimator with 16 features.
Fitting estimator with 15 features.
Fitting estimator with 14 features.
Fitting estimator with 13 features.
Fitting estimator with 12 features.
Fitting estimator with 11 features.
Fitting estimator with 10 features.
Fitting estimator with 9 features.
Fitting estimator with 8 features.
Fitting estimator with 7 features.
Fitting estimator with 6 features.</code></pre>
</div>
</div>
<div id="cell-33" class="cell" data-cell_id="dc715458d9e442b4bcd724a633fb04ca" data-deepnote_cell_type="code" data-execution_count="17">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>joblib.dump(selector_lin_reg_cv, <span class="st">"selector_lin_reg_cv.joblib"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>['selector_lin_reg_cv.joblib']</code></pre>
</div>
</div>
<p>As shown in the graph below, as the number of feature decreases, the validation loss tend to decrease.</p>
<div id="cell-35" class="cell" data-cell_id="cfde4ec4e21e4261b12bfffff57eab48" data-deepnote_cell_type="code" data-execution_count="18">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_n_feature_val_loss(selector, step, model_name<span class="op">=</span><span class="st">""</span>, mark<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Plot the validation loss (MSE) versus the number of features used.</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">        selector: The RFECV selector containing cv_results_.</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">        step (int): The step size used in feature elimination.</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">        model_name (str): Name of the model for title annotation.</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">        mark: Marker style for the plot.</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract results.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> np.array(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(selector.cv_results_[<span class="st">"mean_test_score"</span>]) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    validation_loss <span class="op">=</span> <span class="op">-</span>selector.cv_results_[<span class="st">"mean_test_score"</span>]  <span class="co"># Convert to positive MSE.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot.</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    plt.plot(n_features <span class="op">*</span> step, validation_loss, marker<span class="op">=</span>mark, linestyle<span class="op">=</span><span class="st">"-"</span>, color<span class="op">=</span><span class="st">"b"</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Number of Features vs Validation Loss (MSE) of </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Number of Features"</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Validation Loss (MSE)"</span>)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>plot_n_feature_val_loss(</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    selector_lin_reg_cv,</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    step<span class="op">=</span>step_lin_reg,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    model_name<span class="op">=</span><span class="st">"Linear Regression (Cross-Validation)"</span>,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    mark<span class="op">=</span><span class="va">None</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-36" class="cell" data-cell_id="4f12b9db37ac4ae0beb51602ead3b268" data-deepnote_cell_type="code" data-execution_context_id="1293d179-fdbd-4dae-89be-413eaf88ce42" data-execution_millis="0" data-execution_start="1740288873454" data-source_hash="64c34762" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> selector_result(selector, X_train_temp<span class="op">=</span>X_train, y_train_temp<span class="op">=</span>y_train,</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>                    X_test_temp<span class="op">=</span>X_test, y_test_temp<span class="op">=</span>y_test,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                    model_name<span class="op">=</span><span class="st">""</span>, print_out<span class="op">=</span><span class="va">True</span>, predict<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluate the selected model and print or return performance metrics.</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">        selector: The RFECV selector object containing cv_results_ and estimator.</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        X_train_temp (DataFrame): Training feature set.</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">        y_train_temp (Series): Training labels.</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">        X_test_temp (DataFrame): Testing feature set.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">        y_test_temp (Series): Testing labels.</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">        model_name (str): Name of the model for display purposes.</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">        print_out (bool): If True, print the metrics.</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">        predict (bool): If True, return predictions instead of metrics.</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Tuple: (train MSE, test MSE, train RMSE, test RMSE) if predict is False,</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co">               otherwise a list of predictions.</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> X_train_temp.columns[selector.support_]</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    mse_bs_series <span class="op">=</span> pd.Series(<span class="op">-</span>selector.cv_results_[<span class="st">"mean_test_score"</span>])</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    best_model <span class="op">=</span> selector.estimator</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>    best_model.fit(X_train_temp[features], y_train_temp)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>    best_model_mse_train <span class="op">=</span> mean_squared_error(best_model.predict(X_train_temp[features]),</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>                                              y_train_temp)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>    y_predict <span class="op">=</span> best_model.predict(X_test_temp[features])</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    best_model_mse_test <span class="op">=</span> mean_squared_error(best_model.predict(X_test_temp[features]),</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>                                             y_test_temp)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    best_model_rmse_train <span class="op">=</span> np.sqrt(best_model_mse_train)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    best_model_rmse_test <span class="op">=</span> np.sqrt(best_model_mse_test)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_out:</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Optimal number of features for </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>selector<span class="sc">.</span>n_features_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Selected features for Linear </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Best Validation MSE:"</span>, mse_bs_series.<span class="bu">min</span>())</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"-----------------------------"</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Model Train MSE: </span><span class="sc">{</span>best_model_mse_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Model Test MSE: </span><span class="sc">{</span>best_model_mse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Model Train RMSE: </span><span class="sc">{</span>best_model_rmse_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Model Test RMSE: </span><span class="sc">{</span>best_model_rmse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> predict:</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> best_model_mse_train, best_model_mse_test, best_model_rmse_train, best_model_rmse_test</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">list</span>(y_predict)</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> selector_result(selector_lin_reg_cv, X_train, y_train, X_test, y_test, <span class="st">"Linear Regression (Cross-Validation)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal number of features for Linear Regression (Cross-Validation): 5
Selected features for Linear Linear Regression (Cross-Validation): Index(['TE3_day-1', 'TE1_day-4', 'TE2_day-4', 'DE5_day-4', 'DE6_day-4'], dtype='object')
Best Validation MSE: 0.00010527116357773034
-----------------------------
Best Linear Regression (Cross-Validation) Model Train MSE: 0.00012614083944832163
Best Linear Regression (Cross-Validation) Model Test MSE: 5.793372382508834e-05
Best Linear Regression (Cross-Validation) Model Train RMSE: 0.011231243895861297
Best Linear Regression (Cross-Validation) Model Test RMSE: 0.007611420618063906</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="tree-based-models-for-regression" class="level2">
<h2 class="anchored" data-anchor-id="tree-based-models-for-regression">3. Tree Based Models for Regression</h2>
<p>We implement the same feature selection strategy for tree models</p>
<section id="decision-tree" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree">3.1 Decision Tree</h3>
<section id="feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection">3.1.1 Feature Selection</h4>
<div id="cell-39" class="cell" data-cell_id="cebe42bccfea4521aa0ba0c7ae90813c" data-deepnote_cell_type="code" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Series Cross-Validation</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>tscv <span class="op">=</span> TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>step_dt <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RFECV</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>selector_dt <span class="op">=</span> RFECV(</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>),</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    step<span class="op">=</span>step_dt,  <span class="co"># Remove 1 feature at each step</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,      <span class="co"># Time series split</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>  <span class="co"># Negative MSE for scoring</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and automatically select optimal number of features</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>selector_dt.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting estimator with 880 features.
Fitting estimator with 879 features.
Fitting estimator with 878 features.
Fitting estimator with 877 features.
Fitting estimator with 876 features.
Fitting estimator with 875 features.
Fitting estimator with 874 features.
Fitting estimator with 873 features.
Fitting estimator with 872 features.
Fitting estimator with 871 features.
Fitting estimator with 870 features.
Fitting estimator with 869 features.
Fitting estimator with 868 features.
Fitting estimator with 867 features.
Fitting estimator with 866 features.
Fitting estimator with 865 features.
Fitting estimator with 864 features.
Fitting estimator with 863 features.
Fitting estimator with 862 features.
Fitting estimator with 861 features.
Fitting estimator with 860 features.
Fitting estimator with 859 features.
Fitting estimator with 858 features.
Fitting estimator with 857 features.
Fitting estimator with 856 features.
Fitting estimator with 855 features.
Fitting estimator with 854 features.
Fitting estimator with 853 features.
Fitting estimator with 852 features.
Fitting estimator with 851 features.
Fitting estimator with 850 features.
Fitting estimator with 849 features.
Fitting estimator with 848 features.
Fitting estimator with 847 features.
Fitting estimator with 846 features.
Fitting estimator with 845 features.
Fitting estimator with 844 features.
Fitting estimator with 843 features.
Fitting estimator with 842 features.
Fitting estimator with 841 features.
Fitting estimator with 840 features.
Fitting estimator with 839 features.
Fitting estimator with 838 features.
Fitting estimator with 837 features.
Fitting estimator with 836 features.
Fitting estimator with 835 features.
Fitting estimator with 834 features.
Fitting estimator with 833 features.
Fitting estimator with 832 features.
Fitting estimator with 831 features.
Fitting estimator with 830 features.
Fitting estimator with 829 features.
Fitting estimator with 828 features.
Fitting estimator with 827 features.
Fitting estimator with 826 features.
Fitting estimator with 825 features.
Fitting estimator with 824 features.
Fitting estimator with 823 features.
Fitting estimator with 822 features.
Fitting estimator with 821 features.
Fitting estimator with 820 features.
Fitting estimator with 819 features.
Fitting estimator with 818 features.
Fitting estimator with 817 features.
Fitting estimator with 816 features.
Fitting estimator with 815 features.
Fitting estimator with 814 features.
Fitting estimator with 813 features.
Fitting estimator with 812 features.
Fitting estimator with 811 features.
Fitting estimator with 810 features.
Fitting estimator with 809 features.
Fitting estimator with 808 features.
Fitting estimator with 807 features.
Fitting estimator with 806 features.
Fitting estimator with 805 features.
Fitting estimator with 804 features.
Fitting estimator with 803 features.
Fitting estimator with 802 features.
Fitting estimator with 801 features.
Fitting estimator with 800 features.
Fitting estimator with 799 features.
Fitting estimator with 798 features.
Fitting estimator with 797 features.
Fitting estimator with 796 features.
Fitting estimator with 795 features.
Fitting estimator with 794 features.
Fitting estimator with 793 features.
Fitting estimator with 792 features.
Fitting estimator with 791 features.
Fitting estimator with 790 features.
Fitting estimator with 789 features.
Fitting estimator with 788 features.
Fitting estimator with 787 features.
Fitting estimator with 786 features.
Fitting estimator with 785 features.
Fitting estimator with 784 features.
Fitting estimator with 783 features.
Fitting estimator with 782 features.
Fitting estimator with 781 features.
Fitting estimator with 780 features.
Fitting estimator with 779 features.
Fitting estimator with 778 features.
Fitting estimator with 777 features.
Fitting estimator with 776 features.
Fitting estimator with 775 features.
Fitting estimator with 774 features.
Fitting estimator with 773 features.
Fitting estimator with 772 features.
Fitting estimator with 771 features.
Fitting estimator with 770 features.
Fitting estimator with 769 features.
Fitting estimator with 768 features.
Fitting estimator with 767 features.
Fitting estimator with 766 features.
Fitting estimator with 765 features.
Fitting estimator with 764 features.
Fitting estimator with 763 features.
Fitting estimator with 762 features.
Fitting estimator with 761 features.
Fitting estimator with 760 features.
Fitting estimator with 759 features.
Fitting estimator with 758 features.
Fitting estimator with 757 features.
Fitting estimator with 756 features.
Fitting estimator with 755 features.
Fitting estimator with 754 features.
Fitting estimator with 753 features.
Fitting estimator with 752 features.
Fitting estimator with 751 features.
Fitting estimator with 750 features.
Fitting estimator with 749 features.
Fitting estimator with 748 features.
Fitting estimator with 747 features.
Fitting estimator with 746 features.
Fitting estimator with 745 features.
Fitting estimator with 744 features.
Fitting estimator with 743 features.
Fitting estimator with 742 features.
Fitting estimator with 741 features.
Fitting estimator with 740 features.
Fitting estimator with 739 features.
Fitting estimator with 738 features.
Fitting estimator with 737 features.
Fitting estimator with 736 features.
Fitting estimator with 735 features.
Fitting estimator with 734 features.
Fitting estimator with 733 features.
Fitting estimator with 732 features.
Fitting estimator with 731 features.
Fitting estimator with 730 features.
Fitting estimator with 729 features.
Fitting estimator with 728 features.
Fitting estimator with 727 features.
Fitting estimator with 726 features.
Fitting estimator with 725 features.
Fitting estimator with 724 features.
Fitting estimator with 723 features.
Fitting estimator with 722 features.
Fitting estimator with 721 features.
Fitting estimator with 720 features.
Fitting estimator with 719 features.
Fitting estimator with 718 features.
Fitting estimator with 717 features.
Fitting estimator with 716 features.
Fitting estimator with 715 features.
Fitting estimator with 714 features.
Fitting estimator with 713 features.
Fitting estimator with 712 features.
Fitting estimator with 711 features.
Fitting estimator with 710 features.
Fitting estimator with 709 features.
Fitting estimator with 708 features.
Fitting estimator with 707 features.
Fitting estimator with 706 features.
Fitting estimator with 705 features.
Fitting estimator with 704 features.
Fitting estimator with 703 features.
Fitting estimator with 702 features.
Fitting estimator with 701 features.
Fitting estimator with 700 features.
Fitting estimator with 699 features.
Fitting estimator with 698 features.
Fitting estimator with 697 features.
Fitting estimator with 696 features.
Fitting estimator with 695 features.
Fitting estimator with 694 features.
Fitting estimator with 693 features.
Fitting estimator with 692 features.
Fitting estimator with 691 features.
Fitting estimator with 690 features.
Fitting estimator with 689 features.
Fitting estimator with 688 features.
Fitting estimator with 687 features.
Fitting estimator with 686 features.
Fitting estimator with 685 features.
Fitting estimator with 684 features.
Fitting estimator with 683 features.
Fitting estimator with 682 features.
Fitting estimator with 681 features.
Fitting estimator with 680 features.
Fitting estimator with 679 features.
Fitting estimator with 678 features.
Fitting estimator with 677 features.
Fitting estimator with 676 features.
Fitting estimator with 675 features.
Fitting estimator with 674 features.
Fitting estimator with 673 features.
Fitting estimator with 672 features.
Fitting estimator with 671 features.
Fitting estimator with 670 features.
Fitting estimator with 669 features.
Fitting estimator with 668 features.
Fitting estimator with 667 features.
Fitting estimator with 666 features.
Fitting estimator with 665 features.
Fitting estimator with 664 features.
Fitting estimator with 663 features.
Fitting estimator with 662 features.
Fitting estimator with 661 features.
Fitting estimator with 660 features.
Fitting estimator with 659 features.
Fitting estimator with 658 features.
Fitting estimator with 657 features.
Fitting estimator with 656 features.
Fitting estimator with 655 features.
Fitting estimator with 654 features.
Fitting estimator with 653 features.
Fitting estimator with 652 features.
Fitting estimator with 651 features.
Fitting estimator with 650 features.
Fitting estimator with 649 features.
Fitting estimator with 648 features.
Fitting estimator with 647 features.
Fitting estimator with 646 features.
Fitting estimator with 645 features.
Fitting estimator with 644 features.
Fitting estimator with 643 features.
Fitting estimator with 642 features.
Fitting estimator with 641 features.
Fitting estimator with 640 features.
Fitting estimator with 639 features.
Fitting estimator with 638 features.
Fitting estimator with 637 features.
Fitting estimator with 636 features.
Fitting estimator with 635 features.
Fitting estimator with 634 features.
Fitting estimator with 633 features.
Fitting estimator with 632 features.
Fitting estimator with 631 features.
Fitting estimator with 630 features.
Fitting estimator with 629 features.
Fitting estimator with 628 features.
Fitting estimator with 627 features.
Fitting estimator with 626 features.
Fitting estimator with 625 features.
Fitting estimator with 624 features.
Fitting estimator with 623 features.
Fitting estimator with 622 features.
Fitting estimator with 621 features.
Fitting estimator with 620 features.
Fitting estimator with 619 features.
Fitting estimator with 618 features.
Fitting estimator with 617 features.
Fitting estimator with 616 features.
Fitting estimator with 615 features.
Fitting estimator with 614 features.
Fitting estimator with 613 features.
Fitting estimator with 612 features.
Fitting estimator with 611 features.
Fitting estimator with 610 features.
Fitting estimator with 609 features.
Fitting estimator with 608 features.
Fitting estimator with 607 features.
Fitting estimator with 606 features.
Fitting estimator with 605 features.
Fitting estimator with 604 features.
Fitting estimator with 603 features.
Fitting estimator with 602 features.
Fitting estimator with 601 features.
Fitting estimator with 600 features.
Fitting estimator with 599 features.
Fitting estimator with 598 features.
Fitting estimator with 597 features.
Fitting estimator with 596 features.
Fitting estimator with 595 features.
Fitting estimator with 594 features.
Fitting estimator with 593 features.
Fitting estimator with 592 features.
Fitting estimator with 591 features.
Fitting estimator with 590 features.
Fitting estimator with 589 features.
Fitting estimator with 588 features.
Fitting estimator with 587 features.
Fitting estimator with 586 features.
Fitting estimator with 585 features.
Fitting estimator with 584 features.
Fitting estimator with 583 features.
Fitting estimator with 582 features.
Fitting estimator with 581 features.
Fitting estimator with 580 features.
Fitting estimator with 579 features.
Fitting estimator with 578 features.
Fitting estimator with 577 features.
Fitting estimator with 576 features.
Fitting estimator with 575 features.
Fitting estimator with 574 features.
Fitting estimator with 573 features.
Fitting estimator with 572 features.
Fitting estimator with 571 features.
Fitting estimator with 570 features.
Fitting estimator with 569 features.
Fitting estimator with 568 features.
Fitting estimator with 567 features.
Fitting estimator with 566 features.
Fitting estimator with 565 features.
Fitting estimator with 564 features.
Fitting estimator with 563 features.
Fitting estimator with 562 features.
Fitting estimator with 561 features.
Fitting estimator with 560 features.
Fitting estimator with 559 features.
Fitting estimator with 558 features.
Fitting estimator with 557 features.
Fitting estimator with 556 features.
Fitting estimator with 555 features.
Fitting estimator with 554 features.
Fitting estimator with 553 features.
Fitting estimator with 552 features.
Fitting estimator with 551 features.
Fitting estimator with 550 features.
Fitting estimator with 549 features.
Fitting estimator with 548 features.
Fitting estimator with 547 features.
Fitting estimator with 546 features.
Fitting estimator with 545 features.
Fitting estimator with 544 features.
Fitting estimator with 543 features.
Fitting estimator with 542 features.
Fitting estimator with 541 features.
Fitting estimator with 540 features.
Fitting estimator with 539 features.
Fitting estimator with 538 features.
Fitting estimator with 537 features.
Fitting estimator with 536 features.
Fitting estimator with 535 features.
Fitting estimator with 534 features.
Fitting estimator with 533 features.
Fitting estimator with 532 features.
Fitting estimator with 531 features.
Fitting estimator with 530 features.
Fitting estimator with 529 features.
Fitting estimator with 528 features.
Fitting estimator with 527 features.
Fitting estimator with 526 features.
Fitting estimator with 525 features.
Fitting estimator with 524 features.
Fitting estimator with 523 features.
Fitting estimator with 522 features.
Fitting estimator with 521 features.
Fitting estimator with 520 features.
Fitting estimator with 519 features.
Fitting estimator with 518 features.
Fitting estimator with 517 features.
Fitting estimator with 516 features.
Fitting estimator with 515 features.
Fitting estimator with 514 features.
Fitting estimator with 513 features.
Fitting estimator with 512 features.
Fitting estimator with 511 features.
Fitting estimator with 510 features.
Fitting estimator with 509 features.
Fitting estimator with 508 features.
Fitting estimator with 507 features.
Fitting estimator with 506 features.
Fitting estimator with 505 features.
Fitting estimator with 504 features.
Fitting estimator with 503 features.
Fitting estimator with 502 features.
Fitting estimator with 501 features.
Fitting estimator with 500 features.
Fitting estimator with 499 features.
Fitting estimator with 498 features.
Fitting estimator with 497 features.
Fitting estimator with 496 features.
Fitting estimator with 495 features.
Fitting estimator with 494 features.
Fitting estimator with 493 features.
Fitting estimator with 492 features.
Fitting estimator with 491 features.
Fitting estimator with 490 features.
Fitting estimator with 489 features.
Fitting estimator with 488 features.
Fitting estimator with 487 features.
Fitting estimator with 486 features.
Fitting estimator with 485 features.
Fitting estimator with 484 features.
Fitting estimator with 483 features.
Fitting estimator with 482 features.
Fitting estimator with 481 features.
Fitting estimator with 480 features.
Fitting estimator with 479 features.
Fitting estimator with 478 features.
Fitting estimator with 477 features.
Fitting estimator with 476 features.
Fitting estimator with 475 features.
Fitting estimator with 474 features.
Fitting estimator with 473 features.
Fitting estimator with 472 features.
Fitting estimator with 471 features.
Fitting estimator with 470 features.
Fitting estimator with 469 features.
Fitting estimator with 468 features.
Fitting estimator with 467 features.
Fitting estimator with 466 features.
Fitting estimator with 465 features.
Fitting estimator with 464 features.
Fitting estimator with 463 features.
Fitting estimator with 462 features.
Fitting estimator with 461 features.
Fitting estimator with 460 features.
Fitting estimator with 459 features.
Fitting estimator with 458 features.
Fitting estimator with 457 features.
Fitting estimator with 456 features.
Fitting estimator with 455 features.
Fitting estimator with 454 features.
Fitting estimator with 453 features.
Fitting estimator with 452 features.
Fitting estimator with 451 features.
Fitting estimator with 450 features.
Fitting estimator with 449 features.
Fitting estimator with 448 features.
Fitting estimator with 447 features.
Fitting estimator with 446 features.
Fitting estimator with 445 features.
Fitting estimator with 444 features.
Fitting estimator with 443 features.
Fitting estimator with 442 features.
Fitting estimator with 441 features.
Fitting estimator with 440 features.
Fitting estimator with 439 features.
Fitting estimator with 438 features.
Fitting estimator with 437 features.
Fitting estimator with 436 features.
Fitting estimator with 435 features.
Fitting estimator with 434 features.
Fitting estimator with 433 features.
Fitting estimator with 432 features.
Fitting estimator with 431 features.
Fitting estimator with 430 features.
Fitting estimator with 429 features.
Fitting estimator with 428 features.
Fitting estimator with 427 features.
Fitting estimator with 426 features.
Fitting estimator with 425 features.
Fitting estimator with 424 features.
Fitting estimator with 423 features.
Fitting estimator with 422 features.
Fitting estimator with 421 features.
Fitting estimator with 420 features.
Fitting estimator with 419 features.
Fitting estimator with 418 features.
Fitting estimator with 417 features.
Fitting estimator with 416 features.
Fitting estimator with 415 features.
Fitting estimator with 414 features.
Fitting estimator with 413 features.
Fitting estimator with 412 features.
Fitting estimator with 411 features.
Fitting estimator with 410 features.
Fitting estimator with 409 features.
Fitting estimator with 408 features.
Fitting estimator with 407 features.
Fitting estimator with 406 features.
Fitting estimator with 405 features.
Fitting estimator with 404 features.
Fitting estimator with 403 features.
Fitting estimator with 402 features.
Fitting estimator with 401 features.
Fitting estimator with 400 features.
Fitting estimator with 399 features.
Fitting estimator with 398 features.
Fitting estimator with 397 features.
Fitting estimator with 396 features.
Fitting estimator with 395 features.
Fitting estimator with 394 features.
Fitting estimator with 393 features.
Fitting estimator with 392 features.
Fitting estimator with 391 features.
Fitting estimator with 390 features.
Fitting estimator with 389 features.
Fitting estimator with 388 features.
Fitting estimator with 387 features.
Fitting estimator with 386 features.
Fitting estimator with 385 features.
Fitting estimator with 384 features.
Fitting estimator with 383 features.
Fitting estimator with 382 features.
Fitting estimator with 381 features.
Fitting estimator with 380 features.
Fitting estimator with 379 features.
Fitting estimator with 378 features.
Fitting estimator with 377 features.
Fitting estimator with 376 features.
Fitting estimator with 375 features.
Fitting estimator with 374 features.
Fitting estimator with 373 features.
Fitting estimator with 372 features.
Fitting estimator with 371 features.
Fitting estimator with 370 features.
Fitting estimator with 369 features.
Fitting estimator with 368 features.
Fitting estimator with 367 features.
Fitting estimator with 366 features.
Fitting estimator with 365 features.
Fitting estimator with 364 features.
Fitting estimator with 363 features.
Fitting estimator with 362 features.
Fitting estimator with 361 features.
Fitting estimator with 360 features.
Fitting estimator with 359 features.
Fitting estimator with 358 features.
Fitting estimator with 357 features.
Fitting estimator with 356 features.
Fitting estimator with 355 features.
Fitting estimator with 354 features.
Fitting estimator with 353 features.
Fitting estimator with 352 features.
Fitting estimator with 351 features.
Fitting estimator with 350 features.
Fitting estimator with 349 features.
Fitting estimator with 348 features.
Fitting estimator with 347 features.
Fitting estimator with 346 features.
Fitting estimator with 345 features.
Fitting estimator with 344 features.
Fitting estimator with 343 features.
Fitting estimator with 342 features.
Fitting estimator with 341 features.
Fitting estimator with 340 features.
Fitting estimator with 339 features.
Fitting estimator with 338 features.
Fitting estimator with 337 features.
Fitting estimator with 336 features.
Fitting estimator with 335 features.
Fitting estimator with 334 features.
Fitting estimator with 333 features.
Fitting estimator with 332 features.
Fitting estimator with 331 features.
Fitting estimator with 330 features.
Fitting estimator with 329 features.
Fitting estimator with 328 features.
Fitting estimator with 327 features.
Fitting estimator with 326 features.
Fitting estimator with 325 features.
Fitting estimator with 324 features.
Fitting estimator with 323 features.
Fitting estimator with 322 features.
Fitting estimator with 321 features.
Fitting estimator with 320 features.
Fitting estimator with 319 features.
Fitting estimator with 318 features.
Fitting estimator with 317 features.
Fitting estimator with 316 features.
Fitting estimator with 315 features.
Fitting estimator with 314 features.
Fitting estimator with 313 features.
Fitting estimator with 312 features.
Fitting estimator with 311 features.
Fitting estimator with 310 features.
Fitting estimator with 309 features.
Fitting estimator with 308 features.
Fitting estimator with 307 features.
Fitting estimator with 306 features.
Fitting estimator with 305 features.
Fitting estimator with 304 features.
Fitting estimator with 303 features.
Fitting estimator with 302 features.
Fitting estimator with 301 features.
Fitting estimator with 300 features.
Fitting estimator with 299 features.
Fitting estimator with 298 features.
Fitting estimator with 297 features.
Fitting estimator with 296 features.
Fitting estimator with 295 features.
Fitting estimator with 294 features.
Fitting estimator with 293 features.
Fitting estimator with 292 features.
Fitting estimator with 291 features.
Fitting estimator with 290 features.
Fitting estimator with 289 features.
Fitting estimator with 288 features.
Fitting estimator with 287 features.
Fitting estimator with 286 features.
Fitting estimator with 285 features.
Fitting estimator with 284 features.
Fitting estimator with 283 features.
Fitting estimator with 282 features.
Fitting estimator with 281 features.
Fitting estimator with 280 features.
Fitting estimator with 279 features.
Fitting estimator with 278 features.
Fitting estimator with 277 features.
Fitting estimator with 276 features.
Fitting estimator with 275 features.
Fitting estimator with 274 features.
Fitting estimator with 273 features.
Fitting estimator with 272 features.
Fitting estimator with 271 features.
Fitting estimator with 270 features.
Fitting estimator with 269 features.
Fitting estimator with 268 features.
Fitting estimator with 267 features.
Fitting estimator with 266 features.
Fitting estimator with 265 features.
Fitting estimator with 264 features.
Fitting estimator with 263 features.
Fitting estimator with 262 features.
Fitting estimator with 261 features.
Fitting estimator with 260 features.
Fitting estimator with 259 features.
Fitting estimator with 258 features.
Fitting estimator with 257 features.
Fitting estimator with 256 features.
Fitting estimator with 255 features.
Fitting estimator with 254 features.
Fitting estimator with 253 features.
Fitting estimator with 252 features.
Fitting estimator with 251 features.
Fitting estimator with 250 features.
Fitting estimator with 249 features.
Fitting estimator with 248 features.
Fitting estimator with 247 features.
Fitting estimator with 246 features.
Fitting estimator with 245 features.
Fitting estimator with 244 features.
Fitting estimator with 243 features.
Fitting estimator with 242 features.
Fitting estimator with 241 features.
Fitting estimator with 240 features.
Fitting estimator with 239 features.
Fitting estimator with 238 features.
Fitting estimator with 237 features.
Fitting estimator with 236 features.
Fitting estimator with 235 features.
Fitting estimator with 234 features.
Fitting estimator with 233 features.
Fitting estimator with 232 features.
Fitting estimator with 231 features.
Fitting estimator with 230 features.
Fitting estimator with 229 features.
Fitting estimator with 228 features.
Fitting estimator with 227 features.
Fitting estimator with 226 features.
Fitting estimator with 225 features.
Fitting estimator with 224 features.
Fitting estimator with 223 features.
Fitting estimator with 222 features.
Fitting estimator with 221 features.
Fitting estimator with 220 features.
Fitting estimator with 219 features.
Fitting estimator with 218 features.
Fitting estimator with 217 features.
Fitting estimator with 216 features.
Fitting estimator with 215 features.
Fitting estimator with 214 features.
Fitting estimator with 213 features.
Fitting estimator with 212 features.
Fitting estimator with 211 features.
Fitting estimator with 210 features.
Fitting estimator with 209 features.
Fitting estimator with 208 features.
Fitting estimator with 207 features.
Fitting estimator with 206 features.
Fitting estimator with 205 features.
Fitting estimator with 204 features.
Fitting estimator with 203 features.
Fitting estimator with 202 features.
Fitting estimator with 201 features.
Fitting estimator with 200 features.
Fitting estimator with 199 features.
Fitting estimator with 198 features.
Fitting estimator with 197 features.
Fitting estimator with 196 features.
Fitting estimator with 195 features.
Fitting estimator with 194 features.
Fitting estimator with 193 features.
Fitting estimator with 192 features.
Fitting estimator with 191 features.
Fitting estimator with 190 features.
Fitting estimator with 189 features.
Fitting estimator with 188 features.
Fitting estimator with 187 features.
Fitting estimator with 186 features.
Fitting estimator with 185 features.
Fitting estimator with 184 features.
Fitting estimator with 183 features.
Fitting estimator with 182 features.
Fitting estimator with 181 features.
Fitting estimator with 180 features.
Fitting estimator with 179 features.
Fitting estimator with 178 features.
Fitting estimator with 177 features.
Fitting estimator with 176 features.
Fitting estimator with 175 features.
Fitting estimator with 174 features.
Fitting estimator with 173 features.
Fitting estimator with 172 features.
Fitting estimator with 171 features.
Fitting estimator with 170 features.
Fitting estimator with 169 features.
Fitting estimator with 168 features.
Fitting estimator with 167 features.
Fitting estimator with 166 features.
Fitting estimator with 165 features.
Fitting estimator with 164 features.
Fitting estimator with 163 features.
Fitting estimator with 162 features.
Fitting estimator with 161 features.
Fitting estimator with 160 features.
Fitting estimator with 159 features.
Fitting estimator with 158 features.
Fitting estimator with 157 features.
Fitting estimator with 156 features.
Fitting estimator with 155 features.
Fitting estimator with 154 features.
Fitting estimator with 153 features.
Fitting estimator with 152 features.
Fitting estimator with 151 features.
Fitting estimator with 150 features.
Fitting estimator with 149 features.
Fitting estimator with 148 features.
Fitting estimator with 147 features.
Fitting estimator with 146 features.
Fitting estimator with 145 features.
Fitting estimator with 144 features.
Fitting estimator with 143 features.
Fitting estimator with 142 features.
Fitting estimator with 141 features.
Fitting estimator with 140 features.
Fitting estimator with 139 features.
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
Fitting estimator with 40 features.
Fitting estimator with 39 features.
Fitting estimator with 38 features.
Fitting estimator with 37 features.
Fitting estimator with 36 features.
Fitting estimator with 35 features.
Fitting estimator with 34 features.
Fitting estimator with 33 features.
Fitting estimator with 32 features.
Fitting estimator with 31 features.
Fitting estimator with 30 features.
Fitting estimator with 29 features.
Fitting estimator with 28 features.
Fitting estimator with 27 features.
Fitting estimator with 26 features.
Fitting estimator with 25 features.
Fitting estimator with 24 features.
Fitting estimator with 23 features.
Fitting estimator with 22 features.
Fitting estimator with 21 features.
Fitting estimator with 20 features.
Fitting estimator with 19 features.
Fitting estimator with 18 features.
Fitting estimator with 17 features.
Fitting estimator with 16 features.
Fitting estimator with 15 features.
Fitting estimator with 14 features.
Fitting estimator with 13 features.
Fitting estimator with 12 features.
Fitting estimator with 11 features.
Fitting estimator with 10 features.
Fitting estimator with 9 features.
Fitting estimator with 8 features.
Fitting estimator with 7 features.
Fitting estimator with 6 features.
Fitting estimator with 5 features.
Fitting estimator with 4 features.
Fitting estimator with 3 features.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=DecisionTreeRegressor(max_depth=10, random_state=42), n_jobs=-1,
      scoring='neg_mean_squared_error', verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">RFECV</label><div class="sk-toggleable__content"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=DecisionTreeRegressor(max_depth=10, random_state=42), n_jobs=-1,
      scoring='neg_mean_squared_error', verbose=1)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=10, random_state=42)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<div id="cell-40" class="cell" data-cell_id="7b81fe15e6a84b3385879516547c42d2" data-deepnote_cell_type="code" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>joblib.dump(selector_dt, <span class="st">"selector_dt.joblib"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>['selector_dt.joblib']</code></pre>
</div>
</div>
<div id="cell-41" class="cell" data-cell_id="363328eed41b4b7296ed5857552a0805" data-deepnote_cell_type="code" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>plot_n_feature_val_loss(selector_dt, step<span class="op">=</span>step_dt, model_name<span class="op">=</span><span class="st">'Decision Tree'</span>, mark<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-42" class="cell" data-cell_id="5799ef8e8eb9422a9bbf8d5dded33389" data-deepnote_cell_type="code" data-execution_count="23">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> selector_result(selector_dt, X_train, y_train, X_test, y_test, <span class="st">"Decision Tree"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal number of features for Decision Tree: 2
Selected features for Linear Decision Tree: Index(['ROC_10_day-1', 'NASDAQ-F_day-3'], dtype='object')
Best Validation MSE: 0.00013017646633121612
-----------------------------
Best Decision Tree Model Train MSE: 9.389818141190975e-05
Best Decision Tree Model Test MSE: 5.319664601451538e-05
Best Decision Tree Model Train RMSE: 0.00969010739940016
Best Decision Tree Model Test RMSE: 0.0072936030886329</code></pre>
</div>
</div>
</section>
<section id="hyperparameter-optimization" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-optimization">3.1.2 Hyperparameter Optimization</h4>
<div id="cell-44" class="cell" data-cell_id="7fd2205df3494b0abf08838d42457ff2" data-deepnote_cell_type="code" data-execution_count="24">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get selected features</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>X_train_selected_dt <span class="op">=</span> X_train.iloc[:, selector_dt.support_]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter optimization</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>param_grid_dt <span class="op">=</span> {</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="va">None</span>],</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>],</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>grid_search_dt <span class="op">=</span> GridSearchCV(</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">42</span>),</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid_dt,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>grid_search_dt.fit(X_train_selected_dt, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 3 folds for each of 42 candidates, totalling 126 fits</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=DecisionTreeRegressor(random_state=42), n_jobs=-1,
             param_grid={'max_depth': [2, 3, 4, 6, 8, 10, None],
                         'min_samples_split': [2, 3, 4, 6, 8, 10]},
             scoring='neg_mean_squared_error', verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=DecisionTreeRegressor(random_state=42), n_jobs=-1,
             param_grid={'max_depth': [2, 3, 4, 6, 8, 10, None],
                         'min_samples_split': [2, 3, 4, 6, 8, 10]},
             scoring='neg_mean_squared_error', verbose=1)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox"><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<div id="cell-45" class="cell" data-cell_id="9ef0f7657f984bad9cb0916474e58171" data-deepnote_cell_type="code" data-execution_count="25">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> two_hyperparameter_visualization(grid_search, hp1, hp2, model_name,</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>                                     criterion<span class="op">=</span><span class="st">'mean_test_score'</span>,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>                                     criterion_name<span class="op">=</span><span class="st">'RMSE'</span>):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract results</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> grid_search.cv_results_</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    mean_scores <span class="op">=</span> np.sqrt(<span class="op">-</span>results[criterion])  <span class="co"># Convert to positive RMSE</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    param_1 <span class="op">=</span> grid_search.param_grid[hp1]</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    param_2 <span class="op">=</span> grid_search.param_grid[hp2]</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape scores into a matrix</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    scores_matrix <span class="op">=</span> np.reshape(mean_scores, (<span class="bu">len</span>(param_1), <span class="bu">len</span>(param_2)))</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a heatmap</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        scores_matrix,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>        annot<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>        fmt<span class="op">=</span><span class="st">".3f"</span>,</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>        xticklabels<span class="op">=</span>param_1,</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>        yticklabels<span class="op">=</span>param_2,</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>        cmap<span class="op">=</span><span class="st">"viridis"</span>,</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(hp1)</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(hp2)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    plt.title(</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Decision Tree Hyperparameter Tuning Results "</span></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"(</span><span class="sc">{</span>criterion_name<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>two_hyperparameter_visualization(</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    grid_search_dt, <span class="st">'max_depth'</span>, <span class="st">'min_samples_split'</span>, <span class="st">'Decision Tree'</span></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-46" class="cell" data-cell_id="868b7899840541c18bb03c0147a5769f" data-deepnote_cell_type="code" data-execution_count="26">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the final model</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>best_model_dt_final <span class="op">=</span> grid_search_dt.best_estimator_</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>best_model_dt_final.fit(</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    X_train[X.columns[selector_dt.support_]], y_train</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>joblib.dump(best_model_dt_final, <span class="st">'best_model_dt_final.joblib'</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grid_search_result(grid_search, selector, final_model, model_name<span class="op">=</span><span class="st">''</span>, print_out<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    best_model_mse_train_final <span class="op">=</span> mean_squared_error(</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        final_model.predict(X_train[X.columns[selector.support_]]), y_train</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    best_model_mse_test_final <span class="op">=</span> mean_squared_error(</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        final_model.predict(X_test[X.columns[selector.support_]]), y_test</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    best_model_rmse_train_final <span class="op">=</span> np.sqrt(best_model_mse_train_final)</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    best_model_rmse_test_final <span class="op">=</span> np.sqrt(best_model_mse_test_final)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_out:</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best hyperparameters:"</span>, grid_search.best_params_)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best Validation MSE:"</span>, <span class="op">-</span>grid_search.best_score_)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"-----------------------------"</span>)</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best Model Train MSE:"</span>, best_model_mse_train_final)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best Model Test MSE:"</span>, best_model_mse_test_final)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best Model Train RMSE:"</span>, best_model_rmse_train_final)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Best Model Test RMSE:"</span>, best_model_rmse_test_final)</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        best_model_mse_train_final,</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        best_model_mse_test_final,</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>        best_model_rmse_train_final,</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>        best_model_rmse_test_final,</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> grid_search_result(grid_search_dt, selector_dt, best_model_dt_final, <span class="st">'Decision Tree'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Decision Tree Best hyperparameters: {'max_depth': 3, 'min_samples_split': 3}
Decision Tree Best Validation MSE: 0.00010894315706034298
-----------------------------
Decision Tree Best Model Train MSE: 0.00011617992780627547
Decision Tree Best Model Test MSE: 5.0685852201722405e-05
Decision Tree Best Model Train RMSE: 0.010778679316422558
Decision Tree Best Model Test RMSE: 0.007119399707961508</code></pre>
</div>
</div>
</section>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">3.2 Random Forest</h3>
<section id="feature-selection-1" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-1">3.2.1 Feature Selection</h4>
<p>Because of the limited computing resource we have, we remove 5 features at each step in the RFE process.</p>
<div id="cell-48" class="cell" data-cell_id="b6d428b214bc4dcda9f71f1c68366667" data-deepnote_cell_type="code" data-execution_count="27">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Series Cross-Validation</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>tscv <span class="op">=</span> TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>step_rf <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RFECV, select some hyperparameters</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>selector_rf <span class="op">=</span> RFECV(</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>RandomForestRegressor(</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">30</span>,       <span class="co"># Fewer trees for speed (but not too few)</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">10</span>,          <span class="co"># Limit tree depth to avoid overfitting</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        max_samples<span class="op">=</span><span class="fl">0.6</span>,       <span class="co"># Use 60% of data per tree (balance speed and diversity)</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        min_samples_split<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Prevent splits on small nodes</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        min_samples_leaf<span class="op">=</span><span class="dv">15</span>,   <span class="co"># Require larger leaves</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        max_features<span class="op">=</span><span class="st">'log2'</span>,   <span class="co"># log2(800) ≈ 9 features per split (faster than 'sqrt')</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span>,             <span class="co"># Use all CPU cores</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,       <span class="co"># Reproducibility</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>        bootstrap<span class="op">=</span><span class="va">True</span>,        <span class="co"># Bootstrap sampling for diversity</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    step<span class="op">=</span>step_rf,             <span class="co"># Remove 5 features at each step</span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,                  <span class="co"># Time series split</span></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>  <span class="co"># Negative MSE for scoring</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and automatically select optimal number of features</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>selector_rf.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=RandomForestRegressor(max_depth=10, max_features='log2',
                                      max_samples=0.6, min_samples_leaf=15,
                                      min_samples_split=10, n_estimators=30,
                                      n_jobs=-1, random_state=42),
      scoring='neg_mean_squared_error', step=5, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox"><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">RFECV</label><div class="sk-toggleable__content"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=RandomForestRegressor(max_depth=10, max_features='log2',
                                      max_samples=0.6, min_samples_leaf=15,
                                      min_samples_split=10, n_estimators=30,
                                      n_jobs=-1, random_state=42),
      scoring='neg_mean_squared_error', step=5, verbose=1)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox"><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(max_depth=10, max_features='log2', max_samples=0.6,
                      min_samples_leaf=15, min_samples_split=10,
                      n_estimators=30, n_jobs=-1, random_state=42)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox"><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(max_depth=10, max_features='log2', max_samples=0.6,
                      min_samples_leaf=15, min_samples_split=10,
                      n_estimators=30, n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<div id="cell-49" class="cell" data-cell_id="5043c3d67b324a509611ed22d9c5d289" data-deepnote_cell_type="code" data-execution_count="28">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>joblib.dump(selector_rf, <span class="st">'selector_rf.joblib'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>['selector_rf.joblib']</code></pre>
</div>
</div>
<div id="cell-50" class="cell" data-cell_id="32b9ee726a5546edb94000a35c1ca505" data-deepnote_cell_type="code" data-execution_count="29">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>plot_n_feature_val_loss(selector_rf, step<span class="op">=</span>step_rf, model_name<span class="op">=</span><span class="st">'Random Forest'</span>, mark<span class="op">=</span><span class="st">'x'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-51" class="cell" data-cell_id="cebb4bfdd84644ea9da229a76e71b360" data-deepnote_cell_type="code" data-execution_count="30">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>selector_result(selector_rf, X_train, y_train, X_test, y_test, <span class="st">'Random Forest'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal number of features for Random Forest: 535
Selected features for Linear Random Forest: Index(['ROC_20_day-1', 'ROC_20_day-1^2', 'EMA_10_day-1', 'EMA_10_day-1^2',
       'EMA_20_day-1', 'EMA_20_day-1^2', 'EMA_50_day-1', 'EMA_50_day-1^2',
       'DTB4WK_day-1', 'DTB4WK_day-1^2',
       ...
       'Dollar index-F_day-5^2', 'Dollar index_day-5', 'Dollar index_day-5^2',
       'wheat-F_day-5', 'wheat-F_day-5^2', 'XAG_day-5', 'XAG_day-5^2',
       'XAU_day-5', 'XAU_day-5^2', 'day_of_week_day-5'],
      dtype='object', length=535)
Best Validation MSE: 0.00010155466999754498
-----------------------------
Best Random Forest Model Train MSE: 0.00010452561376682597
Best Random Forest Model Test MSE: 5.417641101158336e-05
Best Random Forest Model Train RMSE: 0.010223776883658307
Best Random Forest Model Test RMSE: 0.007360462690047641</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(0.00010452561376682597,
 5.417641101158336e-05,
 0.010223776883658307,
 0.007360462690047641)</code></pre>
</div>
</div>
</section>
<section id="hyperparameter-optimization-1" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-optimization-1">3.2.2 Hyperparameter Optimization</h4>
<div id="cell-53" class="cell" data-cell_id="1c30e2f3bed24afc9b7cf2dbe75d21b6" data-deepnote_cell_type="code" data-execution_count="31">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get selected features</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>X_train_selected_rf <span class="op">=</span> X_train.iloc[:, selector_rf.support_]</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter optimization</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>param_grid_rf <span class="op">=</span> {</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>],      <span class="co"># Fewer trees than default (balance speed and stability)</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>],            <span class="co"># Shallower trees to prevent overfitting</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],   <span class="co"># Larger splits for noisy data</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>],     <span class="co"># Larger leaves for regularization</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'log2'</span>, <span class="fl">0.05</span>],      <span class="co"># Fewer features per split (critical for high dimensions)</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bootstrap'</span>: [<span class="va">True</span>],               <span class="co"># Bootstrap sampling for robustness</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_samples'</span>: [<span class="fl">0.6</span>, <span class="fl">0.8</span>],           <span class="co"># Subsample data to increase diversity</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>grid_search_rf <span class="op">=</span> GridSearchCV(</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>RandomForestRegressor(n_jobs<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid_rf,</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>grid_search_rf.fit(X_train_selected_rf, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 3 folds for each of 324 candidates, totalling 972 fits</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=RandomForestRegressor(n_jobs=-1),
             param_grid={'bootstrap': [True], 'max_depth': [5, 10, 15],
                         'max_features': ['log2', 0.05],
                         'max_samples': [0.6, 0.8],
                         'min_samples_leaf': [5, 10, 15],
                         'min_samples_split': [10, 20, 30],
                         'n_estimators': [50, 100, 150]},
             scoring='neg_mean_squared_error', verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox"><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=RandomForestRegressor(n_jobs=-1),
             param_grid={'bootstrap': [True], 'max_depth': [5, 10, 15],
                         'max_features': ['log2', 0.05],
                         'max_samples': [0.6, 0.8],
                         'min_samples_leaf': [5, 10, 15],
                         'min_samples_split': [10, 20, 30],
                         'n_estimators': [50, 100, 150]},
             scoring='neg_mean_squared_error', verbose=1)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox"><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(n_jobs=-1)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox"><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(n_jobs=-1)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<p>Here’s the optimized parameters for random forest.</p>
<div id="cell-55" class="cell" data-cell_id="ff92f38f984340da92fee0731ea994e0" data-deepnote_cell_type="code" data-execution_count="32">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the final model</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>best_model_rf_final <span class="op">=</span> grid_search_rf.best_estimator_</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>best_model_rf_final.fit(X_train[X.columns[selector_rf.support_]], y_train)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>joblib.dump(best_model_rf_final, <span class="st">'best_model_rf_final.joblib'</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> grid_search_result(grid_search_rf, selector_rf, best_model_rf_final, <span class="st">'Random Forest'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Forest Best hyperparameters: {'bootstrap': True, 'max_depth': 5, 'max_features': 'log2', 'max_samples': 0.8, 'min_samples_leaf': 15, 'min_samples_split': 10, 'n_estimators': 50}
Random Forest Best Validation MSE: 0.00010222025743517132
-----------------------------
Random Forest Best Model Train MSE: 0.00011301699070842933
Random Forest Best Model Test MSE: 5.164476926179037e-05
Random Forest Best Model Train RMSE: 0.010630944958395248
Random Forest Best Model Test RMSE: 0.0071864295211036735</code></pre>
</div>
</div>
</section>
</section>
<section id="xgboost" class="level3">
<h3 class="anchored" data-anchor-id="xgboost">3.3 XGBoost</h3>
<section id="feature-selection-2" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-2">3.3.1 Feature Selection</h4>
<div id="cell-58" class="cell" data-cell_id="f82ffd65bd5a4fe3b24dadfd96e8aba8" data-deepnote_cell_type="code" data-execution_count="33">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Series Cross-Validation</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>tscv <span class="op">=</span> TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>step_xgb <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RFECV</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>selector_xgb <span class="op">=</span> RFECV(</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>XGBRegressor(</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">50</span>,  <span class="co"># Number of trees (balance speed and performance)</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">5</span>,  <span class="co"># Limit tree depth to prevent overfitting</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>        subsample<span class="op">=</span><span class="fl">0.8</span>,  <span class="co"># Use 80% of samples per tree (introduce diversity)</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>        min_child_weight<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Minimum sum of instance weights (controls splits)</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        tree_method<span class="op">=</span><span class="st">'hist'</span>,  <span class="co"># Histogram-based tree method (faster than 'exact')</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span><span class="st">'cuda'</span>,  <span class="co"># Use GPU for training (if available)</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,  <span class="co"># Reproducibility</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span>  <span class="co"># Use all CPU cores for parallel processing</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    step<span class="op">=</span>step_xgb,  <span class="co"># Remove 5 features at each step</span></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,  <span class="co"># Time series split</span></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>  <span class="co"># Negative MSE for scoring</span></span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and automatically select optimal number of features</span></span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>selector_xgb.fit(X_train, y_train)</span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting estimator with 880 features.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Roaming\Python\Python311\site-packages\xgboost\core.py:158: UserWarning: [19:22:12] WARNING: C:\buildkite-agent\builds\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\xgboost\xgboost-ci-windows\src\common\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  warnings.warn(smsg, UserWarning)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.
Fitting estimator with 880 features.
Fitting estimator with 875 features.
Fitting estimator with 870 features.
Fitting estimator with 865 features.
Fitting estimator with 860 features.
Fitting estimator with 855 features.
Fitting estimator with 850 features.
Fitting estimator with 845 features.
Fitting estimator with 840 features.
Fitting estimator with 835 features.
Fitting estimator with 830 features.
Fitting estimator with 825 features.
Fitting estimator with 820 features.
Fitting estimator with 815 features.
Fitting estimator with 810 features.
Fitting estimator with 805 features.
Fitting estimator with 800 features.
Fitting estimator with 795 features.
Fitting estimator with 790 features.
Fitting estimator with 785 features.
Fitting estimator with 780 features.
Fitting estimator with 775 features.
Fitting estimator with 770 features.
Fitting estimator with 765 features.
Fitting estimator with 760 features.
Fitting estimator with 755 features.
Fitting estimator with 750 features.
Fitting estimator with 745 features.
Fitting estimator with 740 features.
Fitting estimator with 735 features.
Fitting estimator with 730 features.
Fitting estimator with 725 features.
Fitting estimator with 720 features.
Fitting estimator with 715 features.
Fitting estimator with 710 features.
Fitting estimator with 705 features.
Fitting estimator with 700 features.
Fitting estimator with 695 features.
Fitting estimator with 690 features.
Fitting estimator with 685 features.
Fitting estimator with 680 features.
Fitting estimator with 675 features.
Fitting estimator with 670 features.
Fitting estimator with 665 features.
Fitting estimator with 660 features.
Fitting estimator with 655 features.
Fitting estimator with 650 features.
Fitting estimator with 645 features.
Fitting estimator with 640 features.
Fitting estimator with 635 features.
Fitting estimator with 630 features.
Fitting estimator with 625 features.
Fitting estimator with 620 features.
Fitting estimator with 615 features.
Fitting estimator with 610 features.
Fitting estimator with 605 features.
Fitting estimator with 600 features.
Fitting estimator with 595 features.
Fitting estimator with 590 features.
Fitting estimator with 585 features.
Fitting estimator with 580 features.
Fitting estimator with 575 features.
Fitting estimator with 570 features.
Fitting estimator with 565 features.
Fitting estimator with 560 features.
Fitting estimator with 555 features.
Fitting estimator with 550 features.
Fitting estimator with 545 features.
Fitting estimator with 540 features.
Fitting estimator with 535 features.
Fitting estimator with 530 features.
Fitting estimator with 525 features.
Fitting estimator with 520 features.
Fitting estimator with 515 features.
Fitting estimator with 510 features.
Fitting estimator with 505 features.
Fitting estimator with 500 features.
Fitting estimator with 495 features.
Fitting estimator with 490 features.
Fitting estimator with 485 features.
Fitting estimator with 480 features.
Fitting estimator with 475 features.
Fitting estimator with 470 features.
Fitting estimator with 465 features.
Fitting estimator with 460 features.
Fitting estimator with 455 features.
Fitting estimator with 450 features.
Fitting estimator with 445 features.
Fitting estimator with 440 features.
Fitting estimator with 435 features.
Fitting estimator with 430 features.
Fitting estimator with 425 features.
Fitting estimator with 420 features.
Fitting estimator with 415 features.
Fitting estimator with 410 features.
Fitting estimator with 405 features.
Fitting estimator with 400 features.
Fitting estimator with 395 features.
Fitting estimator with 390 features.
Fitting estimator with 385 features.
Fitting estimator with 380 features.
Fitting estimator with 375 features.
Fitting estimator with 370 features.
Fitting estimator with 365 features.
Fitting estimator with 360 features.
Fitting estimator with 355 features.
Fitting estimator with 350 features.
Fitting estimator with 345 features.
Fitting estimator with 340 features.
Fitting estimator with 335 features.
Fitting estimator with 330 features.
Fitting estimator with 325 features.
Fitting estimator with 320 features.
Fitting estimator with 315 features.
Fitting estimator with 310 features.
Fitting estimator with 305 features.
Fitting estimator with 300 features.
Fitting estimator with 295 features.
Fitting estimator with 290 features.
Fitting estimator with 285 features.
Fitting estimator with 280 features.
Fitting estimator with 275 features.
Fitting estimator with 270 features.
Fitting estimator with 265 features.
Fitting estimator with 260 features.
Fitting estimator with 255 features.
Fitting estimator with 250 features.
Fitting estimator with 245 features.
Fitting estimator with 240 features.
Fitting estimator with 235 features.
Fitting estimator with 230 features.
Fitting estimator with 225 features.
Fitting estimator with 220 features.
Fitting estimator with 215 features.
Fitting estimator with 210 features.
Fitting estimator with 205 features.
Fitting estimator with 200 features.
Fitting estimator with 195 features.
Fitting estimator with 190 features.
Fitting estimator with 185 features.
Fitting estimator with 180 features.
Fitting estimator with 175 features.
Fitting estimator with 170 features.
Fitting estimator with 165 features.
Fitting estimator with 160 features.
Fitting estimator with 155 features.
Fitting estimator with 150 features.
Fitting estimator with 145 features.
Fitting estimator with 140 features.
Fitting estimator with 135 features.
Fitting estimator with 130 features.
Fitting estimator with 125 features.
Fitting estimator with 120 features.
Fitting estimator with 115 features.
Fitting estimator with 110 features.
Fitting estimator with 105 features.
Fitting estimator with 100 features.
Fitting estimator with 95 features.
Fitting estimator with 90 features.
Fitting estimator with 85 features.
Fitting estimator with 80 features.
Fitting estimator with 75 features.
Fitting estimator with 70 features.
Fitting estimator with 65 features.
Fitting estimator with 60 features.
Fitting estimator with 55 features.
Fitting estimator with 50 features.
Fitting estimator with 45 features.
Fitting estimator with 40 features.
Fitting estimator with 35 features.
Fitting estimator with 30 features.
Fitting estimator with 25 features.
Fitting estimator with 20 features.
Fitting estimator with 15 features.
Fitting estimator with 10 features.
Fitting estimator with 5 features.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="33">
<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=XGBRegressor(base_score=None, booster=None, callbacks=None,
                             colsample_bylevel=None, colsample_bynode=None,
                             colsample_bytree=None, device='cuda',
                             early_stopping_rounds=None,
                             enable_categorical=False, eval_metric=None,
                             feature_types=None, gamma=None, grow_policy=None,
                             importan...teraction_constraints=None,
                             learning_rate=None, max_bin=None,
                             max_cat_threshold=None, max_cat_to_onehot=None,
                             max_delta_step=None, max_depth=5, max_leaves=None,
                             min_child_weight=3, missing=nan,
                             monotone_constraints=None, multi_strategy=None,
                             n_estimators=50, n_jobs=-1, num_parallel_tree=None,
                             random_state=42, ...),
      scoring='neg_mean_squared_error', step=5, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox"><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">RFECV</label><div class="sk-toggleable__content"><pre>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
      estimator=XGBRegressor(base_score=None, booster=None, callbacks=None,
                             colsample_bylevel=None, colsample_bynode=None,
                             colsample_bytree=None, device='cuda',
                             early_stopping_rounds=None,
                             enable_categorical=False, eval_metric=None,
                             feature_types=None, gamma=None, grow_policy=None,
                             importan...teraction_constraints=None,
                             learning_rate=None, max_bin=None,
                             max_cat_threshold=None, max_cat_to_onehot=None,
                             max_delta_step=None, max_depth=5, max_leaves=None,
                             min_child_weight=3, missing=nan,
                             monotone_constraints=None, multi_strategy=None,
                             n_estimators=50, n_jobs=-1, num_parallel_tree=None,
                             random_state=42, ...),
      scoring='neg_mean_squared_error', step=5, verbose=1)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-14" type="checkbox"><label for="sk-estimator-id-14" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: XGBRegressor</label><div class="sk-toggleable__content"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cuda', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=5, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=50, n_jobs=-1,
             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-15" type="checkbox"><label for="sk-estimator-id-15" class="sk-toggleable__label sk-toggleable__label-arrow">XGBRegressor</label><div class="sk-toggleable__content"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cuda', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=5, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=50, n_jobs=-1,
             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<div id="cell-59" class="cell" data-cell_id="afb06893707e4fb5bcf44b0537aa222e" data-deepnote_cell_type="code" data-execution_count="34">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>joblib.dump(selector_xgb, <span class="st">'selector_xgb.joblib'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>['selector_xgb.joblib']</code></pre>
</div>
</div>
<div id="cell-60" class="cell" data-cell_id="0c9091655ab6462a84fb6153071e3695" data-deepnote_cell_type="code" data-execution_count="35">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>plot_n_feature_val_loss(selector_xgb, step<span class="op">=</span>step_xgb, model_name<span class="op">=</span><span class="st">'XGBoost'</span>, mark<span class="op">=</span><span class="st">'x'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-36-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-61" class="cell" data-cell_id="7914c0444a6e4c7cafee4421e2ffff80" data-deepnote_cell_type="code" data-execution_count="36">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>selector_result(selector_xgb, X_train, y_train, X_test, y_test, <span class="st">'XGBoost'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal number of features for XGBoost: 1
Selected features for Linear XGBoost: Index(['ROC_20_day-1'], dtype='object')
Best Validation MSE: 0.00012259275374271517
-----------------------------
Best XGBoost Model Train MSE: 0.00010600596254067053
Best XGBoost Model Test MSE: 5.943372552324083e-05
Best XGBoost Model Train RMSE: 0.010295919703487907
Best XGBoost Model Test RMSE: 0.007709327177078479</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>(0.00010600596254067053,
 5.943372552324083e-05,
 0.010295919703487907,
 0.007709327177078479)</code></pre>
</div>
</div>
</section>
<section id="hyperparameter-optimization-2" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-optimization-2">3.3.2 Hyperparameter Optimization</h4>
<div id="cell-63" class="cell" data-cell_id="241e96901d5e4b9d932153fc7940c571" data-deepnote_cell_type="code" data-execution_count="37">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, TimeSeriesSplit</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Series Cross-Validation</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>tscv <span class="op">=</span> TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize GridSearchCV</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>param_grid_xgb <span class="op">=</span> {</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">50</span>, <span class="dv">100</span>],  <span class="co"># Number of trees</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],  <span class="co"># Maximum tree depth</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>],  <span class="co"># Fraction of samples per tree</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colsample_bytree'</span>: [<span class="fl">0.6</span>, <span class="fl">1.0</span>],  <span class="co"># Fraction of features per tree</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gamma'</span>: [<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],  <span class="co"># Minimum loss reduction for a split</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_child_weight'</span>: [<span class="dv">3</span>, <span class="dv">5</span>],  <span class="co"># Minimum sum of instance weights</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>grid_search_xgb <span class="op">=</span> GridSearchCV(</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>XGBRegressor(</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>        tree_method<span class="op">=</span><span class="st">'hist'</span>,  <span class="co"># Use GPU for training</span></span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span><span class="st">'cuda'</span>,  <span class="co"># Specify GPU device</span></span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span>,  <span class="co"># Enable CPU parallelism (optional, if GPU is not available)</span></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid_xgb,</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>tscv,  <span class="co"># Time series split</span></span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,  <span class="co"># Negative MSE for scoring</span></span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Print progress</span></span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,  <span class="co"># Use all CPU cores</span></span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a>grid_search_xgb.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 3 folds for each of 144 candidates, totalling 432 fits</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\ProgramData\anaconda3\Lib\site-packages\joblib\externals\loky\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="37">
<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=XGBRegressor(base_score=None, booster=None,
                                    callbacks=None, colsample_bylevel=None,
                                    colsample_bynode=None,
                                    colsample_bytree=None, device='cuda',
                                    early_stopping_rounds=None,
                                    enable_categorical=False, eval_metric=None,
                                    feature_types=None, gamma=None,
                                    grow_policy=None, i...
                                    min_child_weight=None, missing=nan,
                                    monotone_constraints=None,
                                    multi_strategy=None, n_estimators=None,
                                    n_jobs=-1, num_parallel_tree=None,
                                    random_state=42, ...),
             n_jobs=-1,
             param_grid={'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.1, 0.2],
                         'max_depth': [3, 5, 7], 'min_child_weight': [3, 5],
                         'n_estimators': [50, 100], 'subsample': [0.8, 1.0]},
             scoring='neg_mean_squared_error', verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-16" type="checkbox"><label for="sk-estimator-id-16" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),
             estimator=XGBRegressor(base_score=None, booster=None,
                                    callbacks=None, colsample_bylevel=None,
                                    colsample_bynode=None,
                                    colsample_bytree=None, device='cuda',
                                    early_stopping_rounds=None,
                                    enable_categorical=False, eval_metric=None,
                                    feature_types=None, gamma=None,
                                    grow_policy=None, i...
                                    min_child_weight=None, missing=nan,
                                    monotone_constraints=None,
                                    multi_strategy=None, n_estimators=None,
                                    n_jobs=-1, num_parallel_tree=None,
                                    random_state=42, ...),
             n_jobs=-1,
             param_grid={'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.1, 0.2],
                         'max_depth': [3, 5, 7], 'min_child_weight': [3, 5],
                         'n_estimators': [50, 100], 'subsample': [0.8, 1.0]},
             scoring='neg_mean_squared_error', verbose=2)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-17" type="checkbox"><label for="sk-estimator-id-17" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: XGBRegressor</label><div class="sk-toggleable__content"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cuda', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-18" type="checkbox"><label for="sk-estimator-id-18" class="sk-toggleable__label sk-toggleable__label-arrow">XGBRegressor</label><div class="sk-toggleable__content"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cuda', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
<div id="cell-64" class="cell" data-cell_id="d63e3c4a14c34ea896cdc46a20bc3f9d" data-deepnote_cell_type="code" data-execution_count="38">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the final model</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>best_model_xgb_final <span class="op">=</span> grid_search_xgb.best_estimator_</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>best_model_xgb_final.fit(X_train[X.columns[selector_xgb.support_]], y_train)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>joblib.dump(best_model_xgb_final, <span class="st">'best_model_xgb_final.joblib'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>['best_model_xgb_final.joblib']</code></pre>
</div>
</div>
<div id="cell-65" class="cell" data-cell_id="62a99addf8b644349b12593974fe37ec" data-deepnote_cell_type="code" data-execution_count="39">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> grid_search_result(grid_search_xgb, selector_xgb, best_model_xgb_final, <span class="st">'XGBoost'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>XGBoost Best hyperparameters: {'colsample_bytree': 0.6, 'gamma': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.8}
XGBoost Best Validation MSE: 0.00010268245284882257
-----------------------------
XGBoost Best Model Train MSE: 0.00012705996490768268
XGBoost Best Model Test MSE: 5.086330269091728e-05
XGBoost Best Model Train RMSE: 0.011272087868167222
XGBoost Best Model Test RMSE: 0.00713185128076275</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="neural-network-models-for-regression" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-models-for-regression">4. Neural Network Models for Regression</h2>
<p>Because of the limited computing resource, we didn’t implement feature or hyperparameter optimization for Neural Network models. These models are just exploratory attempts.</p>
<section id="multilayer-perceptron" class="level3">
<h3 class="anchored" data-anchor-id="multilayer-perceptron">4.1 Multilayer Perceptron</h3>
<div id="cell-67" class="cell" data-cell_id="de8d08eb3b2b4bf799fccfd5d8b8b871" data-deepnote_cell_type="code" data-deepnote_to_be_reexecuted="true" data-execution_context_id="f873b5e3-7c84-405a-bfcf-9d6255ea39cd" data-execution_millis="2511" data-execution_start="1740857819090" data-source_hash="db849fe6" data-execution_count="40">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>MLP_reg <span class="op">=</span> Sequential([</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(X_train.shape[<span class="dv">1</span>],)),  <span class="co"># Input layer</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>),  <span class="co"># Hidden layer</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>)  <span class="co"># Output layer (linear activation)</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>MLP_reg.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>), loss<span class="op">=</span><span class="st">'mse'</span>)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>split_date <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.75</span> <span class="op">*</span> <span class="bu">len</span>(X_train))  <span class="co"># 60% training (0.75*80%), 20% validation</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>X_train_split, X_val <span class="op">=</span> X_train[:split_date], X_train[split_date:]</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>y_train_split, y_val <span class="op">=</span> y_train[:split_date], y_train[split_date:]</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> EarlyStopping(</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">'val_loss'</span>, </span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>    restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>reduce_lr <span class="op">=</span> ReduceLROnPlateau(</span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>    factor<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>    min_lr<span class="op">=</span><span class="fl">1e-6</span>,</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [</span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a>    early_stopping,</span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a>    reduce_lr,</span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-32"><a href="#cb77-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb77-33"><a href="#cb77-33" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> MLP_reg.fit(</span>
<span id="cb77-34"><a href="#cb77-34" aria-hidden="true" tabindex="-1"></a>    X_train_split,</span>
<span id="cb77-35"><a href="#cb77-35" aria-hidden="true" tabindex="-1"></a>    y_train_split,</span>
<span id="cb77-36"><a href="#cb77-36" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb77-37"><a href="#cb77-37" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb77-38"><a href="#cb77-38" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb77-39"><a href="#cb77-39" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>callbacks,</span>
<span id="cb77-40"><a href="#cb77-40" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Roaming\Python\Python311\site-packages\keras\src\layers\core\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre>Epoch 1/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">2s</span> 12ms/step - loss: 48.4190 - val_loss: 0.0687 - learning_rate: 0.0100

Epoch 2/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0013 - val_loss: 0.0241 - learning_rate: 0.0100

Epoch 3/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0010 - val_loss: 0.0222 - learning_rate: 0.0100

Epoch 4/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 7.7949e-04 - val_loss: 0.0210 - learning_rate: 0.0100

Epoch 5/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.6575e-04 - val_loss: 0.0201 - learning_rate: 0.0100

Epoch 6/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 4.1016e-04 - val_loss: 0.0195 - learning_rate: 0.0100

Epoch 7/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 2.6531e-04 - val_loss: 0.0188 - learning_rate: 0.0100

Epoch 8/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 2.0890e-04 - val_loss: 0.0184 - learning_rate: 0.0100

Epoch 9/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.7962e-04 - val_loss: 0.0179 - learning_rate: 0.0100

Epoch 10/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4781e-04 - val_loss: 0.0176 - learning_rate: 0.0100

Epoch 11/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2998e-04 - val_loss: 0.0174 - learning_rate: 0.0100

Epoch 12/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3270e-04 - val_loss: 0.0173 - learning_rate: 0.0100

Epoch 13/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3352e-04 - val_loss: 0.0172 - learning_rate: 0.0100

Epoch 14/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3359e-04 - val_loss: 0.0171 - learning_rate: 0.0100

Epoch 15/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3150e-04 - val_loss: 0.0171 - learning_rate: 0.0100

Epoch 16/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3089e-04 - val_loss: 0.0171 - learning_rate: 0.0100

Epoch 17/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2469e-04 - val_loss: 0.0171 - learning_rate: 0.0100

Epoch 18/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3430e-04 - val_loss: 0.0170 - learning_rate: 0.0100

Epoch 19/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2950e-04 - val_loss: 0.0170 - learning_rate: 0.0100

Epoch 20/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2746e-04 - val_loss: 0.0170 - learning_rate: 0.0100

Epoch 21/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2412e-04 - val_loss: 0.0170 - learning_rate: 0.0100

Epoch 22/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4509e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03

Epoch 23/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3784e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03

Epoch 24/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3388e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03

Epoch 25/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3646e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03

Epoch 26/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.2867e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-03

Epoch 27/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.4292e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04

Epoch 28/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.2472e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04

Epoch 29/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2470e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04

Epoch 30/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2030e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04

Epoch 31/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4047e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-04

Epoch 32/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4166e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05

Epoch 33/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3643e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05

Epoch 34/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2316e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05

Epoch 35/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2334e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05

Epoch 36/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3595e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-05

Epoch 37/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4116e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 38/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3190e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 39/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3455e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 40/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3248e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 41/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3989e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 42/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3399e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 43/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.2894e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06

Epoch 44/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3307e-04 - val_loss: 0.0170 - learning_rate: 1.0000e-06
</pre>
</div>
</div>
</div>
<div id="cell-68" class="cell" data-cell_id="9a5de7463bc94926b3b8dad49e0bd986" data-deepnote_cell_type="code" data-execution_count="41">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>MLP_reg.save(<span class="st">'MLP_reg.keras'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-69" class="cell" data-cell_id="5abdefd1dfc84db9b0bdbacae109a62d" data-deepnote_cell_type="code" data-deepnote_to_be_reexecuted="true" data-execution_context_id="f3954174-178d-417f-82c6-efb0a8cff8c7" data-execution_millis="531" data-execution_start="1740767303778" data-source_hash="5b680c6b" data-execution_count="42">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training and Validation Loss of MLP for Regression'</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-70" class="cell" data-cell_id="33447c69167b4efb9b8a9f085abbaa18" data-deepnote_cell_type="code" data-execution_count="43">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> result_nn(model, model_name<span class="op">=</span><span class="st">''</span>, X_tr<span class="op">=</span>X_train, y_tr<span class="op">=</span>y_train, X_te<span class="op">=</span>X_test, y_te<span class="op">=</span>y_test, print_out<span class="op">=</span><span class="va">True</span>, predict<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    train_MSE <span class="op">=</span> model.evaluate(X_tr, y_tr)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    test_MSE <span class="op">=</span> model.evaluate(X_te, y_te)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    train_RMSE <span class="op">=</span> np.sqrt(train_MSE)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    test_RMSE <span class="op">=</span> np.sqrt(test_MSE)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    y_test_predict <span class="op">=</span> model.predict(X_te)</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_out:</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Train MSE: </span><span class="sc">{</span>train_MSE<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Test MSE: </span><span class="sc">{</span>test_MSE<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Train RMSE: </span><span class="sc">{</span>train_RMSE<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Test RMSE: </span><span class="sc">{</span>test_RMSE<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> predict:</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> train_MSE, test_MSE, train_RMSE, test_RMSE</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">list</span>(y_test_predict)</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> result_nn(MLP_reg, <span class="st">'MLP (Regression)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">49/49</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 1ms/step - loss: 8.8159e-04 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0018 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step 

MLP (Regression) Train MSE: 0.00435810349881649

MLP (Regression) Test MSE: 0.001041724462993443

MLP (Regression) Train RMSE: 0.06601593367374645

MLP (Regression) Test RMSE: 0.03227575658281991
</pre>
</div>
</div>
</div>
</section>
<section id="long-short-term-memory-lstm-network" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm-network">4.2 Long Short Term Memory (LSTM) Network</h3>
<section id="additional-data-transformation" class="level4">
<h4 class="anchored" data-anchor-id="additional-data-transformation">4.2.1 Additional data transformation</h4>
<p>We need to modify the data shape to fit into the LSTM network.</p>
<div id="cell-73" class="cell" data-cell_id="044f87c096b74222a7a2673cd803bd14" data-deepnote_cell_type="code" data-execution_count="44">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose the target variable</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>target_col <span class="op">=</span> <span class="st">'Close_ROC'</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>df_nn <span class="op">=</span> df_raw.drop(columns<span class="op">=</span>[<span class="st">'Date'</span>, <span class="st">'Name'</span>], errors<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>df_nn <span class="op">=</span> df_nn.dropna()</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale all features using StandardScaler</span></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>scaled_data <span class="op">=</span> scaler.fit_transform(df_nn)</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We use the previous 5 days data to predict</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-74" class="cell" data-cell_id="4e135b65f9654955be7b293d49bc1db9" data-deepnote_cell_type="code" data-execution_count="45">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sequences (X) and target (y)</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>X_nn, y_nn <span class="op">=</span> [], []</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df_nn) <span class="op">-</span> sequence_length):</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    X_nn.append(df_nn.iloc[i:i<span class="op">+</span>sequence_length].values)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    y_nn.append(df_nn[target_col].iloc[i<span class="op">+</span>sequence_length])</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to numpy arrays</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>X_nn <span class="op">=</span> np.array(X_nn)</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>y_nn <span class="op">=</span> np.array(y_nn)</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For example, split the data into training (60%), validation (20%), and test (20%) sets</span></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.60</span> <span class="op">*</span> <span class="bu">len</span>(X_nn))</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.20</span> <span class="op">*</span> <span class="bu">len</span>(X_nn))</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>test_split <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.80</span> <span class="op">*</span> <span class="bu">len</span>(X_nn))</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>X_train_nn, y_train_nn <span class="op">=</span> X_nn[:train_size], y_nn[:train_size]</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>X_val_nn, y_val_nn <span class="op">=</span> X_nn[train_size:train_size<span class="op">+</span>val_size], y_nn[train_size:train_size<span class="op">+</span>val_size]</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>X_test_nn, y_test_nn <span class="op">=</span> X_nn[test_split:], y_nn[test_split:]</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X_train shape:"</span>, X_train_nn.shape)  <span class="co"># (num_train_samples, window_size, num_features)</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y_train shape:"</span>, y_train_nn.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X_train shape: (1157, 5, 88)
y_train shape: (1157,)</code></pre>
</div>
</div>
</section>
<section id="training" class="level4">
<h4 class="anchored" data-anchor-id="training">4.2.2 Training</h4>
<div id="cell-76" class="cell" data-cell_id="a53c89f7598f48349aede4a0cf238e76" data-deepnote_cell_type="code" data-execution_count="46">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>lstm_reg <span class="op">=</span> Sequential([</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    LSTM(</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>        <span class="dv">128</span>,</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>        return_sequences<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>        input_shape<span class="op">=</span>(X_train_nn.shape[<span class="dv">1</span>], X_train_nn.shape[<span class="dv">2</span>]),</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>        dropout<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>        recurrent_dropout<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.5</span>),</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.5</span>),</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>),</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>lstm_reg.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>), loss<span class="op">=</span><span class="st">'mse'</span>)</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Early stopping</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> EarlyStopping(</span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a>    restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> lstm_reg.fit(</span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>    X_train_nn,</span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>    y_train_nn,</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val_nn, y_val_nn),</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping],</span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-34"><a href="#cb85-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the trained model</span></span>
<span id="cb85-35"><a href="#cb85-35" aria-hidden="true" tabindex="-1"></a>lstm_reg.save(<span class="st">'lstm_reg.keras'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Roaming\Python\Python311\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">4s</span> 17ms/step - loss: 0.6221 - val_loss: 0.0699

Epoch 2/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.3012 - val_loss: 0.0381

Epoch 3/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.2395 - val_loss: 0.0055

Epoch 4/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.1609 - val_loss: 8.2306e-04

Epoch 5/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0949 - val_loss: 1.4668e-04

Epoch 6/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0803 - val_loss: 1.2618e-04

Epoch 7/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0571 - val_loss: 1.1901e-04

Epoch 8/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0468 - val_loss: 2.3850e-04

Epoch 9/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0439 - val_loss: 2.5668e-04

Epoch 10/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0344 - val_loss: 3.3398e-04

Epoch 11/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0267 - val_loss: 3.1353e-04

Epoch 12/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0273 - val_loss: 1.1845e-04

Epoch 13/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0197 - val_loss: 1.3941e-04

Epoch 14/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0177 - val_loss: 1.5279e-04

Epoch 15/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 7ms/step - loss: 0.0172 - val_loss: 1.5038e-04

Epoch 16/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0121 - val_loss: 2.1746e-04

Epoch 17/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0108 - val_loss: 1.5716e-04

Epoch 18/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0088 - val_loss: 1.7635e-04

Epoch 19/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0076 - val_loss: 1.4064e-04

Epoch 20/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0082 - val_loss: 1.6360e-04

Epoch 21/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0055 - val_loss: 1.7268e-04

Epoch 22/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0073 - val_loss: 2.0646e-04

Epoch 23/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0053 - val_loss: 1.3289e-04

Epoch 24/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0046 - val_loss: 1.6827e-04

Epoch 25/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0043 - val_loss: 1.6713e-04

Epoch 26/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0039 - val_loss: 1.3411e-04

Epoch 27/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0038 - val_loss: 1.5198e-04

Epoch 28/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0033 - val_loss: 1.4898e-04

Epoch 29/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0028 - val_loss: 1.1839e-04

Epoch 30/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0026 - val_loss: 1.4790e-04

Epoch 31/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0022 - val_loss: 1.3562e-04

Epoch 32/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0024 - val_loss: 1.3838e-04

Epoch 33/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0021 - val_loss: 1.2952e-04

Epoch 34/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0019 - val_loss: 1.2973e-04

Epoch 35/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0019 - val_loss: 1.3039e-04

Epoch 36/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0015 - val_loss: 1.1754e-04

Epoch 37/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0019 - val_loss: 1.1735e-04

Epoch 38/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0017 - val_loss: 1.1970e-04

Epoch 39/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0016 - val_loss: 1.2148e-04

Epoch 40/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 7ms/step - loss: 0.0012 - val_loss: 1.2273e-04

Epoch 41/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0013 - val_loss: 1.2091e-04

Epoch 42/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 0.0014 - val_loss: 1.2954e-04

Epoch 43/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0012 - val_loss: 1.2313e-04

Epoch 44/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 9.2076e-04 - val_loss: 1.1762e-04

Epoch 45/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 9.8157e-04 - val_loss: 1.2080e-04

Epoch 46/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 7.5778e-04 - val_loss: 1.2152e-04

Epoch 47/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 8.6865e-04 - val_loss: 1.2130e-04

Epoch 48/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 8.2736e-04 - val_loss: 1.1941e-04

Epoch 49/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 8.1362e-04 - val_loss: 1.2545e-04

Epoch 50/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 8.3782e-04 - val_loss: 1.1824e-04

Epoch 51/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 9.2354e-04 - val_loss: 1.1838e-04

Epoch 52/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.8213e-04 - val_loss: 1.1737e-04

Epoch 53/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.9316e-04 - val_loss: 1.1770e-04

Epoch 54/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 7.9941e-04 - val_loss: 1.1710e-04

Epoch 55/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.0735e-04 - val_loss: 1.1709e-04

Epoch 56/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 5.3108e-04 - val_loss: 1.1969e-04

Epoch 57/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 6.1958e-04 - val_loss: 1.1820e-04

Epoch 58/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 7.5449e-04 - val_loss: 1.1930e-04

Epoch 59/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.4645e-04 - val_loss: 1.2019e-04

Epoch 60/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 4.7346e-04 - val_loss: 1.1966e-04

Epoch 61/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 3.9326e-04 - val_loss: 1.1716e-04

Epoch 62/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 5.5395e-04 - val_loss: 1.2005e-04

Epoch 63/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 6.4767e-04 - val_loss: 1.1854e-04

Epoch 64/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 4.5624e-04 - val_loss: 1.1788e-04

Epoch 65/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 4.4687e-04 - val_loss: 1.1709e-04

Epoch 66/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.9228e-04 - val_loss: 1.1749e-04

Epoch 67/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 3.8819e-04 - val_loss: 1.1715e-04

Epoch 68/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.5908e-04 - val_loss: 1.1854e-04

Epoch 69/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.3763e-04 - val_loss: 1.1755e-04

Epoch 70/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 3.7734e-04 - val_loss: 1.1709e-04

Epoch 71/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.5650e-04 - val_loss: 1.1760e-04

Epoch 72/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.1977e-04 - val_loss: 1.1709e-04

Epoch 73/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 3.7925e-04 - val_loss: 1.1741e-04

Epoch 74/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.5705e-04 - val_loss: 1.1708e-04

Epoch 75/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.0423e-04 - val_loss: 1.1726e-04

Epoch 76/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 4.2551e-04 - val_loss: 1.1716e-04

Epoch 77/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.1666e-04 - val_loss: 1.1729e-04

Epoch 78/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.1278e-04 - val_loss: 1.1805e-04

Epoch 79/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.0009e-04 - val_loss: 1.1710e-04

Epoch 80/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.4286e-04 - val_loss: 1.2090e-04

Epoch 81/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.4950e-04 - val_loss: 1.1722e-04

Epoch 82/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.5682e-04 - val_loss: 1.1727e-04

Epoch 83/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.3842e-04 - val_loss: 1.1708e-04

Epoch 84/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 3.6073e-04 - val_loss: 1.1724e-04

Epoch 85/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.4341e-04 - val_loss: 1.1784e-04

Epoch 86/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.2171e-04 - val_loss: 1.1711e-04

Epoch 87/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.0149e-04 - val_loss: 1.1723e-04

Epoch 88/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.8024e-04 - val_loss: 1.1805e-04

Epoch 89/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.2784e-04 - val_loss: 1.1708e-04

Epoch 90/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.3941e-04 - val_loss: 1.1712e-04

Epoch 91/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.8911e-04 - val_loss: 1.1848e-04

Epoch 92/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.5088e-04 - val_loss: 1.1739e-04

Epoch 93/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 2.2310e-04 - val_loss: 1.1789e-04

Epoch 94/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.8202e-04 - val_loss: 1.1739e-04

Epoch 95/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 2.1815e-04 - val_loss: 1.1801e-04

Epoch 96/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 1.9053e-04 - val_loss: 1.1784e-04

Epoch 97/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 7ms/step - loss: 2.1361e-04 - val_loss: 1.1913e-04

Epoch 98/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 2.0350e-04 - val_loss: 1.1918e-04

Epoch 99/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 2.1561e-04 - val_loss: 1.2011e-04

Epoch 100/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 1.8266e-04 - val_loss: 1.1839e-04
</pre>
</div>
</div>
</div>
<div id="cell-77" class="cell" data-cell_id="f444fe1dd6244cd6a134bca7d7b09639" data-deepnote_cell_type="code" data-execution_count="47">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training and Validation Loss of LSTM for Regression'</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-78" class="cell" data-cell_id="142c241fa06b45138edc5766f6768321" data-deepnote_cell_type="code" data-execution_count="48">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> result_nn(</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    lstm_reg,</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LSTM (Regression)'</span>,</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    X_tr<span class="op">=</span>X_train_nn,</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    y_tr<span class="op">=</span>y_train_nn,</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    X_te<span class="op">=</span>X_test_nn,</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    y_te<span class="op">=</span>y_test_nn,</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 1.6815e-04

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 6.2900e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">1s</span> 53ms/step

LSTM (Regression) Train MSE: 0.0001305853365920484

LSTM (Regression) Test MSE: 5.119918205309659e-05

LSTM (Regression) Train RMSE: 0.01142739412954889

LSTM (Regression) Test RMSE: 0.007155360371993614
</pre>
</div>
</div>
</div>
</section>
</section>
<section id="convolutional-neural-network-cnn" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-network-cnn">4.3 Convolutional Neural Network (CNN)</h3>
<div id="cell-80" class="cell" data-cell_id="f5b8ec2e967b4a52827734c5c8ab0f16" data-deepnote_cell_type="code" data-execution_count="49">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>num_features <span class="op">=</span> X_train_nn.shape[<span class="dv">2</span>]  <span class="co"># number of columns in original dataset</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>CNN1D_reg <span class="op">=</span> Sequential([</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    Conv1D(</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'same'</span>,</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        input_shape<span class="op">=</span>(sequence_length, num_features),</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>    MaxPooling1D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    Conv1D(</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'same'</span>,</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>    MaxPooling1D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>    Flatten(),</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>),</span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a>CNN1D_reg.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.1</span>), loss<span class="op">=</span><span class="st">'mse'</span>)</span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-26"><a href="#cb90-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Early stopping</span></span>
<span id="cb90-27"><a href="#cb90-27" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> EarlyStopping(</span>
<span id="cb90-28"><a href="#cb90-28" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb90-29"><a href="#cb90-29" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb90-30"><a href="#cb90-30" aria-hidden="true" tabindex="-1"></a>    restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb90-31"><a href="#cb90-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb90-32"><a href="#cb90-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-33"><a href="#cb90-33" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> CNN1D_reg.fit(</span>
<span id="cb90-34"><a href="#cb90-34" aria-hidden="true" tabindex="-1"></a>    X_train_nn,</span>
<span id="cb90-35"><a href="#cb90-35" aria-hidden="true" tabindex="-1"></a>    y_train_nn,</span>
<span id="cb90-36"><a href="#cb90-36" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val_nn, y_val_nn),</span>
<span id="cb90-37"><a href="#cb90-37" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb90-38"><a href="#cb90-38" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb90-39"><a href="#cb90-39" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb90-40"><a href="#cb90-40" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb90-41"><a href="#cb90-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb90-42"><a href="#cb90-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-43"><a href="#cb90-43" aria-hidden="true" tabindex="-1"></a>CNN1D_reg.save(<span class="st">'CNN1D_reg.keras'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\zln92\AppData\Roaming\Python\Python311\site-packages\keras\src\layers\convolutional\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">3s</span> 9ms/step - loss: 1447449344.0000 - val_loss: 6036.1299 - learning_rate: 0.1000

Epoch 2/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 819.7986 - val_loss: 72.9835 - learning_rate: 0.1000

Epoch 3/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 11.2172 - val_loss: 0.7078 - learning_rate: 0.1000

Epoch 4/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.1930 - val_loss: 0.4386 - learning_rate: 0.1000

Epoch 5/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0319 - val_loss: 0.1091 - learning_rate: 0.1000

Epoch 6/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0172 - val_loss: 0.0432 - learning_rate: 0.1000

Epoch 7/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0208 - val_loss: 0.2549 - learning_rate: 0.1000

Epoch 8/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0197 - val_loss: 0.0231 - learning_rate: 0.1000

Epoch 9/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0278 - val_loss: 0.0348 - learning_rate: 0.1000

Epoch 10/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0258 - val_loss: 0.2173 - learning_rate: 0.1000

Epoch 11/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0241 - val_loss: 0.3775 - learning_rate: 0.1000

Epoch 12/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0492 - val_loss: 0.6089 - learning_rate: 0.1000

Epoch 13/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0616 - val_loss: 0.4884 - learning_rate: 0.1000

Epoch 14/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0375 - val_loss: 0.0468 - learning_rate: 0.0100

Epoch 15/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0174 - val_loss: 0.0760 - learning_rate: 0.0100

Epoch 16/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0150 - val_loss: 0.0345 - learning_rate: 0.0100

Epoch 17/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0159 - val_loss: 0.0963 - learning_rate: 0.0100

Epoch 18/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0129 - val_loss: 0.0694 - learning_rate: 0.0100

Epoch 19/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0125 - val_loss: 0.0744 - learning_rate: 1.0000e-03

Epoch 20/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0126 - val_loss: 0.0772 - learning_rate: 1.0000e-03

Epoch 21/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0134 - val_loss: 0.0679 - learning_rate: 1.0000e-03

Epoch 22/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0129 - val_loss: 0.0854 - learning_rate: 1.0000e-03

Epoch 23/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0139 - val_loss: 0.0701 - learning_rate: 1.0000e-03

Epoch 24/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0131 - val_loss: 0.0674 - learning_rate: 1.0000e-04

Epoch 25/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0131 - val_loss: 0.0684 - learning_rate: 1.0000e-04

Epoch 26/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0128 - val_loss: 0.0665 - learning_rate: 1.0000e-04

Epoch 27/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 0.0132 - val_loss: 0.0673 - learning_rate: 1.0000e-04

Epoch 28/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0130 - val_loss: 0.0680 - learning_rate: 1.0000e-04
</pre>
</div>
</div>
</div>
<div id="cell-81" class="cell" data-cell_id="977dc76327914aa9904d759f5eba8e24" data-deepnote_cell_type="code" data-execution_count="50">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training and Validation Loss of 1D CNN for Regression'</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-51-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-82" class="cell" data-cell_id="75dc4ab46e024b3fa7b5a03e9eb8fb0f" data-deepnote_cell_type="code" data-execution_count="51">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> result_nn(</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    CNN1D_reg,</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'1D CNN (Regression)'</span>,</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    X_tr<span class="op">=</span>X_train_nn,</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    y_tr<span class="op">=</span>y_train_nn,</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    X_te<span class="op">=</span>X_test_nn,</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    y_te<span class="op">=</span>y_test_nn,</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0319 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0471 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 8ms/step

1D CNN (Regression) Train MSE: 0.01970905438065529

1D CNN (Regression) Test MSE: 0.07369030267000198

1D CNN (Regression) Train RMSE: 0.14038893966639712

1D CNN (Regression) Test RMSE: 0.27145957833534257
</pre>
</div>
</div>
</div>
</section>
<section id="cnn-lstm" class="level3">
<h3 class="anchored" data-anchor-id="cnn-lstm">4.4 CNN LSTM</h3>
<div id="cell-84" class="cell" data-cell_id="2ec458feb8684988a74483c223a05a3f" data-deepnote_cell_type="code" data-execution_count="52">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>CNN1D_lstm_reg <span class="op">=</span> Sequential([</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    Conv1D(</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'same'</span>,</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>        input_shape<span class="op">=</span>(sequence_length, num_features),</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>    MaxPooling1D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>    Conv1D(</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">'relu'</span>,</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'same'</span>,</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    MaxPooling1D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>    LSTM(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>),</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>CNN1D_lstm_reg.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.1</span>), loss<span class="op">=</span><span class="st">'mse'</span>)</span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Early stopping</span></span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> EarlyStopping(</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a>    restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> CNN1D_lstm_reg.fit(</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>    X_train_nn,</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>    y_train_nn,</span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val_nn, y_val_nn),</span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a>CNN1D_lstm_reg.save(<span class="st">'CNN1D_lstm_reg.keras'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre>Epoch 1/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">3s</span> 12ms/step - loss: 0.8922 - val_loss: 0.0024 - learning_rate: 0.1000

Epoch 2/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 0.0021 - val_loss: 1.2201e-04 - learning_rate: 0.1000

Epoch 3/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.5326e-04 - val_loss: 1.1884e-04 - learning_rate: 0.1000

Epoch 4/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.5140e-04 - val_loss: 1.2202e-04 - learning_rate: 0.1000

Epoch 5/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2620e-04 - val_loss: 1.2179e-04 - learning_rate: 0.1000

Epoch 6/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4153e-04 - val_loss: 1.3648e-04 - learning_rate: 0.1000

Epoch 7/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4528e-04 - val_loss: 1.1876e-04 - learning_rate: 0.1000

Epoch 8/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2227e-04 - val_loss: 1.1712e-04 - learning_rate: 0.0100

Epoch 9/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.1566e-04 - val_loss: 1.1713e-04 - learning_rate: 0.0100

Epoch 10/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.5773e-04 - val_loss: 1.1710e-04 - learning_rate: 0.0100

Epoch 11/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2773e-04 - val_loss: 1.1764e-04 - learning_rate: 0.0100

Epoch 12/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2657e-04 - val_loss: 1.1710e-04 - learning_rate: 0.0100

Epoch 13/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3492e-04 - val_loss: 1.1714e-04 - learning_rate: 1.0000e-03

Epoch 14/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2028e-04 - val_loss: 1.1716e-04 - learning_rate: 1.0000e-03

Epoch 15/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3301e-04 - val_loss: 1.1712e-04 - learning_rate: 1.0000e-03

Epoch 16/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3516e-04 - val_loss: 1.1722e-04 - learning_rate: 1.0000e-03

Epoch 17/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.4031e-04 - val_loss: 1.1730e-04 - learning_rate: 1.0000e-03

Epoch 18/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.4178e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04

Epoch 19/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2104e-04 - val_loss: 1.1730e-04 - learning_rate: 1.0000e-04

Epoch 20/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3550e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04

Epoch 21/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.2281e-04 - val_loss: 1.1727e-04 - learning_rate: 1.0000e-04

Epoch 22/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3376e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-04

Epoch 23/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3211e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05

Epoch 24/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3694e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05

Epoch 25/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.2585e-04 - val_loss: 1.1728e-04 - learning_rate: 1.0000e-05

Epoch 26/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step - loss: 1.3536e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05

Epoch 27/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.3663e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-05

Epoch 28/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 1.2884e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06

Epoch 29/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">1s</span> 18ms/step - loss: 1.3420e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06

Epoch 30/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 1.2918e-04 - val_loss: 1.1728e-04 - learning_rate: 1.0000e-06

Epoch 31/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 6ms/step - loss: 1.5119e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06

Epoch 32/100

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 5ms/step - loss: 1.3169e-04 - val_loss: 1.1729e-04 - learning_rate: 1.0000e-06
</pre>
</div>
</div>
</div>
<div id="cell-85" class="cell" data-cell_id="582912c31c724cc1858e1cf7a33adad6" data-deepnote_cell_type="code" data-execution_count="53">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training and Validation Loss of CNN LSTM for Regression'</span>)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-54-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-86" class="cell" data-cell_id="7f15580001274f9d840627fc700c1deb" data-deepnote_cell_type="code" data-execution_count="54">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> result_nn(</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    CNN1D_lstm_reg,</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CNN LSTM (Regression)'</span>,</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    X_tr<span class="op">=</span>X_train_nn,</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    y_tr<span class="op">=</span>y_train_nn,</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    X_te<span class="op">=</span>X_test_nn,</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    y_te<span class="op">=</span>y_test_nn,</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 1.6813e-04

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 6.2670e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">1s</span> 22ms/step

CNN LSTM (Regression) Train MSE: 0.00013050153211224824

CNN LSTM (Regression) Test MSE: 5.0988739531021565e-05

CNN LSTM (Regression) Train RMSE: 0.011423726717330393

CNN LSTM (Regression) Test RMSE: 0.007140639994497802
</pre>
</div>
</div>
</div>
</section>
</section>
<section id="model-comparison-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="model-comparison-and-discussion">5. Model Comparison and Discussion</h2>
<section id="comparison-of-rmse" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-rmse">5.1 Comparison of RMSE</h3>
<p>First, we compare the training and testing RMSE of the 8 models we’ve trained. We use RMSE as criterion.</p>
<div id="cell-90" class="cell" data-cell_id="38fe3e2d11934c7ea4ba1d17095da7d4" data-deepnote_cell_type="code" data-execution_count="55">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load saved models for comparison</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>lin_reg_c <span class="op">=</span> joblib.load(<span class="st">'selector_lin_reg_cv.joblib'</span>)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>dt_c <span class="op">=</span> joblib.load(<span class="st">'selector_dt.joblib'</span>)</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>rf_c <span class="op">=</span> joblib.load(<span class="st">'selector_rf.joblib'</span>)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>xgb_c <span class="op">=</span> joblib.load(<span class="st">'selector_xgb.joblib'</span>)</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>mlp_c <span class="op">=</span> load_model(<span class="st">'MLP_reg.keras'</span>)</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>lstm_c <span class="op">=</span> load_model(<span class="st">'lstm_reg.keras'</span>)</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>cnn_c <span class="op">=</span> load_model(<span class="st">'CNN1D_reg.keras'</span>)</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>cnn_lstm_c <span class="op">=</span> load_model(<span class="st">'CNN1D_lstm_reg.keras'</span>)</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>model_list <span class="op">=</span> [</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'baseline'</span>,</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>    lin_reg_c,</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>    dt_c,</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>    rf_c,</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>    xgb_c,</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>    mlp_c,</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>    lstm_c,</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>    cnn_c,</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>    cnn_lstm_c,</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-23"><a href="#cb98-23" aria-hidden="true" tabindex="-1"></a>model_names <span class="op">=</span> [</span>
<span id="cb98-24"><a href="#cb98-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Baseline'</span>,</span>
<span id="cb98-25"><a href="#cb98-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Linear Regression'</span>,</span>
<span id="cb98-26"><a href="#cb98-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Decision Tree'</span>,</span>
<span id="cb98-27"><a href="#cb98-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Random Forest'</span>,</span>
<span id="cb98-28"><a href="#cb98-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'XGBoost'</span>,</span>
<span id="cb98-29"><a href="#cb98-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MLP'</span>,</span>
<span id="cb98-30"><a href="#cb98-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LSTM'</span>,</span>
<span id="cb98-31"><a href="#cb98-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'1D CNN'</span>,</span>
<span id="cb98-32"><a href="#cb98-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CNN LSTM'</span>,</span>
<span id="cb98-33"><a href="#cb98-33" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-91" class="cell" data-cell_id="2c74f0e8166b44aab961184ad2d91f0a" data-deepnote_cell_type="code" data-execution_count="56">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>df_model_comparison <span class="op">=</span> pd.DataFrame(</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">'model'</span>,</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'model_name'</span>,</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_MSE'</span>,</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_MSE'</span>,</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_RMSE'</span>,</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_RMSE'</span>,</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'y_test_predict'</span>,</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>df_model_comparison[<span class="st">'model'</span>] <span class="op">=</span> model_list</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>df_model_comparison[<span class="st">'model_name'</span>] <span class="op">=</span> model_names</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>df_model_comparison.iloc[<span class="dv">0</span>, <span class="dv">2</span>:<span class="dv">6</span>] <span class="op">=</span> (</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>    baseline_mse_train_final,</span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>    baseline_mse_test_final,</span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>    baseline_rmse_train_final,</span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>    baseline_rmse_test_final,</span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>df_model_comparison.iat[<span class="dv">0</span>, <span class="dv">6</span>] <span class="op">=</span> baseline_test_predict</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">5</span>):</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.iloc[i, <span class="dv">2</span>:<span class="dv">6</span>] <span class="op">=</span> selector_result(</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>        df_model_comparison.loc[i, <span class="st">'model'</span>], print_out<span class="op">=</span><span class="va">False</span></span>
<span id="cb99-27"><a href="#cb99-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb99-28"><a href="#cb99-28" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.iat[i, <span class="dv">6</span>] <span class="op">=</span> selector_result(</span>
<span id="cb99-29"><a href="#cb99-29" aria-hidden="true" tabindex="-1"></a>        df_model_comparison.loc[i, <span class="st">'model'</span>], print_out<span class="op">=</span><span class="va">False</span>, predict<span class="op">=</span><span class="va">True</span></span>
<span id="cb99-30"><a href="#cb99-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb99-31"><a href="#cb99-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-32"><a href="#cb99-32" aria-hidden="true" tabindex="-1"></a>df_model_comparison.iloc[<span class="dv">5</span>, <span class="dv">2</span>:<span class="dv">6</span>] <span class="op">=</span> result_nn(</span>
<span id="cb99-33"><a href="#cb99-33" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.loc[<span class="dv">5</span>, <span class="st">'model'</span>], print_out<span class="op">=</span><span class="va">False</span></span>
<span id="cb99-34"><a href="#cb99-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-35"><a href="#cb99-35" aria-hidden="true" tabindex="-1"></a>df_model_comparison.iat[<span class="dv">5</span>, <span class="dv">6</span>] <span class="op">=</span> result_nn(</span>
<span id="cb99-36"><a href="#cb99-36" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.loc[<span class="dv">5</span>, <span class="st">'model'</span>], print_out<span class="op">=</span><span class="va">False</span>, predict<span class="op">=</span><span class="va">True</span></span>
<span id="cb99-37"><a href="#cb99-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-38"><a href="#cb99-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-39"><a href="#cb99-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>, <span class="dv">9</span>):</span>
<span id="cb99-40"><a href="#cb99-40" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.iloc[i, <span class="dv">2</span>:<span class="dv">6</span>] <span class="op">=</span> result_nn(</span>
<span id="cb99-41"><a href="#cb99-41" aria-hidden="true" tabindex="-1"></a>        df_model_comparison.loc[i, <span class="st">'model'</span>],</span>
<span id="cb99-42"><a href="#cb99-42" aria-hidden="true" tabindex="-1"></a>        model_name<span class="op">=</span><span class="st">''</span>,</span>
<span id="cb99-43"><a href="#cb99-43" aria-hidden="true" tabindex="-1"></a>        X_tr<span class="op">=</span>X_train_nn,</span>
<span id="cb99-44"><a href="#cb99-44" aria-hidden="true" tabindex="-1"></a>        y_tr<span class="op">=</span>y_train_nn,</span>
<span id="cb99-45"><a href="#cb99-45" aria-hidden="true" tabindex="-1"></a>        X_te<span class="op">=</span>X_test_nn,</span>
<span id="cb99-46"><a href="#cb99-46" aria-hidden="true" tabindex="-1"></a>        y_te<span class="op">=</span>y_test_nn,</span>
<span id="cb99-47"><a href="#cb99-47" aria-hidden="true" tabindex="-1"></a>        print_out<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb99-48"><a href="#cb99-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb99-49"><a href="#cb99-49" aria-hidden="true" tabindex="-1"></a>    df_model_comparison.iat[i, <span class="dv">6</span>] <span class="op">=</span> result_nn(</span>
<span id="cb99-50"><a href="#cb99-50" aria-hidden="true" tabindex="-1"></a>        df_model_comparison.loc[i, <span class="st">'model'</span>],</span>
<span id="cb99-51"><a href="#cb99-51" aria-hidden="true" tabindex="-1"></a>        model_name<span class="op">=</span><span class="st">''</span>,</span>
<span id="cb99-52"><a href="#cb99-52" aria-hidden="true" tabindex="-1"></a>        X_tr<span class="op">=</span>X_train_nn,</span>
<span id="cb99-53"><a href="#cb99-53" aria-hidden="true" tabindex="-1"></a>        y_tr<span class="op">=</span>y_train_nn,</span>
<span id="cb99-54"><a href="#cb99-54" aria-hidden="true" tabindex="-1"></a>        X_te<span class="op">=</span>X_test_nn,</span>
<span id="cb99-55"><a href="#cb99-55" aria-hidden="true" tabindex="-1"></a>        y_te<span class="op">=</span>y_test_nn,</span>
<span id="cb99-56"><a href="#cb99-56" aria-hidden="true" tabindex="-1"></a>        print_out<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb99-57"><a href="#cb99-57" aria-hidden="true" tabindex="-1"></a>        predict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb99-58"><a href="#cb99-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb99-59"><a href="#cb99-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-60"><a href="#cb99-60" aria-hidden="true" tabindex="-1"></a>df_model_comparison[<span class="st">'y_test_predict'</span>] <span class="op">=</span> df_model_comparison[<span class="st">'y_test_predict'</span>].<span class="bu">apply</span>(</span>
<span id="cb99-61"><a href="#cb99-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: np.ravel(x)</span>
<span id="cb99-62"><a href="#cb99-62" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-63"><a href="#cb99-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-64"><a href="#cb99-64" aria-hidden="true" tabindex="-1"></a>df_model_comparison</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-bold">49/49</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 8.8159e-04 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0018 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 4ms/step 

<span class="ansi-bold">49/49</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 1ms/step - loss: 8.8159e-04

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0018 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step 

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 1.6815e-04  

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 6.2900e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">1s</span> 30ms/step

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 1.6815e-04

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 6.2900e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step 

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0319  

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 0.0471 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 10ms/step

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 1ms/step - loss: 0.0319 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 3ms/step - loss: 0.0471 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step 

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 1.6813e-04  

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 6.2670e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 16ms/step

<span class="ansi-bold">37/37</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 1.6813e-04 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step - loss: 6.2670e-05 

<span class="ansi-bold">13/13</span> <span class="ansi-green-fg">━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-bold">0s</span> 2ms/step 
</pre>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="56">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">model_name</th>
<th data-quarto-table-cell-role="th">train_MSE</th>
<th data-quarto-table-cell-role="th">test_MSE</th>
<th data-quarto-table-cell-role="th">train_RMSE</th>
<th data-quarto-table-cell-role="th">test_RMSE</th>
<th data-quarto-table-cell-role="th">y_test_predict</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>baseline</td>
<td>Baseline</td>
<td>0.000127</td>
<td>0.000052</td>
<td>0.011283</td>
<td>0.007178</td>
<td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size...</td>
<td>Linear Regression</td>
<td>0.000126</td>
<td>0.000058</td>
<td>0.011231</td>
<td>0.007611</td>
<td>[9.748103946616224e-05, 0.000582024958710533, ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size...</td>
<td>Decision Tree</td>
<td>0.000094</td>
<td>0.000053</td>
<td>0.00969</td>
<td>0.007294</td>
<td>[0.0003911573186231354, 0.0003911573186231354,...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size...</td>
<td>Random Forest</td>
<td>0.000105</td>
<td>0.000054</td>
<td>0.010224</td>
<td>0.00736</td>
<td>[0.0019121735784212019, 0.0010096476163310333,...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>RFECV(cv=TimeSeriesSplit(gap=0, max_train_size...</td>
<td>XGBoost</td>
<td>0.000106</td>
<td>0.000059</td>
<td>0.010296</td>
<td>0.007709</td>
<td>[-0.004676743, 0.002464143, 0.0020128984, 0.00...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>&lt;Sequential name=sequential, built=True&gt;</td>
<td>MLP</td>
<td>0.004358</td>
<td>0.001042</td>
<td>0.066016</td>
<td>0.032276</td>
<td>[0.00056827633, 0.00056827633, 0.00056827633, ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>&lt;Sequential name=sequential_1, built=True&gt;</td>
<td>LSTM</td>
<td>0.000131</td>
<td>0.000051</td>
<td>0.011427</td>
<td>0.007155</td>
<td>[0.00019381603, 0.00019381603, 0.00019381603, ...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>&lt;Sequential name=sequential_2, built=True&gt;</td>
<td>1D CNN</td>
<td>0.019709</td>
<td>0.07369</td>
<td>0.140389</td>
<td>0.27146</td>
<td>[0.15607905, 0.14068294, 0.13164973, 0.1305968...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>&lt;Sequential name=sequential_3, built=True&gt;</td>
<td>CNN LSTM</td>
<td>0.000131</td>
<td>0.000051</td>
<td>0.011424</td>
<td>0.007141</td>
<td>[0.00035510492, 0.00035510492, 0.00035510492, ...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We didn’t find a proper method to set random states for neural network models, so you may get different results if you run the code again.</p>
<p>From the bar graph below, we can observe that there’s no model that is significantly better than the baseline model.</p>
<p>Some testing RMSE is lower than the traing RMSE. The reason might be that our testing data not obtained by randomly sampling, and the testing data is substantially different from the training data, according to our UMAP analysis in section 1.3.1.</p>
<p>From the table above, we can see that the predicted values some models including the neural network models are constants. They may just predict the rate of change to be the daily average rate of change. So, these model fails to grasp the increasing or decreasing trend of the Nasdaq Index.</p>
<div id="cell-93" class="cell" data-cell_id="7103c65ad994478fac8d1a7fbc46e720" data-deepnote_cell_type="code" data-execution_count="57">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Melt the dataframe to have train_MSE and test_MSE in one column</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>df_melted <span class="op">=</span> df_model_comparison.melt(</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    id_vars<span class="op">=</span>[<span class="st">'model_name'</span>],</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>    value_vars<span class="op">=</span>[<span class="st">'train_RMSE'</span>, <span class="st">'test_RMSE'</span>],</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    var_name<span class="op">=</span><span class="st">'RMSE_Type'</span>,</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>    value_name<span class="op">=</span><span class="st">'RMSE_Value'</span>,</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the figure size</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the bars side by side using hue</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'model_name'</span>, y<span class="op">=</span><span class="st">'RMSE_Value'</span>, hue<span class="op">=</span><span class="st">'RMSE_Type'</span>, data<span class="op">=</span>df_melted</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on each bar</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> ax.patches:</span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a>    ax.annotate(</span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f'</span><span class="sc">{</span>p<span class="sc">.</span>get_height()<span class="sc">:.5f}</span><span class="ss">'</span>,</span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>        (p.get_x() <span class="op">+</span> p.get_width() <span class="op">/</span> <span class="fl">2.</span>, p.get_height()),</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a>        va<span class="op">=</span><span class="st">'bottom'</span>,</span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb100-26"><a href="#cb100-26" aria-hidden="true" tabindex="-1"></a>        rotation<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb100-27"><a href="#cb100-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-28"><a href="#cb100-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-29"><a href="#cb100-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels and title</span></span>
<span id="cb100-30"><a href="#cb100-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Model'</span>)</span>
<span id="cb100-31"><a href="#cb100-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb100-32"><a href="#cb100-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train and Test RMSE for Each Model'</span>)</span>
<span id="cb100-33"><a href="#cb100-33" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb100-34"><a href="#cb100-34" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb100-35"><a href="#cb100-35" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb100-36"><a href="#cb100-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-37"><a href="#cb100-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb100-38"><a href="#cb100-38" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb100-39"><a href="#cb100-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-58-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="predicting-nasdaq-index" class="level3">
<h3 class="anchored" data-anchor-id="predicting-nasdaq-index">5.2 Predicting Nasdaq Index</h3>
<div id="cell-95" class="cell" data-cell_id="180acb9b23304c60a1affc2bb0d8f28a" data-deepnote_cell_type="code" data-execution_count="58">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep only 'Close' and 'Close_day-1' columns in df_compare</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>df_compare <span class="op">=</span> df.iloc[split_index:][[<span class="st">'Close'</span>, <span class="st">'Close_day-1'</span>]]</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>):</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    model_name <span class="op">=</span> df_model_comparison.loc[i, <span class="st">'model_name'</span>]</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    y_test_predict <span class="op">=</span> df_model_comparison.loc[i, <span class="st">'y_test_predict'</span>]</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    df_compare[model_name] <span class="op">=</span> df_compare[<span class="st">'Close_day-1'</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> y_test_predict)</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>df_compare</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Close</th>
<th data-quarto-table-cell-role="th">Close_day-1</th>
<th data-quarto-table-cell-role="th">Baseline</th>
<th data-quarto-table-cell-role="th">Linear Regression</th>
<th data-quarto-table-cell-role="th">Decision Tree</th>
<th data-quarto-table-cell-role="th">Random Forest</th>
<th data-quarto-table-cell-role="th">XGBoost</th>
<th data-quarto-table-cell-role="th">MLP</th>
<th data-quarto-table-cell-role="th">LSTM</th>
<th data-quarto-table-cell-role="th">1D CNN</th>
<th data-quarto-table-cell-role="th">CNN LSTM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1598</td>
<td>4750.209961</td>
<td>4736.160156</td>
<td>4736.160156</td>
<td>4736.621842</td>
<td>4738.012740</td>
<td>4745.216516</td>
<td>4714.010275</td>
<td>4738.851577</td>
<td>4737.078186</td>
<td>5475.375552</td>
<td>4737.842082</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1599</td>
<td>4809.879883</td>
<td>4750.209961</td>
<td>4750.209961</td>
<td>4752.974702</td>
<td>4752.068040</td>
<td>4755.005999</td>
<td>4761.915311</td>
<td>4752.909366</td>
<td>4751.130715</td>
<td>5418.483444</td>
<td>4751.896877</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1600</td>
<td>4760.689941</td>
<td>4809.879883</td>
<td>4809.879883</td>
<td>4814.637871</td>
<td>4811.761303</td>
<td>4810.711296</td>
<td>4819.561444</td>
<td>4812.613197</td>
<td>4810.812203</td>
<td>5443.099283</td>
<td>4811.587989</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1601</td>
<td>4737.330078</td>
<td>4760.689941</td>
<td>4760.689941</td>
<td>4765.626611</td>
<td>4762.552120</td>
<td>4763.370205</td>
<td>4807.241778</td>
<td>4763.395302</td>
<td>4761.612726</td>
<td>5382.421176</td>
<td>4762.380579</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1602</td>
<td>4717.680176</td>
<td>4737.330078</td>
<td>4737.330078</td>
<td>4738.565476</td>
<td>4739.183119</td>
<td>4740.332310</td>
<td>4706.732521</td>
<td>4740.022164</td>
<td>4738.248335</td>
<td>5437.549106</td>
<td>4739.012420</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1979</td>
<td>6750.049805</td>
<td>6789.120117</td>
<td>6789.120117</td>
<td>6763.667137</td>
<td>6791.775731</td>
<td>6785.791067</td>
<td>6793.182125</td>
<td>6792.978175</td>
<td>6790.436081</td>
<td>9504.084121</td>
<td>6791.531100</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1980</td>
<td>6750.939941</td>
<td>6750.049805</td>
<td>6750.049805</td>
<td>6723.597959</td>
<td>6752.690136</td>
<td>6748.439763</td>
<td>6762.724141</td>
<td>6753.885660</td>
<td>6751.358196</td>
<td>9488.322709</td>
<td>6752.446913</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1981</td>
<td>6757.600098</td>
<td>6750.939941</td>
<td>6750.939941</td>
<td>6725.801424</td>
<td>6753.580621</td>
<td>6741.231789</td>
<td>6756.923441</td>
<td>6754.776302</td>
<td>6752.248505</td>
<td>9552.616779</td>
<td>6753.337365</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1982</td>
<td>6737.870117</td>
<td>6757.600098</td>
<td>6757.600098</td>
<td>6730.846650</td>
<td>6760.243383</td>
<td>6756.024509</td>
<td>6767.336201</td>
<td>6761.440244</td>
<td>6758.909953</td>
<td>9578.126533</td>
<td>6759.999887</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1983</td>
<td>6706.209961</td>
<td>6737.870117</td>
<td>6737.870117</td>
<td>6712.351766</td>
<td>6740.505684</td>
<td>6726.680086</td>
<td>6732.851619</td>
<td>6741.699051</td>
<td>6739.176147</td>
<td>9479.838308</td>
<td>6740.262900</td>
</tr>
</tbody>
</table>

<p>386 rows × 11 columns</p>
</div>
</div>
</div>
<p>We can see that most model follows closely to the true value, while some model substantially deviate from the true value.</p>
<div id="cell-97" class="cell" data-cell_id="f9a4ce6946af48999f8c7e696f978f78" data-deepnote_cell_type="code" data-execution_count="59">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>test_dates <span class="op">=</span> df_raw.loc[y_test.index, <span class="st">'Date'</span>]</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> column <span class="kw">in</span> df_compare.columns:</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> column <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'Close_day-1'</span>]:</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="st">"True Nasdaq Index"</span> <span class="cf">if</span> column <span class="op">==</span> <span class="st">"Close"</span> <span class="cf">else</span> column</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>        plt.plot(test_dates, df_compare[column], label<span class="op">=</span>label)</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Index"</span>)</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Comparison of True Nasdaq Index and Other Models"</span>)</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="investment-simulation" class="level3">
<h3 class="anchored" data-anchor-id="investment-simulation">5.3 Investment Simulation</h3>
<div id="cell-99" class="cell" data-cell_id="cc04146e6a664005880bf724371a9f89" data-deepnote_cell_type="code" data-execution_count="60">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the true daily return using the 'Close' and 'Close_day-1' columns.</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Daily return is defined as (Close / Close_day-1) - 1.</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>df_compare[<span class="st">'y_test'</span>] <span class="op">=</span> df_compare[<span class="st">'Close'</span>] <span class="op">/</span> df_compare[<span class="st">'Close_day-1'</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the cumulative return for the baseline (Invest &amp; Hold) strategy.</span></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Invest &amp; Hold: simply holding the asset, so cumulative return is the</span></span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a><span class="co"># cumulative product of (1 + true daily return).</span></span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>invest_hold_return <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> df_compare[<span class="st">'y_test'</span>]).cumprod()</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to calculate cumulative strategy returns.</span></span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a><span class="co"># For each day, if the predicted return (y_pred) is &gt;= 0, we take a long</span></span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a><span class="co"># position (+1); otherwise, a short position (-1). The strategy's daily</span></span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a><span class="co"># return is then: position * true daily return.</span></span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_strategy_returns(y_true, y_pred):</span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Determine position: long if predicted return &gt;= 0, else short.</span></span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> np.where(y_pred <span class="op">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate daily strategy returns.</span></span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a>    daily_returns <span class="op">=</span> position <span class="op">*</span> y_true</span>
<span id="cb103-25"><a href="#cb103-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute cumulative returns as the cumulative product of (1 + daily return).</span></span>
<span id="cb103-26"><a href="#cb103-26" aria-hidden="true" tabindex="-1"></a>    cum_returns <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> pd.Series(daily_returns, index<span class="op">=</span>y_true.index)).cumprod()</span>
<span id="cb103-27"><a href="#cb103-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cum_returns, daily_returns</span>
<span id="cb103-28"><a href="#cb103-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-29"><a href="#cb103-29" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-30"><a href="#cb103-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dictionary to hold cumulative return series for each model.</span></span>
<span id="cb103-31"><a href="#cb103-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop over each model in df_model_comparison and compute its strategy returns.</span></span>
<span id="cb103-32"><a href="#cb103-32" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-33"><a href="#cb103-33" aria-hidden="true" tabindex="-1"></a>strategy_returns <span class="op">=</span> {}</span>
<span id="cb103-34"><a href="#cb103-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df_model_comparison)):</span>
<span id="cb103-35"><a href="#cb103-35" aria-hidden="true" tabindex="-1"></a>    model_name <span class="op">=</span> df_model_comparison.loc[i, <span class="st">'model_name'</span>]</span>
<span id="cb103-36"><a href="#cb103-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract predicted returns for the test set and convert them to a Series </span></span>
<span id="cb103-37"><a href="#cb103-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># with the same index as df_compare.</span></span>
<span id="cb103-38"><a href="#cb103-38" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> pd.Series(</span>
<span id="cb103-39"><a href="#cb103-39" aria-hidden="true" tabindex="-1"></a>        df_model_comparison.loc[i, <span class="st">'y_test_predict'</span>], index<span class="op">=</span>df_compare.index</span>
<span id="cb103-40"><a href="#cb103-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb103-41"><a href="#cb103-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the strategy cumulative returns using the true returns and predicted returns.</span></span>
<span id="cb103-42"><a href="#cb103-42" aria-hidden="true" tabindex="-1"></a>    strategy, _ <span class="op">=</span> calc_strategy_returns(df_compare[<span class="st">'y_test'</span>], y_pred)</span>
<span id="cb103-43"><a href="#cb103-43" aria-hidden="true" tabindex="-1"></a>    strategy_returns[model_name] <span class="op">=</span> strategy</span>
<span id="cb103-44"><a href="#cb103-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-45"><a href="#cb103-45" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-46"><a href="#cb103-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the final cumulative return for the Invest &amp; Hold strategy and</span></span>
<span id="cb103-47"><a href="#cb103-47" aria-hidden="true" tabindex="-1"></a><span class="co"># each model-based strategy.</span></span>
<span id="cb103-48"><a href="#cb103-48" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-49"><a href="#cb103-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb103-50"><a href="#cb103-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Invest &amp; Hold Final Cumulative Return: </span><span class="sc">{:.4f}</span><span class="st">"</span>.<span class="bu">format</span>(</span>
<span id="cb103-51"><a href="#cb103-51" aria-hidden="true" tabindex="-1"></a>        invest_hold_return.iloc[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb103-52"><a href="#cb103-52" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb103-53"><a href="#cb103-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-54"><a href="#cb103-54" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model_name, cum_return <span class="kw">in</span> strategy_returns.items():</span>
<span id="cb103-55"><a href="#cb103-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Strategy Final Cumulative Return: </span><span class="sc">{</span>cum_return<span class="sc">.</span>iloc[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb103-56"><a href="#cb103-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-57"><a href="#cb103-57" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-58"><a href="#cb103-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the cumulative return curves for comparison.</span></span>
<span id="cb103-59"><a href="#cb103-59" aria-hidden="true" tabindex="-1"></a><span class="co"># The Invest &amp; Hold curve is plotted in a solid line.</span></span>
<span id="cb103-60"><a href="#cb103-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Each model strategy is plotted with a dashed line.</span></span>
<span id="cb103-61"><a href="#cb103-61" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------</span></span>
<span id="cb103-62"><a href="#cb103-62" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb103-63"><a href="#cb103-63" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb103-64"><a href="#cb103-64" aria-hidden="true" tabindex="-1"></a>    test_dates, invest_hold_return, label<span class="op">=</span><span class="st">"Invest &amp; Hold"</span>, linewidth<span class="op">=</span><span class="dv">2</span></span>
<span id="cb103-65"><a href="#cb103-65" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-66"><a href="#cb103-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each model strategy except for the Baseline (since it's equivalent to Invest &amp; Hold).</span></span>
<span id="cb103-67"><a href="#cb103-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model_name, cum_return <span class="kw">in</span> strategy_returns.items():</span>
<span id="cb103-68"><a href="#cb103-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model_name <span class="op">!=</span> <span class="st">'Baseline'</span>:</span>
<span id="cb103-69"><a href="#cb103-69" aria-hidden="true" tabindex="-1"></a>        plt.plot(</span>
<span id="cb103-70"><a href="#cb103-70" aria-hidden="true" tabindex="-1"></a>            test_dates,</span>
<span id="cb103-71"><a href="#cb103-71" aria-hidden="true" tabindex="-1"></a>            cum_return,</span>
<span id="cb103-72"><a href="#cb103-72" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss"> Strategy"</span>,</span>
<span id="cb103-73"><a href="#cb103-73" aria-hidden="true" tabindex="-1"></a>            linestyle<span class="op">=</span><span class="st">"--"</span></span>
<span id="cb103-74"><a href="#cb103-74" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb103-75"><a href="#cb103-75" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb103-76"><a href="#cb103-76" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Cumulative Return"</span>)</span>
<span id="cb103-77"><a href="#cb103-77" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Comparison of Strategy Cumulative Returns"</span>)</span>
<span id="cb103-78"><a href="#cb103-78" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb103-79"><a href="#cb103-79" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb103-80"><a href="#cb103-80" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Invest &amp; Hold Final Cumulative Return: 1.4160
Baseline Strategy Final Cumulative Return: 1.4160
Linear Regression Strategy Final Cumulative Return: 0.8317
Decision Tree Strategy Final Cumulative Return: 1.3419
Random Forest Strategy Final Cumulative Return: 0.8919
XGBoost Strategy Final Cumulative Return: 1.2267
MLP Strategy Final Cumulative Return: 1.3785
LSTM Strategy Final Cumulative Return: 1.4160
1D CNN Strategy Final Cumulative Return: 1.4160
CNN LSTM Strategy Final Cumulative Return: 1.4160</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nasdaq-prediction_files/figure-html/cell-61-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that no model is significantly better than the simple “invest and hold” strategy. The neural network models closely follows the baseline strategy because their predictions for rate of change are positive constants.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">6. Conclusion</h2>
<p>Although we didn’t find any model that successfully predict the Nasdaq index, we showed that, using this dataset, we cannot use the linear regression, tree based models, and neural network models with the hyperparameter we’ve tried to predict Nasdaq index.</p>
<p>Predicting the financial market and make profit is not an easy task, but machine learning and deep learning is promising tool to do it. We can explore other available data, models, optimization methods to make successful prediction in the future.</p>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<ol type="1">
<li>CNNpred: CNN-based stock market prediction using a diverse set of variables https://archive.ics.uci.edu/dataset/554/cnnpred+cnn+based+stock+market+prediction+using+a+diverse+set+of+variables</li>
<li>The idea of RFE and Time Series Cross-Validation are coming from ChatGPT and DeepSeek. The investment simulation part is modified from ChatGPT. Used ChatGPT to check the PEP8 format.</li>
<li>Martingale (probability theory) https://en.wikipedia.org/wiki/Martingale_(probability_theory)</li>
<li>Recursive feature elimination with cross-validation https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html</li>
</ol>
<p><a style="text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;" href="https://deepnote.com?utm_source=created-in-deepnote-cell&amp;projectId=0931ec6e-a024-4399-8459-27f67845c293" target="_blank"> <img alt="Created in deepnote.com" style="display:inline;max-height:16px;margin:0px;margin-right:7.5px;" src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+">  Created in <span style="font-weight:600;margin-left:4px;">Deepnote</span></a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/luoninz1\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Luoning Zhang · Built with curiosity, ❤️, and <a href="https://quarto.org">Quarto</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>